SpoonOS Cookbook â€“ Combined Markdown Export
Generated by cookbook/scripts/generate-llm.js

---

FILE: docs/api-reference/index.md

# SpoonOS API Reference (auto-generated)

Generated from spoon_ai sources. Each module below has its own page.

- [spoon_ai](./spoon_ai.md)

---

FILE: docs/api-reference/spoon_ai/index.md

---
id: spoon_ai
slug: /api-reference/spoon_ai//index
title: spoon_ai
---

# Table of Contents

* [spoon\_ai](#spoon_ai)
* [spoon\_ai.graph](#spoon_ai.graph)
  * [GraphExecutionError](#spoon_ai.graph.GraphExecutionError)
  * [NodeExecutionError](#spoon_ai.graph.NodeExecutionError)
  * [StateValidationError](#spoon_ai.graph.StateValidationError)
  * [CheckpointError](#spoon_ai.graph.CheckpointError)
  * [GraphConfigurationError](#spoon_ai.graph.GraphConfigurationError)
  * [EdgeRoutingError](#spoon_ai.graph.EdgeRoutingError)
  * [InterruptError](#spoon_ai.graph.InterruptError)
  * [Command](#spoon_ai.graph.Command)
  * [StateSnapshot](#spoon_ai.graph.StateSnapshot)
  * [InMemoryCheckpointer](#spoon_ai.graph.InMemoryCheckpointer)
    * [\_\_init\_\_](#spoon_ai.graph.InMemoryCheckpointer.__init__)
    * [save\_checkpoint](#spoon_ai.graph.InMemoryCheckpointer.save_checkpoint)
    * [get\_checkpoint](#spoon_ai.graph.InMemoryCheckpointer.get_checkpoint)
    * [list\_checkpoints](#spoon_ai.graph.InMemoryCheckpointer.list_checkpoints)
    * [clear\_thread](#spoon_ai.graph.InMemoryCheckpointer.clear_thread)
    * [get\_stats](#spoon_ai.graph.InMemoryCheckpointer.get_stats)
  * [add\_messages](#spoon_ai.graph.add_messages)
  * [interrupt](#spoon_ai.graph.interrupt)
  * [StateGraph](#spoon_ai.graph.StateGraph)
    * [\_\_init\_\_](#spoon_ai.graph.StateGraph.__init__)
    * [add\_node](#spoon_ai.graph.StateGraph.add_node)
    * [add\_llm\_node](#spoon_ai.graph.StateGraph.add_llm_node)
    * [add\_edge](#spoon_ai.graph.StateGraph.add_edge)
    * [add\_conditional\_edges](#spoon_ai.graph.StateGraph.add_conditional_edges)
    * [set\_entry\_point](#spoon_ai.graph.StateGraph.set_entry_point)
    * [compile](#spoon_ai.graph.StateGraph.compile)
  * [CompiledGraph](#spoon_ai.graph.CompiledGraph)
    * [\_\_init\_\_](#spoon_ai.graph.CompiledGraph.__init__)
    * [invoke](#spoon_ai.graph.CompiledGraph.invoke)
    * [stream](#spoon_ai.graph.CompiledGraph.stream)
    * [get\_execution\_history](#spoon_ai.graph.CompiledGraph.get_execution_history)
    * [get\_state](#spoon_ai.graph.CompiledGraph.get_state)
* [spoon\_ai.llm.vlm\_provider.gemini](#spoon_ai.llm.vlm_provider.gemini)
  * [GeminiConfig](#spoon_ai.llm.vlm_provider.gemini.GeminiConfig)
    * [validate\_api\_key](#spoon_ai.llm.vlm_provider.gemini.GeminiConfig.validate_api_key)
  * [GeminiProvider](#spoon_ai.llm.vlm_provider.gemini.GeminiProvider)
    * [\_\_init\_\_](#spoon_ai.llm.vlm_provider.gemini.GeminiProvider.__init__)
    * [chat](#spoon_ai.llm.vlm_provider.gemini.GeminiProvider.chat)
    * [completion](#spoon_ai.llm.vlm_provider.gemini.GeminiProvider.completion)
    * [chat\_with\_tools](#spoon_ai.llm.vlm_provider.gemini.GeminiProvider.chat_with_tools)
    * [generate\_content](#spoon_ai.llm.vlm_provider.gemini.GeminiProvider.generate_content)
* [spoon\_ai.llm.vlm\_provider.base](#spoon_ai.llm.vlm_provider.base)
  * [LLMConfig](#spoon_ai.llm.vlm_provider.base.LLMConfig)
  * [LLMResponse](#spoon_ai.llm.vlm_provider.base.LLMResponse)
    * [text](#spoon_ai.llm.vlm_provider.base.LLMResponse.text)
  * [LLMBase](#spoon_ai.llm.vlm_provider.base.LLMBase)
    * [\_\_init\_\_](#spoon_ai.llm.vlm_provider.base.LLMBase.__init__)
    * [chat](#spoon_ai.llm.vlm_provider.base.LLMBase.chat)
    * [completion](#spoon_ai.llm.vlm_provider.base.LLMBase.completion)
    * [chat\_with\_tools](#spoon_ai.llm.vlm_provider.base.LLMBase.chat_with_tools)
    * [generate\_image](#spoon_ai.llm.vlm_provider.base.LLMBase.generate_image)
    * [reset\_output\_handler](#spoon_ai.llm.vlm_provider.base.LLMBase.reset_output_handler)
* [spoon\_ai.llm.factory](#spoon_ai.llm.factory)
  * [LLMFactory](#spoon_ai.llm.factory.LLMFactory)
    * [register](#spoon_ai.llm.factory.LLMFactory.register)
    * [create](#spoon_ai.llm.factory.LLMFactory.create)
* [spoon\_ai.llm.monitoring](#spoon_ai.llm.monitoring)
  * [RequestMetrics](#spoon_ai.llm.monitoring.RequestMetrics)
  * [ProviderStats](#spoon_ai.llm.monitoring.ProviderStats)
    * [get](#spoon_ai.llm.monitoring.ProviderStats.get)
    * [success\_rate](#spoon_ai.llm.monitoring.ProviderStats.success_rate)
    * [avg\_response\_time](#spoon_ai.llm.monitoring.ProviderStats.avg_response_time)
  * [DebugLogger](#spoon_ai.llm.monitoring.DebugLogger)
    * [\_\_init\_\_](#spoon_ai.llm.monitoring.DebugLogger.__init__)
    * [log\_request](#spoon_ai.llm.monitoring.DebugLogger.log_request)
    * [log\_response](#spoon_ai.llm.monitoring.DebugLogger.log_response)
    * [log\_error](#spoon_ai.llm.monitoring.DebugLogger.log_error)
    * [log\_fallback](#spoon_ai.llm.monitoring.DebugLogger.log_fallback)
    * [get\_request\_history](#spoon_ai.llm.monitoring.DebugLogger.get_request_history)
    * [get\_active\_requests](#spoon_ai.llm.monitoring.DebugLogger.get_active_requests)
    * [clear\_history](#spoon_ai.llm.monitoring.DebugLogger.clear_history)
  * [MetricsCollector](#spoon_ai.llm.monitoring.MetricsCollector)
    * [\_\_init\_\_](#spoon_ai.llm.monitoring.MetricsCollector.__init__)
    * [record\_request](#spoon_ai.llm.monitoring.MetricsCollector.record_request)
    * [get\_provider\_stats](#spoon_ai.llm.monitoring.MetricsCollector.get_provider_stats)
    * [get\_all\_stats](#spoon_ai.llm.monitoring.MetricsCollector.get_all_stats)
    * [get\_rolling\_metrics](#spoon_ai.llm.monitoring.MetricsCollector.get_rolling_metrics)
    * [get\_summary](#spoon_ai.llm.monitoring.MetricsCollector.get_summary)
    * [reset\_stats](#spoon_ai.llm.monitoring.MetricsCollector.reset_stats)
  * [get\_debug\_logger](#spoon_ai.llm.monitoring.get_debug_logger)
  * [get\_metrics\_collector](#spoon_ai.llm.monitoring.get_metrics_collector)
* [spoon\_ai.llm.response\_normalizer](#spoon_ai.llm.response_normalizer)
  * [ResponseNormalizer](#spoon_ai.llm.response_normalizer.ResponseNormalizer)
    * [normalize\_response](#spoon_ai.llm.response_normalizer.ResponseNormalizer.normalize_response)
    * [validate\_response](#spoon_ai.llm.response_normalizer.ResponseNormalizer.validate_response)
    * [add\_provider\_mapping](#spoon_ai.llm.response_normalizer.ResponseNormalizer.add_provider_mapping)
    * [get\_supported\_providers](#spoon_ai.llm.response_normalizer.ResponseNormalizer.get_supported_providers)
  * [get\_response\_normalizer](#spoon_ai.llm.response_normalizer.get_response_normalizer)
* [spoon\_ai.llm.cache](#spoon_ai.llm.cache)
  * [CacheEntry](#spoon_ai.llm.cache.CacheEntry)
    * [is\_expired](#spoon_ai.llm.cache.CacheEntry.is_expired)
    * [touch](#spoon_ai.llm.cache.CacheEntry.touch)
  * [LLMResponseCache](#spoon_ai.llm.cache.LLMResponseCache)
    * [\_\_init\_\_](#spoon_ai.llm.cache.LLMResponseCache.__init__)
    * [get](#spoon_ai.llm.cache.LLMResponseCache.get)
    * [put](#spoon_ai.llm.cache.LLMResponseCache.put)
    * [clear](#spoon_ai.llm.cache.LLMResponseCache.clear)
    * [get\_stats](#spoon_ai.llm.cache.LLMResponseCache.get_stats)
    * [cleanup\_expired](#spoon_ai.llm.cache.LLMResponseCache.cleanup_expired)
  * [get\_global\_cache](#spoon_ai.llm.cache.get_global_cache)
  * [set\_global\_cache](#spoon_ai.llm.cache.set_global_cache)
  * [CachedLLMManager](#spoon_ai.llm.cache.CachedLLMManager)
    * [\_\_init\_\_](#spoon_ai.llm.cache.CachedLLMManager.__init__)
    * [chat](#spoon_ai.llm.cache.CachedLLMManager.chat)
    * [chat\_with\_tools](#spoon_ai.llm.cache.CachedLLMManager.chat_with_tools)
    * [enable\_cache](#spoon_ai.llm.cache.CachedLLMManager.enable_cache)
    * [disable\_cache](#spoon_ai.llm.cache.CachedLLMManager.disable_cache)
    * [clear\_cache](#spoon_ai.llm.cache.CachedLLMManager.clear_cache)
    * [get\_cache\_stats](#spoon_ai.llm.cache.CachedLLMManager.get_cache_stats)
    * [\_\_getattr\_\_](#spoon_ai.llm.cache.CachedLLMManager.__getattr__)
* [spoon\_ai.llm.interface](#spoon_ai.llm.interface)
  * [ProviderCapability](#spoon_ai.llm.interface.ProviderCapability)
  * [ProviderMetadata](#spoon_ai.llm.interface.ProviderMetadata)
  * [LLMResponse](#spoon_ai.llm.interface.LLMResponse)
  * [LLMProviderInterface](#spoon_ai.llm.interface.LLMProviderInterface)
    * [initialize](#spoon_ai.llm.interface.LLMProviderInterface.initialize)
    * [chat](#spoon_ai.llm.interface.LLMProviderInterface.chat)
    * [chat\_stream](#spoon_ai.llm.interface.LLMProviderInterface.chat_stream)
    * [completion](#spoon_ai.llm.interface.LLMProviderInterface.completion)
    * [chat\_with\_tools](#spoon_ai.llm.interface.LLMProviderInterface.chat_with_tools)
    * [get\_metadata](#spoon_ai.llm.interface.LLMProviderInterface.get_metadata)
    * [health\_check](#spoon_ai.llm.interface.LLMProviderInterface.health_check)
    * [cleanup](#spoon_ai.llm.interface.LLMProviderInterface.cleanup)
* [spoon\_ai.llm.providers.deepseek\_provider](#spoon_ai.llm.providers.deepseek_provider)
  * [DeepSeekProvider](#spoon_ai.llm.providers.deepseek_provider.DeepSeekProvider)
    * [get\_metadata](#spoon_ai.llm.providers.deepseek_provider.DeepSeekProvider.get_metadata)
* [spoon\_ai.llm.providers.gemini\_provider](#spoon_ai.llm.providers.gemini_provider)
  * [GeminiProvider](#spoon_ai.llm.providers.gemini_provider.GeminiProvider)
    * [initialize](#spoon_ai.llm.providers.gemini_provider.GeminiProvider.initialize)
    * [chat](#spoon_ai.llm.providers.gemini_provider.GeminiProvider.chat)
    * [chat\_stream](#spoon_ai.llm.providers.gemini_provider.GeminiProvider.chat_stream)
    * [completion](#spoon_ai.llm.providers.gemini_provider.GeminiProvider.completion)
    * [chat\_with\_tools](#spoon_ai.llm.providers.gemini_provider.GeminiProvider.chat_with_tools)
    * [get\_metadata](#spoon_ai.llm.providers.gemini_provider.GeminiProvider.get_metadata)
    * [health\_check](#spoon_ai.llm.providers.gemini_provider.GeminiProvider.health_check)
    * [cleanup](#spoon_ai.llm.providers.gemini_provider.GeminiProvider.cleanup)
* [spoon\_ai.llm.providers.openai\_compatible\_provider](#spoon_ai.llm.providers.openai_compatible_provider)
  * [OpenAICompatibleProvider](#spoon_ai.llm.providers.openai_compatible_provider.OpenAICompatibleProvider)
    * [get\_provider\_name](#spoon_ai.llm.providers.openai_compatible_provider.OpenAICompatibleProvider.get_provider_name)
    * [get\_default\_base\_url](#spoon_ai.llm.providers.openai_compatible_provider.OpenAICompatibleProvider.get_default_base_url)
    * [get\_default\_model](#spoon_ai.llm.providers.openai_compatible_provider.OpenAICompatibleProvider.get_default_model)
    * [get\_additional\_headers](#spoon_ai.llm.providers.openai_compatible_provider.OpenAICompatibleProvider.get_additional_headers)
    * [initialize](#spoon_ai.llm.providers.openai_compatible_provider.OpenAICompatibleProvider.initialize)
    * [chat](#spoon_ai.llm.providers.openai_compatible_provider.OpenAICompatibleProvider.chat)
    * [chat\_stream](#spoon_ai.llm.providers.openai_compatible_provider.OpenAICompatibleProvider.chat_stream)
    * [completion](#spoon_ai.llm.providers.openai_compatible_provider.OpenAICompatibleProvider.completion)
    * [chat\_with\_tools](#spoon_ai.llm.providers.openai_compatible_provider.OpenAICompatibleProvider.chat_with_tools)
    * [get\_metadata](#spoon_ai.llm.providers.openai_compatible_provider.OpenAICompatibleProvider.get_metadata)
    * [health\_check](#spoon_ai.llm.providers.openai_compatible_provider.OpenAICompatibleProvider.health_check)
    * [cleanup](#spoon_ai.llm.providers.openai_compatible_provider.OpenAICompatibleProvider.cleanup)
* [spoon\_ai.llm.providers.openai\_provider](#spoon_ai.llm.providers.openai_provider)
  * [OpenAIProvider](#spoon_ai.llm.providers.openai_provider.OpenAIProvider)
    * [get\_metadata](#spoon_ai.llm.providers.openai_provider.OpenAIProvider.get_metadata)
* [spoon\_ai.llm.providers.anthropic\_provider](#spoon_ai.llm.providers.anthropic_provider)
  * [AnthropicProvider](#spoon_ai.llm.providers.anthropic_provider.AnthropicProvider)
    * [initialize](#spoon_ai.llm.providers.anthropic_provider.AnthropicProvider.initialize)
    * [get\_cache\_metrics](#spoon_ai.llm.providers.anthropic_provider.AnthropicProvider.get_cache_metrics)
    * [chat](#spoon_ai.llm.providers.anthropic_provider.AnthropicProvider.chat)
    * [chat\_stream](#spoon_ai.llm.providers.anthropic_provider.AnthropicProvider.chat_stream)
    * [completion](#spoon_ai.llm.providers.anthropic_provider.AnthropicProvider.completion)
    * [chat\_with\_tools](#spoon_ai.llm.providers.anthropic_provider.AnthropicProvider.chat_with_tools)
    * [get\_metadata](#spoon_ai.llm.providers.anthropic_provider.AnthropicProvider.get_metadata)
    * [health\_check](#spoon_ai.llm.providers.anthropic_provider.AnthropicProvider.health_check)
    * [cleanup](#spoon_ai.llm.providers.anthropic_provider.AnthropicProvider.cleanup)
* [spoon\_ai.llm.providers.openrouter\_provider](#spoon_ai.llm.providers.openrouter_provider)
  * [OpenRouterProvider](#spoon_ai.llm.providers.openrouter_provider.OpenRouterProvider)
    * [get\_additional\_headers](#spoon_ai.llm.providers.openrouter_provider.OpenRouterProvider.get_additional_headers)
    * [get\_metadata](#spoon_ai.llm.providers.openrouter_provider.OpenRouterProvider.get_metadata)
* [spoon\_ai.llm.providers](#spoon_ai.llm.providers)
* [spoon\_ai.llm.manager](#spoon_ai.llm.manager)
  * [ProviderState](#spoon_ai.llm.manager.ProviderState)
    * [can\_retry\_initialization](#spoon_ai.llm.manager.ProviderState.can_retry_initialization)
    * [record\_initialization\_failure](#spoon_ai.llm.manager.ProviderState.record_initialization_failure)
    * [record\_initialization\_success](#spoon_ai.llm.manager.ProviderState.record_initialization_success)
  * [FallbackStrategy](#spoon_ai.llm.manager.FallbackStrategy)
    * [execute\_with\_fallback](#spoon_ai.llm.manager.FallbackStrategy.execute_with_fallback)
  * [LoadBalancer](#spoon_ai.llm.manager.LoadBalancer)
    * [select\_provider](#spoon_ai.llm.manager.LoadBalancer.select_provider)
    * [update\_provider\_health](#spoon_ai.llm.manager.LoadBalancer.update_provider_health)
    * [set\_provider\_weight](#spoon_ai.llm.manager.LoadBalancer.set_provider_weight)
  * [LLMManager](#spoon_ai.llm.manager.LLMManager)
    * [\_\_init\_\_](#spoon_ai.llm.manager.LLMManager.__init__)
    * [cleanup](#spoon_ai.llm.manager.LLMManager.cleanup)
    * [get\_provider\_status](#spoon_ai.llm.manager.LLMManager.get_provider_status)
    * [reset\_provider](#spoon_ai.llm.manager.LLMManager.reset_provider)
    * [chat](#spoon_ai.llm.manager.LLMManager.chat)
    * [chat\_stream](#spoon_ai.llm.manager.LLMManager.chat_stream)
    * [completion](#spoon_ai.llm.manager.LLMManager.completion)
    * [chat\_with\_tools](#spoon_ai.llm.manager.LLMManager.chat_with_tools)
    * [set\_fallback\_chain](#spoon_ai.llm.manager.LLMManager.set_fallback_chain)
    * [enable\_load\_balancing](#spoon_ai.llm.manager.LLMManager.enable_load_balancing)
    * [disable\_load\_balancing](#spoon_ai.llm.manager.LLMManager.disable_load_balancing)
    * [health\_check\_all](#spoon_ai.llm.manager.LLMManager.health_check_all)
    * [cleanup](#spoon_ai.llm.manager.LLMManager.cleanup)
    * [get\_stats](#spoon_ai.llm.manager.LLMManager.get_stats)
  * [get\_llm\_manager](#spoon_ai.llm.manager.get_llm_manager)
  * [set\_llm\_manager](#spoon_ai.llm.manager.set_llm_manager)
* [spoon\_ai.llm.registry](#spoon_ai.llm.registry)
  * [LLMProviderRegistry](#spoon_ai.llm.registry.LLMProviderRegistry)
    * [register](#spoon_ai.llm.registry.LLMProviderRegistry.register)
    * [get\_provider](#spoon_ai.llm.registry.LLMProviderRegistry.get_provider)
    * [list\_providers](#spoon_ai.llm.registry.LLMProviderRegistry.list_providers)
    * [get\_capabilities](#spoon_ai.llm.registry.LLMProviderRegistry.get_capabilities)
    * [is\_registered](#spoon_ai.llm.registry.LLMProviderRegistry.is_registered)
    * [unregister](#spoon_ai.llm.registry.LLMProviderRegistry.unregister)
    * [clear](#spoon_ai.llm.registry.LLMProviderRegistry.clear)
  * [register\_provider](#spoon_ai.llm.registry.register_provider)
  * [get\_global\_registry](#spoon_ai.llm.registry.get_global_registry)
* [spoon\_ai.llm.config](#spoon_ai.llm.config)
  * [ProviderConfig](#spoon_ai.llm.config.ProviderConfig)
    * [\_\_post\_init\_\_](#spoon_ai.llm.config.ProviderConfig.__post_init__)
    * [model\_dump](#spoon_ai.llm.config.ProviderConfig.model_dump)
  * [ConfigurationManager](#spoon_ai.llm.config.ConfigurationManager)
    * [\_\_init\_\_](#spoon_ai.llm.config.ConfigurationManager.__init__)
    * [load\_provider\_config](#spoon_ai.llm.config.ConfigurationManager.load_provider_config)
    * [validate\_config](#spoon_ai.llm.config.ConfigurationManager.validate_config)
    * [get\_default\_provider](#spoon_ai.llm.config.ConfigurationManager.get_default_provider)
    * [get\_fallback\_chain](#spoon_ai.llm.config.ConfigurationManager.get_fallback_chain)
    * [list\_configured\_providers](#spoon_ai.llm.config.ConfigurationManager.list_configured_providers)
    * [get\_available\_providers\_by\_priority](#spoon_ai.llm.config.ConfigurationManager.get_available_providers_by_priority)
    * [get\_provider\_info](#spoon_ai.llm.config.ConfigurationManager.get_provider_info)
    * [reload\_config](#spoon_ai.llm.config.ConfigurationManager.reload_config)
* [spoon\_ai.llm](#spoon_ai.llm)
* [spoon\_ai.llm.errors](#spoon_ai.llm.errors)
  * [LLMError](#spoon_ai.llm.errors.LLMError)
  * [ProviderError](#spoon_ai.llm.errors.ProviderError)
  * [ConfigurationError](#spoon_ai.llm.errors.ConfigurationError)
  * [RateLimitError](#spoon_ai.llm.errors.RateLimitError)
  * [AuthenticationError](#spoon_ai.llm.errors.AuthenticationError)
  * [ModelNotFoundError](#spoon_ai.llm.errors.ModelNotFoundError)
  * [TokenLimitError](#spoon_ai.llm.errors.TokenLimitError)
  * [NetworkError](#spoon_ai.llm.errors.NetworkError)
  * [ProviderUnavailableError](#spoon_ai.llm.errors.ProviderUnavailableError)
  * [ValidationError](#spoon_ai.llm.errors.ValidationError)
* [spoon\_ai.llm.base](#spoon_ai.llm.base)
  * [LLMBase](#spoon_ai.llm.base.LLMBase)
    * [\_\_init\_\_](#spoon_ai.llm.base.LLMBase.__init__)
    * [chat](#spoon_ai.llm.base.LLMBase.chat)
    * [completion](#spoon_ai.llm.base.LLMBase.completion)
    * [chat\_with\_tools](#spoon_ai.llm.base.LLMBase.chat_with_tools)
    * [generate\_image](#spoon_ai.llm.base.LLMBase.generate_image)
    * [reset\_output\_handler](#spoon_ai.llm.base.LLMBase.reset_output_handler)
* [spoon\_ai.utils.utils](#spoon_ai.utils.utils)
* [spoon\_ai.utils.config\_manager](#spoon_ai.utils.config_manager)
  * [ConfigManager](#spoon_ai.utils.config_manager.ConfigManager)
    * [\_\_init\_\_](#spoon_ai.utils.config_manager.ConfigManager.__init__)
    * [refresh](#spoon_ai.utils.config_manager.ConfigManager.refresh)
    * [get](#spoon_ai.utils.config_manager.ConfigManager.get)
    * [set](#spoon_ai.utils.config_manager.ConfigManager.set)
    * [list\_config](#spoon_ai.utils.config_manager.ConfigManager.list_config)
    * [get\_api\_key](#spoon_ai.utils.config_manager.ConfigManager.get_api_key)
    * [set\_api\_key](#spoon_ai.utils.config_manager.ConfigManager.set_api_key)
    * [get\_model\_name](#spoon_ai.utils.config_manager.ConfigManager.get_model_name)
    * [get\_base\_url](#spoon_ai.utils.config_manager.ConfigManager.get_base_url)
    * [get\_llm\_provider](#spoon_ai.utils.config_manager.ConfigManager.get_llm_provider)
* [spoon\_ai.utils.config](#spoon_ai.utils.config)
* [spoon\_ai.utils](#spoon_ai.utils)
* [spoon\_ai.utils.streaming](#spoon_ai.utils.streaming)
  * [StreamOutcome](#spoon_ai.utils.streaming.StreamOutcome)
* [spoon\_ai.runnables.events](#spoon_ai.runnables.events)
  * [StreamEventBuilder](#spoon_ai.runnables.events.StreamEventBuilder)
    * [chain\_start](#spoon_ai.runnables.events.StreamEventBuilder.chain_start)
    * [chain\_stream](#spoon_ai.runnables.events.StreamEventBuilder.chain_stream)
    * [chain\_end](#spoon_ai.runnables.events.StreamEventBuilder.chain_end)
    * [chain\_error](#spoon_ai.runnables.events.StreamEventBuilder.chain_error)
    * [llm\_stream](#spoon_ai.runnables.events.StreamEventBuilder.llm_stream)
* [spoon\_ai.runnables](#spoon_ai.runnables)
* [spoon\_ai.runnables.base](#spoon_ai.runnables.base)
  * [log\_patches\_from\_events](#spoon_ai.runnables.base.log_patches_from_events)
  * [Runnable](#spoon_ai.runnables.base.Runnable)
    * [astream\_log](#spoon_ai.runnables.base.Runnable.astream_log)
    * [astream\_events](#spoon_ai.runnables.base.Runnable.astream_events)
* [spoon\_ai.payments.server](#spoon_ai.payments.server)
  * [create\_paywalled\_router](#spoon_ai.payments.server.create_paywalled_router)
* [spoon\_ai.payments.cli](#spoon_ai.payments.cli)
* [spoon\_ai.payments.facilitator\_client](#spoon_ai.payments.facilitator_client)
  * [X402FacilitatorClient](#spoon_ai.payments.facilitator_client.X402FacilitatorClient)
* [spoon\_ai.payments.exceptions](#spoon_ai.payments.exceptions)
  * [X402PaymentError](#spoon_ai.payments.exceptions.X402PaymentError)
  * [X402ConfigurationError](#spoon_ai.payments.exceptions.X402ConfigurationError)
  * [X402VerificationError](#spoon_ai.payments.exceptions.X402VerificationError)
  * [X402SettlementError](#spoon_ai.payments.exceptions.X402SettlementError)
* [spoon\_ai.payments.x402\_service](#spoon_ai.payments.x402_service)
  * [X402PaymentService](#spoon_ai.payments.x402_service.X402PaymentService)
    * [discover\_resources](#spoon_ai.payments.x402_service.X402PaymentService.discover_resources)
    * [render\_paywall\_html](#spoon_ai.payments.x402_service.X402PaymentService.render_paywall_html)
    * [build\_payment\_header](#spoon_ai.payments.x402_service.X402PaymentService.build_payment_header)
    * [decode\_payment\_response](#spoon_ai.payments.x402_service.X402PaymentService.decode_payment_response)
* [spoon\_ai.payments.config](#spoon_ai.payments.config)
  * [X402ConfigurationError](#spoon_ai.payments.config.X402ConfigurationError)
  * [X402PaywallBranding](#spoon_ai.payments.config.X402PaywallBranding)
  * [X402ClientConfig](#spoon_ai.payments.config.X402ClientConfig)
  * [X402Settings](#spoon_ai.payments.config.X402Settings)
    * [amount\_in\_atomic\_units](#spoon_ai.payments.config.X402Settings.amount_in_atomic_units)
    * [build\_asset\_extra](#spoon_ai.payments.config.X402Settings.build_asset_extra)
    * [load](#spoon_ai.payments.config.X402Settings.load)
* [spoon\_ai.payments.app](#spoon_ai.payments.app)
* [spoon\_ai.payments](#spoon_ai.payments)
* [spoon\_ai.payments.models](#spoon_ai.payments.models)
  * [X402PaymentRequest](#spoon_ai.payments.models.X402PaymentRequest)
  * [X402VerifyResult](#spoon_ai.payments.models.X402VerifyResult)
  * [X402SettleResult](#spoon_ai.payments.models.X402SettleResult)
  * [X402PaymentOutcome](#spoon_ai.payments.models.X402PaymentOutcome)
  * [X402PaymentReceipt](#spoon_ai.payments.models.X402PaymentReceipt)
* [spoon\_ai.chat](#spoon_ai.chat)
  * [ShortTermMemoryConfig](#spoon_ai.chat.ShortTermMemoryConfig)
    * [enabled](#spoon_ai.chat.ShortTermMemoryConfig.enabled)
    * [max\_tokens](#spoon_ai.chat.ShortTermMemoryConfig.max_tokens)
    * [strategy](#spoon_ai.chat.ShortTermMemoryConfig.strategy)
    * [messages\_to\_keep](#spoon_ai.chat.ShortTermMemoryConfig.messages_to_keep)
    * [trim\_strategy](#spoon_ai.chat.ShortTermMemoryConfig.trim_strategy)
    * [keep\_system\_messages](#spoon_ai.chat.ShortTermMemoryConfig.keep_system_messages)
    * [auto\_checkpoint](#spoon_ai.chat.ShortTermMemoryConfig.auto_checkpoint)
    * [checkpoint\_thread\_id](#spoon_ai.chat.ShortTermMemoryConfig.checkpoint_thread_id)
    * [summary\_model](#spoon_ai.chat.ShortTermMemoryConfig.summary_model)
  * [ChatBot](#spoon_ai.chat.ChatBot)
    * [\_\_init\_\_](#spoon_ai.chat.ChatBot.__init__)
    * [update\_mem0\_config](#spoon_ai.chat.ChatBot.update_mem0_config)
    * [ask](#spoon_ai.chat.ChatBot.ask)
    * [ask\_tool](#spoon_ai.chat.ChatBot.ask_tool)
    * [trim\_messages](#spoon_ai.chat.ChatBot.trim_messages)
    * [remove\_message](#spoon_ai.chat.ChatBot.remove_message)
    * [remove\_all\_messages](#spoon_ai.chat.ChatBot.remove_all_messages)
    * [summarize\_messages](#spoon_ai.chat.ChatBot.summarize_messages)
    * [latest\_summary](#spoon_ai.chat.ChatBot.latest_summary)
    * [latest\_removals](#spoon_ai.chat.ChatBot.latest_removals)
    * [save\_checkpoint](#spoon_ai.chat.ChatBot.save_checkpoint)
    * [restore\_checkpoint](#spoon_ai.chat.ChatBot.restore_checkpoint)
    * [list\_checkpoints](#spoon_ai.chat.ChatBot.list_checkpoints)
    * [clear\_checkpoints](#spoon_ai.chat.ChatBot.clear_checkpoints)
    * [astream](#spoon_ai.chat.ChatBot.astream)
    * [astream\_events](#spoon_ai.chat.ChatBot.astream_events)
    * [astream\_log](#spoon_ai.chat.ChatBot.astream_log)
* [spoon\_ai.tools.turnkey\_tools](#spoon_ai.tools.turnkey_tools)
  * [TurnkeyBaseTool](#spoon_ai.tools.turnkey_tools.TurnkeyBaseTool)
    * [client](#spoon_ai.tools.turnkey_tools.TurnkeyBaseTool.client)
  * [SignEVMTransactionTool](#spoon_ai.tools.turnkey_tools.SignEVMTransactionTool)
    * [execute](#spoon_ai.tools.turnkey_tools.SignEVMTransactionTool.execute)
  * [SignMessageTool](#spoon_ai.tools.turnkey_tools.SignMessageTool)
    * [execute](#spoon_ai.tools.turnkey_tools.SignMessageTool.execute)
  * [SignTypedDataTool](#spoon_ai.tools.turnkey_tools.SignTypedDataTool)
    * [execute](#spoon_ai.tools.turnkey_tools.SignTypedDataTool.execute)
  * [BroadcastTransactionTool](#spoon_ai.tools.turnkey_tools.BroadcastTransactionTool)
    * [execute](#spoon_ai.tools.turnkey_tools.BroadcastTransactionTool.execute)
  * [ListWalletsTool](#spoon_ai.tools.turnkey_tools.ListWalletsTool)
    * [execute](#spoon_ai.tools.turnkey_tools.ListWalletsTool.execute)
  * [ListWalletAccountsTool](#spoon_ai.tools.turnkey_tools.ListWalletAccountsTool)
    * [execute](#spoon_ai.tools.turnkey_tools.ListWalletAccountsTool.execute)
  * [GetActivityTool](#spoon_ai.tools.turnkey_tools.GetActivityTool)
    * [execute](#spoon_ai.tools.turnkey_tools.GetActivityTool.execute)
  * [ListActivitiesTool](#spoon_ai.tools.turnkey_tools.ListActivitiesTool)
    * [execute](#spoon_ai.tools.turnkey_tools.ListActivitiesTool.execute)
  * [WhoAmITool](#spoon_ai.tools.turnkey_tools.WhoAmITool)
    * [execute](#spoon_ai.tools.turnkey_tools.WhoAmITool.execute)
  * [BuildUnsignedEIP1559TxTool](#spoon_ai.tools.turnkey_tools.BuildUnsignedEIP1559TxTool)
    * [execute](#spoon_ai.tools.turnkey_tools.BuildUnsignedEIP1559TxTool.execute)
  * [ListAllAccountsTool](#spoon_ai.tools.turnkey_tools.ListAllAccountsTool)
    * [execute](#spoon_ai.tools.turnkey_tools.ListAllAccountsTool.execute)
  * [BatchSignTransactionsTool](#spoon_ai.tools.turnkey_tools.BatchSignTransactionsTool)
    * [execute](#spoon_ai.tools.turnkey_tools.BatchSignTransactionsTool.execute)
  * [CreateWalletTool](#spoon_ai.tools.turnkey_tools.CreateWalletTool)
    * [execute](#spoon_ai.tools.turnkey_tools.CreateWalletTool.execute)
  * [GetWalletTool](#spoon_ai.tools.turnkey_tools.GetWalletTool)
    * [execute](#spoon_ai.tools.turnkey_tools.GetWalletTool.execute)
  * [CreateWalletAccountsTool](#spoon_ai.tools.turnkey_tools.CreateWalletAccountsTool)
    * [execute](#spoon_ai.tools.turnkey_tools.CreateWalletAccountsTool.execute)
  * [CompleteTransactionWorkflowTool](#spoon_ai.tools.turnkey_tools.CompleteTransactionWorkflowTool)
    * [execute](#spoon_ai.tools.turnkey_tools.CompleteTransactionWorkflowTool.execute)
  * [get\_turnkey\_tools](#spoon_ai.tools.turnkey_tools.get_turnkey_tools)
* [spoon\_ai.tools.tool\_manager](#spoon_ai.tools.tool_manager)
  * [ToolManager](#spoon_ai.tools.tool_manager.ToolManager)
    * [reindex](#spoon_ai.tools.tool_manager.ToolManager.reindex)
* [spoon\_ai.tools.neofs\_tools](#spoon_ai.tools.neofs_tools)
  * [get\_shared\_neofs\_client](#spoon_ai.tools.neofs_tools.get_shared_neofs_client)
  * [CreateBearerTokenTool](#spoon_ai.tools.neofs_tools.CreateBearerTokenTool)
  * [CreateContainerTool](#spoon_ai.tools.neofs_tools.CreateContainerTool)
  * [UploadObjectTool](#spoon_ai.tools.neofs_tools.UploadObjectTool)
  * [DownloadObjectByIdTool](#spoon_ai.tools.neofs_tools.DownloadObjectByIdTool)
  * [GetObjectHeaderByIdTool](#spoon_ai.tools.neofs_tools.GetObjectHeaderByIdTool)
  * [DownloadObjectByAttributeTool](#spoon_ai.tools.neofs_tools.DownloadObjectByAttributeTool)
  * [GetObjectHeaderByAttributeTool](#spoon_ai.tools.neofs_tools.GetObjectHeaderByAttributeTool)
  * [DeleteObjectTool](#spoon_ai.tools.neofs_tools.DeleteObjectTool)
  * [SearchObjectsTool](#spoon_ai.tools.neofs_tools.SearchObjectsTool)
  * [SetContainerEaclTool](#spoon_ai.tools.neofs_tools.SetContainerEaclTool)
  * [GetContainerEaclTool](#spoon_ai.tools.neofs_tools.GetContainerEaclTool)
  * [ListContainersTool](#spoon_ai.tools.neofs_tools.ListContainersTool)
  * [GetContainerInfoTool](#spoon_ai.tools.neofs_tools.GetContainerInfoTool)
  * [DeleteContainerTool](#spoon_ai.tools.neofs_tools.DeleteContainerTool)
  * [GetNetworkInfoTool](#spoon_ai.tools.neofs_tools.GetNetworkInfoTool)
  * [GetBalanceTool](#spoon_ai.tools.neofs_tools.GetBalanceTool)
* [spoon\_ai.tools.x402\_payment](#spoon_ai.tools.x402_payment)
  * [X402PaymentHeaderTool](#spoon_ai.tools.x402_payment.X402PaymentHeaderTool)
  * [X402PaywalledRequestTool](#spoon_ai.tools.x402_payment.X402PaywalledRequestTool)
* [spoon\_ai.tools](#spoon_ai.tools)
* [spoon\_ai.tools.mcp\_tool](#spoon_ai.tools.mcp_tool)
  * [MCPTool](#spoon_ai.tools.mcp_tool.MCPTool)
    * [call\_mcp\_tool](#spoon_ai.tools.mcp_tool.MCPTool.call_mcp_tool)
    * [list\_available\_tools](#spoon_ai.tools.mcp_tool.MCPTool.list_available_tools)
* [spoon\_ai.tools.base](#spoon_ai.tools.base)
  * [ToolFailure](#spoon_ai.tools.base.ToolFailure)
* [spoon\_ai.graph.agent](#spoon_ai.graph.agent)
  * [Memory](#spoon_ai.graph.agent.Memory)
    * [clear](#spoon_ai.graph.agent.Memory.clear)
    * [add\_message](#spoon_ai.graph.agent.Memory.add_message)
    * [get\_messages](#spoon_ai.graph.agent.Memory.get_messages)
    * [get\_recent\_messages](#spoon_ai.graph.agent.Memory.get_recent_messages)
    * [search\_messages](#spoon_ai.graph.agent.Memory.search_messages)
    * [get\_statistics](#spoon_ai.graph.agent.Memory.get_statistics)
    * [set\_metadata](#spoon_ai.graph.agent.Memory.set_metadata)
    * [get\_metadata](#spoon_ai.graph.agent.Memory.get_metadata)
  * [MockMemory](#spoon_ai.graph.agent.MockMemory)
  * [GraphAgent](#spoon_ai.graph.agent.GraphAgent)
    * [search\_memory](#spoon_ai.graph.agent.GraphAgent.search_memory)
    * [get\_recent\_memory](#spoon_ai.graph.agent.GraphAgent.get_recent_memory)
    * [get\_memory\_statistics](#spoon_ai.graph.agent.GraphAgent.get_memory_statistics)
    * [set\_memory\_metadata](#spoon_ai.graph.agent.GraphAgent.set_memory_metadata)
    * [get\_memory\_metadata](#spoon_ai.graph.agent.GraphAgent.get_memory_metadata)
    * [save\_session](#spoon_ai.graph.agent.GraphAgent.save_session)
    * [load\_session](#spoon_ai.graph.agent.GraphAgent.load_session)
* [spoon\_ai.graph.types](#spoon_ai.graph.types)
* [spoon\_ai.graph.checkpointer](#spoon_ai.graph.checkpointer)
  * [InMemoryCheckpointer](#spoon_ai.graph.checkpointer.InMemoryCheckpointer)
    * [iter\_checkpoint\_history](#spoon_ai.graph.checkpointer.InMemoryCheckpointer.iter_checkpoint_history)
* [spoon\_ai.graph.builder](#spoon_ai.graph.builder)
  * [Intent](#spoon_ai.graph.builder.Intent)
  * [IntentAnalyzer](#spoon_ai.graph.builder.IntentAnalyzer)
  * [AdaptiveStateBuilder](#spoon_ai.graph.builder.AdaptiveStateBuilder)
  * [ParameterInferenceEngine](#spoon_ai.graph.builder.ParameterInferenceEngine)
  * [NodeSpec](#spoon_ai.graph.builder.NodeSpec)
  * [EdgeSpec](#spoon_ai.graph.builder.EdgeSpec)
    * [end](#spoon_ai.graph.builder.EdgeSpec.end)
  * [ParallelGroupSpec](#spoon_ai.graph.builder.ParallelGroupSpec)
  * [GraphTemplate](#spoon_ai.graph.builder.GraphTemplate)
  * [DeclarativeGraphBuilder](#spoon_ai.graph.builder.DeclarativeGraphBuilder)
  * [NodePlugin](#spoon_ai.graph.builder.NodePlugin)
  * [NodePluginSystem](#spoon_ai.graph.builder.NodePluginSystem)
  * [HighLevelGraphAPI](#spoon_ai.graph.builder.HighLevelGraphAPI)
* [spoon\_ai.graph.mcp\_integration](#spoon_ai.graph.mcp_integration)
  * [MCPToolSpec](#spoon_ai.graph.mcp_integration.MCPToolSpec)
  * [MCPConfigManager](#spoon_ai.graph.mcp_integration.MCPConfigManager)
  * [MCPToolDiscoveryEngine](#spoon_ai.graph.mcp_integration.MCPToolDiscoveryEngine)
  * [MCPIntegrationManager](#spoon_ai.graph.mcp_integration.MCPIntegrationManager)
* [spoon\_ai.graph.exceptions](#spoon_ai.graph.exceptions)
* [spoon\_ai.graph.reducers](#spoon_ai.graph.reducers)
* [spoon\_ai.graph.decorators](#spoon_ai.graph.decorators)
* [spoon\_ai.graph.config](#spoon_ai.graph.config)
  * [RouterConfig](#spoon_ai.graph.config.RouterConfig)
  * [ParallelRetryPolicy](#spoon_ai.graph.config.ParallelRetryPolicy)
  * [ParallelGroupConfig](#spoon_ai.graph.config.ParallelGroupConfig)
    * [quorum](#spoon_ai.graph.config.ParallelGroupConfig.quorum)
    * [error\_strategy](#spoon_ai.graph.config.ParallelGroupConfig.error_strategy)
  * [GraphConfig](#spoon_ai.graph.config.GraphConfig)
* [spoon\_ai.graph.engine](#spoon_ai.graph.engine)
  * [BaseNode](#spoon_ai.graph.engine.BaseNode)
    * [\_\_call\_\_](#spoon_ai.graph.engine.BaseNode.__call__)
  * [RunnableNode](#spoon_ai.graph.engine.RunnableNode)
    * [\_\_call\_\_](#spoon_ai.graph.engine.RunnableNode.__call__)
  * [ToolNode](#spoon_ai.graph.engine.ToolNode)
    * [\_\_call\_\_](#spoon_ai.graph.engine.ToolNode.__call__)
  * [ConditionNode](#spoon_ai.graph.engine.ConditionNode)
    * [\_\_call\_\_](#spoon_ai.graph.engine.ConditionNode.__call__)
  * [interrupt](#spoon_ai.graph.engine.interrupt)
  * [RouteRule](#spoon_ai.graph.engine.RouteRule)
    * [matches](#spoon_ai.graph.engine.RouteRule.matches)
  * [RunningSummary](#spoon_ai.graph.engine.RunningSummary)
  * [SummarizationNode](#spoon_ai.graph.engine.SummarizationNode)
  * [StateGraph](#spoon_ai.graph.engine.StateGraph)
    * [add\_node](#spoon_ai.graph.engine.StateGraph.add_node)
    * [add\_edge](#spoon_ai.graph.engine.StateGraph.add_edge)
    * [add\_conditional\_edges](#spoon_ai.graph.engine.StateGraph.add_conditional_edges)
    * [set\_entry\_point](#spoon_ai.graph.engine.StateGraph.set_entry_point)
    * [add\_tool\_node](#spoon_ai.graph.engine.StateGraph.add_tool_node)
    * [add\_conditional\_node](#spoon_ai.graph.engine.StateGraph.add_conditional_node)
    * [add\_parallel\_group](#spoon_ai.graph.engine.StateGraph.add_parallel_group)
    * [add\_routing\_rule](#spoon_ai.graph.engine.StateGraph.add_routing_rule)
    * [get\_state](#spoon_ai.graph.engine.StateGraph.get_state)
    * [get\_state\_history](#spoon_ai.graph.engine.StateGraph.get_state_history)
    * [add\_pattern\_routing](#spoon_ai.graph.engine.StateGraph.add_pattern_routing)
    * [set\_intelligent\_router](#spoon_ai.graph.engine.StateGraph.set_intelligent_router)
    * [set\_llm\_router](#spoon_ai.graph.engine.StateGraph.set_llm_router)
    * [enable\_llm\_routing](#spoon_ai.graph.engine.StateGraph.enable_llm_routing)
    * [compile](#spoon_ai.graph.engine.StateGraph.compile)
    * [get\_graph](#spoon_ai.graph.engine.StateGraph.get_graph)
  * [CompiledGraph](#spoon_ai.graph.engine.CompiledGraph)
    * [get\_execution\_metrics](#spoon_ai.graph.engine.CompiledGraph.get_execution_metrics)
* [spoon\_ai.neofs.utils](#spoon_ai.neofs.utils)
  * [SignatureError](#spoon_ai.neofs.utils.SignatureError)
  * [sign\_bearer\_token](#spoon_ai.neofs.utils.sign_bearer_token)
* [spoon\_ai.neofs.client](#spoon_ai.neofs.client)
  * [NeoFSClient](#spoon_ai.neofs.client.NeoFSClient)
    * [set\_container\_eacl](#spoon_ai.neofs.client.NeoFSClient.set_container_eacl)
    * [download\_object\_by\_id](#spoon_ai.neofs.client.NeoFSClient.download_object_by_id)
    * [get\_object\_header\_by\_id](#spoon_ai.neofs.client.NeoFSClient.get_object_header_by_id)
    * [download\_object\_by\_attribute](#spoon_ai.neofs.client.NeoFSClient.download_object_by_attribute)
    * [get\_object\_header\_by\_attribute](#spoon_ai.neofs.client.NeoFSClient.get_object_header_by_attribute)
    * [search\_objects](#spoon_ai.neofs.client.NeoFSClient.search_objects)
  * [NeoFSException](#spoon_ai.neofs.client.NeoFSException)
  * [NeoFSAPIException](#spoon_ai.neofs.client.NeoFSAPIException)
* [spoon\_ai.neofs](#spoon_ai.neofs)
* [spoon\_ai.neofs.models](#spoon_ai.neofs.models)
  * [NetworkInfo](#spoon_ai.neofs.models.NetworkInfo)
* [spoon\_ai.agents.toolcall](#spoon_ai.agents.toolcall)
  * [ToolCallAgent](#spoon_ai.agents.toolcall.ToolCallAgent)
    * [tool\_choices](#spoon_ai.agents.toolcall.ToolCallAgent.tool_choices)
    * [mcp\_tools\_cache\_ttl](#spoon_ai.agents.toolcall.ToolCallAgent.mcp_tools_cache_ttl)
    * [run](#spoon_ai.agents.toolcall.ToolCallAgent.run)
    * [step](#spoon_ai.agents.toolcall.ToolCallAgent.step)
* [spoon\_ai.agents.react](#spoon_ai.agents.react)
* [spoon\_ai.agents.mcp\_client\_mixin](#spoon_ai.agents.mcp_client_mixin)
  * [MCPClientMixin](#spoon_ai.agents.mcp_client_mixin.MCPClientMixin)
    * [get\_session](#spoon_ai.agents.mcp_client_mixin.MCPClientMixin.get_session)
    * [list\_mcp\_tools](#spoon_ai.agents.mcp_client_mixin.MCPClientMixin.list_mcp_tools)
    * [call\_mcp\_tool](#spoon_ai.agents.mcp_client_mixin.MCPClientMixin.call_mcp_tool)
    * [send\_mcp\_message](#spoon_ai.agents.mcp_client_mixin.MCPClientMixin.send_mcp_message)
    * [cleanup](#spoon_ai.agents.mcp_client_mixin.MCPClientMixin.cleanup)
    * [get\_session\_stats](#spoon_ai.agents.mcp_client_mixin.MCPClientMixin.get_session_stats)
* [spoon\_ai.agents.spoon\_react\_mcp](#spoon_ai.agents.spoon_react_mcp)
  * [SpoonReactMCP](#spoon_ai.agents.spoon_react_mcp.SpoonReactMCP)
    * [list\_mcp\_tools](#spoon_ai.agents.spoon_react_mcp.SpoonReactMCP.list_mcp_tools)
* [spoon\_ai.agents.monitor](#spoon_ai.agents.monitor)
* [spoon\_ai.agents.rag](#spoon_ai.agents.rag)
  * [RetrievalMixin](#spoon_ai.agents.rag.RetrievalMixin)
    * [initialize\_retrieval\_client](#spoon_ai.agents.rag.RetrievalMixin.initialize_retrieval_client)
    * [add\_documents](#spoon_ai.agents.rag.RetrievalMixin.add_documents)
    * [retrieve\_relevant\_documents](#spoon_ai.agents.rag.RetrievalMixin.retrieve_relevant_documents)
    * [get\_context\_from\_query](#spoon_ai.agents.rag.RetrievalMixin.get_context_from_query)
* [spoon\_ai.agents.custom\_agent](#spoon_ai.agents.custom_agent)
  * [CustomAgent](#spoon_ai.agents.custom_agent.CustomAgent)
    * [add\_tool](#spoon_ai.agents.custom_agent.CustomAgent.add_tool)
    * [add\_tools](#spoon_ai.agents.custom_agent.CustomAgent.add_tools)
    * [remove\_tool](#spoon_ai.agents.custom_agent.CustomAgent.remove_tool)
    * [list\_tools](#spoon_ai.agents.custom_agent.CustomAgent.list_tools)
    * [get\_tool\_info](#spoon_ai.agents.custom_agent.CustomAgent.get_tool_info)
    * [validate\_tools](#spoon_ai.agents.custom_agent.CustomAgent.validate_tools)
    * [run](#spoon_ai.agents.custom_agent.CustomAgent.run)
    * [clear](#spoon_ai.agents.custom_agent.CustomAgent.clear)
* [spoon\_ai.agents.graph\_agent](#spoon_ai.agents.graph_agent)
  * [GraphAgent](#spoon_ai.agents.graph_agent.GraphAgent)
    * [\_\_init\_\_](#spoon_ai.agents.graph_agent.GraphAgent.__init__)
    * [validate\_graph](#spoon_ai.agents.graph_agent.GraphAgent.validate_graph)
    * [run](#spoon_ai.agents.graph_agent.GraphAgent.run)
    * [step](#spoon_ai.agents.graph_agent.GraphAgent.step)
    * [get\_execution\_history](#spoon_ai.agents.graph_agent.GraphAgent.get_execution_history)
    * [get\_execution\_metadata](#spoon_ai.agents.graph_agent.GraphAgent.get_execution_metadata)
    * [clear\_state](#spoon_ai.agents.graph_agent.GraphAgent.clear_state)
    * [update\_initial\_state](#spoon_ai.agents.graph_agent.GraphAgent.update_initial_state)
    * [set\_preserve\_state](#spoon_ai.agents.graph_agent.GraphAgent.set_preserve_state)
* [spoon\_ai.agents](#spoon_ai.agents)
* [spoon\_ai.agents.spoon\_react](#spoon_ai.agents.spoon_react)
  * [create\_configured\_chatbot](#spoon_ai.agents.spoon_react.create_configured_chatbot)
  * [SpoonReactAI](#spoon_ai.agents.spoon_react.SpoonReactAI)
    * [\_\_init\_\_](#spoon_ai.agents.spoon_react.SpoonReactAI.__init__)
    * [initialize](#spoon_ai.agents.spoon_react.SpoonReactAI.initialize)
    * [run](#spoon_ai.agents.spoon_react.SpoonReactAI.run)
* [spoon\_ai.agents.base](#spoon_ai.agents.base)
  * [ThreadSafeOutputQueue](#spoon_ai.agents.base.ThreadSafeOutputQueue)
    * [get](#spoon_ai.agents.base.ThreadSafeOutputQueue.get)
  * [BaseAgent](#spoon_ai.agents.base.BaseAgent)
    * [add\_message](#spoon_ai.agents.base.BaseAgent.add_message)
    * [state\_context](#spoon_ai.agents.base.BaseAgent.state_context)
    * [run](#spoon_ai.agents.base.BaseAgent.run)
    * [step](#spoon_ai.agents.base.BaseAgent.step)
    * [is\_stuck](#spoon_ai.agents.base.BaseAgent.is_stuck)
    * [handle\_stuck\_state](#spoon_ai.agents.base.BaseAgent.handle_stuck_state)
    * [add\_documents](#spoon_ai.agents.base.BaseAgent.add_documents)
    * [save\_chat\_history](#spoon_ai.agents.base.BaseAgent.save_chat_history)
    * [stream](#spoon_ai.agents.base.BaseAgent.stream)
    * [process\_mcp\_message](#spoon_ai.agents.base.BaseAgent.process_mcp_message)
    * [shutdown](#spoon_ai.agents.base.BaseAgent.shutdown)
    * [get\_diagnostics](#spoon_ai.agents.base.BaseAgent.get_diagnostics)
* [spoon\_ai.prompts.toolcall](#spoon_ai.prompts.toolcall)
* [spoon\_ai.prompts](#spoon_ai.prompts)
* [spoon\_ai.prompts.spoon\_react](#spoon_ai.prompts.spoon_react)
* [spoon\_ai.turnkey.client](#spoon_ai.turnkey.client)
  * [Turnkey](#spoon_ai.turnkey.client.Turnkey)
    * [\_\_init\_\_](#spoon_ai.turnkey.client.Turnkey.__init__)
    * [whoami](#spoon_ai.turnkey.client.Turnkey.whoami)
    * [import\_private\_key](#spoon_ai.turnkey.client.Turnkey.import_private_key)
    * [sign\_evm\_transaction](#spoon_ai.turnkey.client.Turnkey.sign_evm_transaction)
    * [sign\_typed\_data](#spoon_ai.turnkey.client.Turnkey.sign_typed_data)
    * [sign\_message](#spoon_ai.turnkey.client.Turnkey.sign_message)
    * [get\_activity](#spoon_ai.turnkey.client.Turnkey.get_activity)
    * [list\_activities](#spoon_ai.turnkey.client.Turnkey.list_activities)
    * [get\_policy\_evaluations](#spoon_ai.turnkey.client.Turnkey.get_policy_evaluations)
    * [get\_private\_key](#spoon_ai.turnkey.client.Turnkey.get_private_key)
    * [create\_wallet](#spoon_ai.turnkey.client.Turnkey.create_wallet)
    * [create\_wallet\_accounts](#spoon_ai.turnkey.client.Turnkey.create_wallet_accounts)
    * [get\_wallet](#spoon_ai.turnkey.client.Turnkey.get_wallet)
    * [get\_wallet\_account](#spoon_ai.turnkey.client.Turnkey.get_wallet_account)
    * [list\_wallets](#spoon_ai.turnkey.client.Turnkey.list_wallets)
    * [list\_wallet\_accounts](#spoon_ai.turnkey.client.Turnkey.list_wallet_accounts)
    * [init\_import\_wallet](#spoon_ai.turnkey.client.Turnkey.init_import_wallet)
    * [encrypt\_wallet](#spoon_ai.turnkey.client.Turnkey.encrypt_wallet)
    * [encrypt\_private\_key](#spoon_ai.turnkey.client.Turnkey.encrypt_private_key)
    * [init\_import\_private\_key](#spoon_ai.turnkey.client.Turnkey.init_import_private_key)
    * [import\_wallet](#spoon_ai.turnkey.client.Turnkey.import_wallet)
* [spoon\_ai.turnkey](#spoon_ai.turnkey)
* [spoon\_ai.callbacks.streaming\_stdout](#spoon_ai.callbacks.streaming_stdout)
  * [StreamingStdOutCallbackHandler](#spoon_ai.callbacks.streaming_stdout.StreamingStdOutCallbackHandler)
    * [on\_llm\_new\_token](#spoon_ai.callbacks.streaming_stdout.StreamingStdOutCallbackHandler.on_llm_new_token)
    * [on\_llm\_end](#spoon_ai.callbacks.streaming_stdout.StreamingStdOutCallbackHandler.on_llm_end)
* [spoon\_ai.callbacks.statistics](#spoon_ai.callbacks.statistics)
  * [StreamingStatisticsCallback](#spoon_ai.callbacks.statistics.StreamingStatisticsCallback)
* [spoon\_ai.callbacks.stream\_event](#spoon_ai.callbacks.stream_event)
  * [StreamEventCallbackHandler](#spoon_ai.callbacks.stream_event.StreamEventCallbackHandler)
* [spoon\_ai.callbacks.manager](#spoon_ai.callbacks.manager)
  * [CallbackManager](#spoon_ai.callbacks.manager.CallbackManager)
* [spoon\_ai.callbacks](#spoon_ai.callbacks)
* [spoon\_ai.callbacks.base](#spoon_ai.callbacks.base)
  * [RetrieverManagerMixin](#spoon_ai.callbacks.base.RetrieverManagerMixin)
    * [on\_retriever\_start](#spoon_ai.callbacks.base.RetrieverManagerMixin.on_retriever_start)
    * [on\_retriever\_end](#spoon_ai.callbacks.base.RetrieverManagerMixin.on_retriever_end)
    * [on\_retriever\_error](#spoon_ai.callbacks.base.RetrieverManagerMixin.on_retriever_error)
  * [LLMManagerMixin](#spoon_ai.callbacks.base.LLMManagerMixin)
    * [on\_llm\_start](#spoon_ai.callbacks.base.LLMManagerMixin.on_llm_start)
    * [on\_llm\_new\_token](#spoon_ai.callbacks.base.LLMManagerMixin.on_llm_new_token)
    * [on\_llm\_end](#spoon_ai.callbacks.base.LLMManagerMixin.on_llm_end)
    * [on\_llm\_error](#spoon_ai.callbacks.base.LLMManagerMixin.on_llm_error)
  * [ChainManagerMixin](#spoon_ai.callbacks.base.ChainManagerMixin)
    * [on\_chain\_start](#spoon_ai.callbacks.base.ChainManagerMixin.on_chain_start)
    * [on\_chain\_end](#spoon_ai.callbacks.base.ChainManagerMixin.on_chain_end)
    * [on\_chain\_error](#spoon_ai.callbacks.base.ChainManagerMixin.on_chain_error)
  * [ToolManagerMixin](#spoon_ai.callbacks.base.ToolManagerMixin)
    * [on\_tool\_start](#spoon_ai.callbacks.base.ToolManagerMixin.on_tool_start)
    * [on\_tool\_end](#spoon_ai.callbacks.base.ToolManagerMixin.on_tool_end)
    * [on\_tool\_error](#spoon_ai.callbacks.base.ToolManagerMixin.on_tool_error)
  * [PromptManagerMixin](#spoon_ai.callbacks.base.PromptManagerMixin)
    * [on\_prompt\_start](#spoon_ai.callbacks.base.PromptManagerMixin.on_prompt_start)
    * [on\_prompt\_end](#spoon_ai.callbacks.base.PromptManagerMixin.on_prompt_end)
    * [on\_prompt\_error](#spoon_ai.callbacks.base.PromptManagerMixin.on_prompt_error)
  * [BaseCallbackHandler](#spoon_ai.callbacks.base.BaseCallbackHandler)
    * [raise\_error](#spoon_ai.callbacks.base.BaseCallbackHandler.raise_error)
    * [run\_inline](#spoon_ai.callbacks.base.BaseCallbackHandler.run_inline)
    * [ignore\_llm](#spoon_ai.callbacks.base.BaseCallbackHandler.ignore_llm)
    * [ignore\_chain](#spoon_ai.callbacks.base.BaseCallbackHandler.ignore_chain)
    * [ignore\_tool](#spoon_ai.callbacks.base.BaseCallbackHandler.ignore_tool)
    * [ignore\_retriever](#spoon_ai.callbacks.base.BaseCallbackHandler.ignore_retriever)
    * [ignore\_prompt](#spoon_ai.callbacks.base.BaseCallbackHandler.ignore_prompt)
  * [AsyncCallbackHandler](#spoon_ai.callbacks.base.AsyncCallbackHandler)
* [spoon\_ai.schema](#spoon_ai.schema)
  * [Function](#spoon_ai.schema.Function)
    * [get\_arguments\_dict](#spoon_ai.schema.Function.get_arguments_dict)
    * [create](#spoon_ai.schema.Function.create)
  * [AgentState](#spoon_ai.schema.AgentState)
  * [ToolChoice](#spoon_ai.schema.ToolChoice)
  * [Role](#spoon_ai.schema.Role)
  * [ROLE\_TYPE](#spoon_ai.schema.ROLE_TYPE)
  * [Message](#spoon_ai.schema.Message)
    * [role](#spoon_ai.schema.Message.role)
  * [SystemMessage](#spoon_ai.schema.SystemMessage)
    * [role](#spoon_ai.schema.SystemMessage.role)
  * [TOOL\_CHOICE\_TYPE](#spoon_ai.schema.TOOL_CHOICE_TYPE)
  * [LLMConfig](#spoon_ai.schema.LLMConfig)
  * [LLMResponse](#spoon_ai.schema.LLMResponse)
    * [text](#spoon_ai.schema.LLMResponse.text)
  * [LLMResponseChunk](#spoon_ai.schema.LLMResponseChunk)
* [spoon\_ai.memory.utils](#spoon_ai.memory.utils)
  * [extract\_memories](#spoon_ai.memory.utils.extract_memories)
  * [extract\_first\_memory\_id](#spoon_ai.memory.utils.extract_first_memory_id)
* [spoon\_ai.memory.short\_term\_manager](#spoon_ai.memory.short_term_manager)
  * [TrimStrategy](#spoon_ai.memory.short_term_manager.TrimStrategy)
    * [FROM\_START](#spoon_ai.memory.short_term_manager.TrimStrategy.FROM_START)
    * [FROM\_END](#spoon_ai.memory.short_term_manager.TrimStrategy.FROM_END)
  * [MessageTokenCounter](#spoon_ai.memory.short_term_manager.MessageTokenCounter)
  * [ShortTermMemoryManager](#spoon_ai.memory.short_term_manager.ShortTermMemoryManager)
    * [trim\_messages](#spoon_ai.memory.short_term_manager.ShortTermMemoryManager.trim_messages)
    * [summarize\_messages](#spoon_ai.memory.short_term_manager.ShortTermMemoryManager.summarize_messages)
* [spoon\_ai.memory.remove\_message](#spoon_ai.memory.remove_message)
  * [RemoveMessage](#spoon_ai.memory.remove_message.RemoveMessage)
* [spoon\_ai.memory](#spoon_ai.memory)
* [spoon\_ai.memory.mem0\_client](#spoon_ai.memory.mem0_client)
  * [SpoonMem0](#spoon_ai.memory.mem0_client.SpoonMem0)
    * [add\_text](#spoon_ai.memory.mem0_client.SpoonMem0.add_text)
    * [get\_all\_memory](#spoon_ai.memory.mem0_client.SpoonMem0.get_all_memory)
* [spoon\_ai.retrieval.chroma](#spoon_ai.retrieval.chroma)
* [spoon\_ai.retrieval.qdrant](#spoon_ai.retrieval.qdrant)
* [spoon\_ai.retrieval.document\_loader](#spoon_ai.retrieval.document_loader)
  * [BasicTextSplitter](#spoon_ai.retrieval.document_loader.BasicTextSplitter)
    * [split\_text](#spoon_ai.retrieval.document_loader.BasicTextSplitter.split_text)
    * [split\_documents](#spoon_ai.retrieval.document_loader.BasicTextSplitter.split_documents)
  * [DocumentLoader](#spoon_ai.retrieval.document_loader.DocumentLoader)
    * [load\_directory](#spoon_ai.retrieval.document_loader.DocumentLoader.load_directory)
    * [load\_file](#spoon_ai.retrieval.document_loader.DocumentLoader.load_file)
* [spoon\_ai.retrieval](#spoon_ai.retrieval)
* [spoon\_ai.retrieval.base](#spoon_ai.retrieval.base)
  * [BaseRetrievalClient](#spoon_ai.retrieval.base.BaseRetrievalClient)

<a id="spoon_ai"></a>

# Module `spoon_ai`

<a id="spoon_ai.graph"></a>

# Module `spoon_ai.graph`

Graph-based execution system for SpoonOS agents.

This module provides a LangGraph-inspired framework with advanced features:
- State management with TypedDict and reducers
- LLM Manager integration
- Error handling and recovery
- Human-in-the-loop patterns
- Multi-agent coordination
- Comprehensive testing support
- Checkpointing and persistence

<a id="spoon_ai.graph.GraphExecutionError"></a>

## `GraphExecutionError` Objects

```python
class GraphExecutionError(Exception)
```

Raised when graph execution encounters an error.

<a id="spoon_ai.graph.NodeExecutionError"></a>

## `NodeExecutionError` Objects

```python
class NodeExecutionError(Exception)
```

Raised when a node fails to execute.

<a id="spoon_ai.graph.StateValidationError"></a>

## `StateValidationError` Objects

```python
class StateValidationError(Exception)
```

Raised when state validation fails.

<a id="spoon_ai.graph.CheckpointError"></a>

## `CheckpointError` Objects

```python
class CheckpointError(Exception)
```

Raised when checkpoint operations fail.

<a id="spoon_ai.graph.GraphConfigurationError"></a>

## `GraphConfigurationError` Objects

```python
class GraphConfigurationError(Exception)
```

Raised when graph configuration is invalid.

<a id="spoon_ai.graph.EdgeRoutingError"></a>

## `EdgeRoutingError` Objects

```python
class EdgeRoutingError(Exception)
```

Raised when edge routing fails.

<a id="spoon_ai.graph.InterruptError"></a>

## `InterruptError` Objects

```python
class InterruptError(Exception)
```

Raised when graph execution is interrupted for human input.

<a id="spoon_ai.graph.Command"></a>

## `Command` Objects

```python
@dataclass
class Command()
```

Command object for controlling graph flow and state updates.

<a id="spoon_ai.graph.StateSnapshot"></a>

## `StateSnapshot` Objects

```python
@dataclass
class StateSnapshot()
```

Snapshot of graph state at a specific point in time.

<a id="spoon_ai.graph.InMemoryCheckpointer"></a>

## `InMemoryCheckpointer` Objects

```python
class InMemoryCheckpointer()
```

Simple in-memory checkpointer for development and testing.

This checkpointer stores state snapshots in memory and provides
basic checkpoint management functionality. For production use,
consider using persistent checkpointers like Redis or PostgreSQL.

<a id="spoon_ai.graph.InMemoryCheckpointer.__init__"></a>

#### `__init__`

```python
def __init__(max_checkpoints_per_thread: int = 100)
```

Initialize the in-memory checkpointer.

**Arguments**:

- `max_checkpoints_per_thread` - Maximum number of checkpoints to keep per thread

<a id="spoon_ai.graph.InMemoryCheckpointer.save_checkpoint"></a>

#### `save_checkpoint`

```python
def save_checkpoint(thread_id: str, snapshot: StateSnapshot) -> None
```

Save a checkpoint for a thread.

**Arguments**:

- `thread_id` - Unique identifier for the thread
- `snapshot` - State snapshot to save
  

**Raises**:

- `CheckpointError` - If checkpoint saving fails

<a id="spoon_ai.graph.InMemoryCheckpointer.get_checkpoint"></a>

#### `get_checkpoint`

```python
def get_checkpoint(thread_id: str,
                   checkpoint_id: str = None) -> Optional[StateSnapshot]
```

Get a specific checkpoint or the latest one.

**Arguments**:

- `thread_id` - Unique identifier for the thread
- `checkpoint_id` - Optional specific checkpoint ID
  

**Returns**:

  StateSnapshot or None if not found
  

**Raises**:

- `CheckpointError` - If checkpoint retrieval fails

<a id="spoon_ai.graph.InMemoryCheckpointer.list_checkpoints"></a>

#### `list_checkpoints`

```python
def list_checkpoints(thread_id: str) -> List[StateSnapshot]
```

List all checkpoints for a thread.

**Arguments**:

- `thread_id` - Unique identifier for the thread
  

**Returns**:

  List of state snapshots
  

**Raises**:

- `CheckpointError` - If checkpoint listing fails

<a id="spoon_ai.graph.InMemoryCheckpointer.clear_thread"></a>

#### `clear_thread`

```python
def clear_thread(thread_id: str) -> None
```

Clear all checkpoints for a thread.

**Arguments**:

- `thread_id` - Unique identifier for the thread

<a id="spoon_ai.graph.InMemoryCheckpointer.get_stats"></a>

#### `get_stats`

```python
def get_stats() -> Dict[str, Any]
```

Get checkpointer statistics.

**Returns**:

  Dictionary with statistics

<a id="spoon_ai.graph.add_messages"></a>

#### `add_messages`

```python
def add_messages(existing: List[Any], new: List[Any]) -> List[Any]
```

Reducer function for adding messages to a list.

<a id="spoon_ai.graph.interrupt"></a>

#### `interrupt`

```python
def interrupt(data: Dict[str, Any]) -> Any
```

Interrupt execution and wait for human input.

<a id="spoon_ai.graph.StateGraph"></a>

## `StateGraph` Objects

```python
class StateGraph()
```

Enhanced StateGraph with LangGraph-inspired features and SpoonOS integration.

Features:
- TypedDict state management with reducers
- LLM Manager integration
- Error handling and recovery
- Human-in-the-loop patterns
- Checkpointing and persistence
- Multi-agent coordination support

<a id="spoon_ai.graph.StateGraph.__init__"></a>

#### `__init__`

```python
def __init__(state_schema: type, checkpointer: InMemoryCheckpointer = None)
```

Initialize the enhanced state graph.

**Arguments**:

- `state_schema` - TypedDict class defining the state structure
- `checkpointer` - Optional checkpointer for state persistence

<a id="spoon_ai.graph.StateGraph.add_node"></a>

#### `add_node`

```python
def add_node(name: str, action: Callable) -> "StateGraph"
```

Add a node to the graph.

**Arguments**:

- `name` - Unique identifier for the node
- `action` - Function or coroutine that processes the state
  Should accept state dict and return dict of updates or Command
  

**Returns**:

  Self for method chaining
  

**Raises**:

- `GraphConfigurationError` - If node name already exists or is invalid

<a id="spoon_ai.graph.StateGraph.add_llm_node"></a>

#### `add_llm_node`

```python
def add_llm_node(
        name: str,
        system_prompt: str,
        provider: Optional[str] = None,
        model_params: Optional[Dict[str, Any]] = None) -> "StateGraph"
```

Add an LLM-powered node to the graph.

**Arguments**:

- `name` - Unique identifier for the node
- `system_prompt` - System prompt for the LLM
- `provider` - Specific LLM provider to use
- `model_params` - Parameters for the LLM call
  

**Returns**:

  Self for method chaining

<a id="spoon_ai.graph.StateGraph.add_edge"></a>

#### `add_edge`

```python
def add_edge(start_node: str, end_node: str) -> "StateGraph"
```

Add a direct, unconditional edge between two nodes.

**Arguments**:

- `start_node` - Name of the source node (or "START")
- `end_node` - Name of the destination node (or "END")
  

**Returns**:

  Self for method chaining
  

**Raises**:

- `GraphConfigurationError` - If nodes don't exist or edge is invalid

<a id="spoon_ai.graph.StateGraph.add_conditional_edges"></a>

#### `add_conditional_edges`

```python
def add_conditional_edges(start_node: str,
                          condition: Callable[[Dict[str, Any]], str],
                          path_map: Dict[str, str]) -> "StateGraph"
```

Add conditional edges that route to different nodes based on state.

**Arguments**:

- `start_node` - Name of the source node
- `condition` - Function that takes state and returns a key from path_map
- `path_map` - Mapping from condition results to destination node names
  

**Returns**:

  Self for method chaining
  

**Raises**:

- `GraphConfigurationError` - If configuration is invalid

<a id="spoon_ai.graph.StateGraph.set_entry_point"></a>

#### `set_entry_point`

```python
def set_entry_point(node_name: str) -> "StateGraph"
```

Set the starting node for graph execution.

**Arguments**:

- `node_name` - Name of the node to start execution from
  

**Returns**:

  Self for method chaining
  

**Raises**:

- `GraphConfigurationError` - If entry point node doesn't exist

<a id="spoon_ai.graph.StateGraph.compile"></a>

#### `compile`

```python
def compile() -> "CompiledGraph"
```

Compile the graph into an executable form.

**Returns**:

  CompiledGraph instance ready for execution
  

**Raises**:

- `GraphConfigurationError` - If graph configuration is invalid

<a id="spoon_ai.graph.CompiledGraph"></a>

## `CompiledGraph` Objects

```python
class CompiledGraph()
```

Executable version of a StateGraph with advanced features.

<a id="spoon_ai.graph.CompiledGraph.__init__"></a>

#### `__init__`

```python
def __init__(graph: StateGraph)
```

Initialize with a compiled StateGraph.

<a id="spoon_ai.graph.CompiledGraph.invoke"></a>

#### `invoke`

```python
async def invoke(initial_state: Optional[Dict[str, Any]] = None,
                 config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]
```

Execute the graph from the entry point.

<a id="spoon_ai.graph.CompiledGraph.stream"></a>

#### `stream`

```python
async def stream(initial_state: Optional[Dict[str, Any]] = None,
                 config: Optional[Dict[str, Any]] = None,
                 stream_mode: str = "values")
```

Stream graph execution with different modes.

<a id="spoon_ai.graph.CompiledGraph.get_execution_history"></a>

#### `get_execution_history`

```python
def get_execution_history() -> List[Dict[str, Any]]
```

Get the execution history for debugging and analysis.

<a id="spoon_ai.graph.CompiledGraph.get_state"></a>

#### `get_state`

```python
def get_state(config: Dict[str, Any]) -> Optional[StateSnapshot]
```

Get the current state snapshot for a thread.

<a id="spoon_ai.llm.vlm_provider.gemini"></a>

# Module `spoon_ai.llm.vlm_provider.gemini`

<a id="spoon_ai.llm.vlm_provider.gemini.GeminiConfig"></a>

## `GeminiConfig` Objects

```python
class GeminiConfig(LLMConfig)
```

Gemini Configuration

<a id="spoon_ai.llm.vlm_provider.gemini.GeminiConfig.validate_api_key"></a>

#### `validate_api_key`

```python
@model_validator(mode='after')
def validate_api_key()
```

Validate that API key is provided

<a id="spoon_ai.llm.vlm_provider.gemini.GeminiProvider"></a>

## `GeminiProvider` Objects

```python
@LLMFactory.register("gemini")
class GeminiProvider(LLMBase)
```

Gemini Provider Implementation

<a id="spoon_ai.llm.vlm_provider.gemini.GeminiProvider.__init__"></a>

#### `__init__`

```python
def __init__(config_path: str = "config/config.toml",
             config_name: str = "chitchat")
```

Initialize Gemini Provider

**Arguments**:

- `config_path` - Configuration file path
- `config_name` - Configuration name
  

**Raises**:

- `ValueError` - If GEMINI_API_KEY environment variable is not set

<a id="spoon_ai.llm.vlm_provider.gemini.GeminiProvider.chat"></a>

#### `chat`

```python
async def chat(messages: List[Message],
               system_msgs: Optional[List[Message]] = None,
               response_modalities: Optional[List[str]] = None,
               **kwargs) -> LLMResponse
```

Send chat request to Gemini and get response

**Arguments**:

- `messages` - List of messages
- `system_msgs` - List of system messages
- `response_modalities` - List of response modalities (optional, e.g. ['Text', 'Image'])
- `**kwargs` - Other parameters
  

**Returns**:

- `LLMResponse` - LLM response

<a id="spoon_ai.llm.vlm_provider.gemini.GeminiProvider.completion"></a>

#### `completion`

```python
async def completion(prompt: str, **kwargs) -> LLMResponse
```

Send text completion request to Gemini and get response

**Arguments**:

- `prompt` - Prompt text
- `**kwargs` - Other parameters
  

**Returns**:

- `LLMResponse` - LLM response

<a id="spoon_ai.llm.vlm_provider.gemini.GeminiProvider.chat_with_tools"></a>

#### `chat_with_tools`

```python
async def chat_with_tools(messages: List[Message],
                          system_msgs: Optional[List[Message]] = None,
                          tools: Optional[List[Dict]] = None,
                          tool_choice: Literal["none", "auto",
                                               "required"] = "auto",
                          **kwargs) -> LLMResponse
```

Send chat request to Gemini that may contain tool calls and get response

Note: Gemini currently doesn't support tool calls, this method will use regular chat method

**Arguments**:

- `messages` - List of messages
- `system_msgs` - List of system messages
- `tools` - List of tools (not supported by Gemini)
- `tool_choice` - Tool choice mode (not supported by Gemini)
- `**kwargs` - Other parameters
  

**Returns**:

- `LLMResponse` - LLM response

<a id="spoon_ai.llm.vlm_provider.gemini.GeminiProvider.generate_content"></a>

#### `generate_content`

```python
async def generate_content(model: Optional[str] = None,
                           contents: Union[str, List, types.Content,
                                           types.Part] = None,
                           config: Optional[
                               types.GenerateContentConfig] = None,
                           **kwargs) -> LLMResponse
```

Directly call Gemini's generate_content interface

**Arguments**:

- `model` - Model name (optional, will override model in configuration)
- `contents` - Request content, can be string, list, or types.Content/types.Part object
- `config` - Generation configuration
- `**kwargs` - Other parameters
  

**Returns**:

- `LLMResponse` - LLM response

<a id="spoon_ai.llm.vlm_provider.base"></a>

# Module `spoon_ai.llm.vlm_provider.base`

<a id="spoon_ai.llm.vlm_provider.base.LLMConfig"></a>

## `LLMConfig` Objects

```python
class LLMConfig(BaseModel)
```

Base class for LLM configuration

<a id="spoon_ai.llm.vlm_provider.base.LLMResponse"></a>

## `LLMResponse` Objects

```python
class LLMResponse(BaseModel)
```

Base class for LLM response

<a id="spoon_ai.llm.vlm_provider.base.LLMResponse.text"></a>

#### `text`

Original text response

<a id="spoon_ai.llm.vlm_provider.base.LLMBase"></a>

## `LLMBase` Objects

```python
class LLMBase(ABC)
```

Base abstract class for LLM, defining interfaces that all LLM providers must implement

<a id="spoon_ai.llm.vlm_provider.base.LLMBase.__init__"></a>

#### `__init__`

```python
def __init__(config_path: str = "config/config.toml",
             config_name: str = "llm")
```

Initialize LLM interface

**Arguments**:

- `config_path` - Configuration file path
- `config_name` - Configuration name

<a id="spoon_ai.llm.vlm_provider.base.LLMBase.chat"></a>

#### `chat`

```python
@abstractmethod
async def chat(messages: List[Message],
               system_msgs: Optional[List[Message]] = None,
               **kwargs) -> LLMResponse
```

Send chat request to LLM and get response

**Arguments**:

- `messages` - List of messages
- `system_msgs` - List of system messages
- `**kwargs` - Other parameters
  

**Returns**:

- `LLMResponse` - LLM response

<a id="spoon_ai.llm.vlm_provider.base.LLMBase.completion"></a>

#### `completion`

```python
@abstractmethod
async def completion(prompt: str, **kwargs) -> LLMResponse
```

Send text completion request to LLM and get response

**Arguments**:

- `prompt` - Prompt text
- `**kwargs` - Other parameters
  

**Returns**:

- `LLMResponse` - LLM response

<a id="spoon_ai.llm.vlm_provider.base.LLMBase.chat_with_tools"></a>

#### `chat_with_tools`

```python
@abstractmethod
async def chat_with_tools(messages: List[Message],
                          system_msgs: Optional[List[Message]] = None,
                          tools: Optional[List[Dict]] = None,
                          tool_choice: Literal["none", "auto",
                                               "required"] = "auto",
                          **kwargs) -> LLMResponse
```

Send chat request that may contain tool calls to LLM and get response

**Arguments**:

- `messages` - List of messages
- `system_msgs` - List of system messages
- `tools` - List of tools
- `tool_choice` - Tool selection mode
- `**kwargs` - Other parameters
  

**Returns**:

- `LLMResponse` - LLM response

<a id="spoon_ai.llm.vlm_provider.base.LLMBase.generate_image"></a>

#### `generate_image`

```python
async def generate_image(prompt: str, **kwargs) -> Union[str, List[str]]
```

Generate image (optional implementation)

**Arguments**:

- `prompt` - Prompt text
- `**kwargs` - Other parameters
  

**Returns**:

  Union[str, List[str]]: Image URL or list of URLs

<a id="spoon_ai.llm.vlm_provider.base.LLMBase.reset_output_handler"></a>

#### `reset_output_handler`

```python
def reset_output_handler()
```

Reset output handler

<a id="spoon_ai.llm.factory"></a>

# Module `spoon_ai.llm.factory`

<a id="spoon_ai.llm.factory.LLMFactory"></a>

## `LLMFactory` Objects

```python
class LLMFactory()
```

LLM factory class, used to create different LLM instances

<a id="spoon_ai.llm.factory.LLMFactory.register"></a>

#### `register`

```python
@classmethod
def register(cls, provider_name: str)
```

Register LLM provider

**Arguments**:

- `provider_name` - Provider name
  

**Returns**:

  Decorator function

<a id="spoon_ai.llm.factory.LLMFactory.create"></a>

#### `create`

```python
@classmethod
def create(cls,
           provider: Optional[str] = None,
           config_path: str = "config/config.toml",
           config_name: str = "llm") -> LLMBase
```

Create LLM instance

**Arguments**:

- `provider` - Provider name, if None, read from configuration file
- `config_path` - Configuration file path
- `config_name` - Configuration name
  

**Returns**:

- `LLMBase` - LLM instance
  

**Raises**:

- `ValueError` - If provider does not exist

<a id="spoon_ai.llm.monitoring"></a>

# Module `spoon_ai.llm.monitoring`

Comprehensive monitoring, debugging, and metrics collection for LLM operations.

<a id="spoon_ai.llm.monitoring.RequestMetrics"></a>

## `RequestMetrics` Objects

```python
@dataclass
class RequestMetrics()
```

Metrics for a single LLM request.

<a id="spoon_ai.llm.monitoring.ProviderStats"></a>

## `ProviderStats` Objects

```python
@dataclass
class ProviderStats()
```

Aggregated statistics for a provider.

<a id="spoon_ai.llm.monitoring.ProviderStats.get"></a>

#### `get`

```python
def get(key: str, default=None)
```

Get attribute value with default fallback for dictionary-like access.

**Arguments**:

- `key` - Attribute name
- `default` - Default value if attribute doesn't exist
  

**Returns**:

  Attribute value or default

<a id="spoon_ai.llm.monitoring.ProviderStats.success_rate"></a>

#### `success_rate`

```python
@property
def success_rate() -> float
```

Calculate success rate as a percentage.

<a id="spoon_ai.llm.monitoring.ProviderStats.avg_response_time"></a>

#### `avg_response_time`

```python
@property
def avg_response_time() -> float
```

Get average response time.

<a id="spoon_ai.llm.monitoring.DebugLogger"></a>

## `DebugLogger` Objects

```python
class DebugLogger()
```

Comprehensive logging and debugging system for LLM operations.

<a id="spoon_ai.llm.monitoring.DebugLogger.__init__"></a>

#### `__init__`

```python
def __init__(max_history: int = 1000, enable_detailed_logging: bool = True)
```

Initialize debug logger.

**Arguments**:

- `max_history` - Maximum number of requests to keep in history
- `enable_detailed_logging` - Whether to enable detailed request/response logging

<a id="spoon_ai.llm.monitoring.DebugLogger.log_request"></a>

#### `log_request`

```python
def log_request(provider: str, method: str, params: Dict[str, Any]) -> str
```

Log request with unique ID.

**Arguments**:

- `provider` - Provider name
- `method` - Method being called (chat, completion, etc.)
- `params` - Request parameters
  

**Returns**:

- `str` - Unique request ID

<a id="spoon_ai.llm.monitoring.DebugLogger.log_response"></a>

#### `log_response`

```python
def log_response(request_id: str, response: LLMResponse,
                 duration: float) -> None
```

Log response with timing information.

**Arguments**:

- `request_id` - Request ID from log_request
- `response` - LLM response object
- `duration` - Request duration in seconds

<a id="spoon_ai.llm.monitoring.DebugLogger.log_error"></a>

#### `log_error`

```python
def log_error(request_id: str, error: Exception, context: Dict[str,
                                                               Any]) -> None
```

Log error with context.

**Arguments**:

- `request_id` - Request ID from log_request
- `error` - Exception that occurred
- `context` - Additional error context

<a id="spoon_ai.llm.monitoring.DebugLogger.log_fallback"></a>

#### `log_fallback`

```python
def log_fallback(from_provider: str, to_provider: str, reason: str) -> None
```

Log provider fallback event.

**Arguments**:

- `from_provider` - Provider that failed
- `to_provider` - Provider being used as fallback
- `reason` - Reason for fallback

<a id="spoon_ai.llm.monitoring.DebugLogger.get_request_history"></a>

#### `get_request_history`

```python
def get_request_history(provider: Optional[str] = None,
                        limit: Optional[int] = None) -> List[RequestMetrics]
```

Get request history.

**Arguments**:

- `provider` - Filter by provider (optional)
- `limit` - Maximum number of requests to return (optional)
  

**Returns**:

- `List[RequestMetrics]` - List of request metrics

<a id="spoon_ai.llm.monitoring.DebugLogger.get_active_requests"></a>

#### `get_active_requests`

```python
def get_active_requests() -> List[RequestMetrics]
```

Get currently active requests.

**Returns**:

- `List[RequestMetrics]` - List of active request metrics

<a id="spoon_ai.llm.monitoring.DebugLogger.clear_history"></a>

#### `clear_history`

```python
def clear_history() -> None
```

Clear request history.

<a id="spoon_ai.llm.monitoring.MetricsCollector"></a>

## `MetricsCollector` Objects

```python
class MetricsCollector()
```

Collects and aggregates performance metrics for LLM providers.

<a id="spoon_ai.llm.monitoring.MetricsCollector.__init__"></a>

#### `__init__`

```python
def __init__(window_size: int = 3600)
```

Initialize metrics collector.

**Arguments**:

- `window_size` - Time window in seconds for rolling metrics

<a id="spoon_ai.llm.monitoring.MetricsCollector.record_request"></a>

#### `record_request`

```python
def record_request(provider: str,
                   method: str,
                   duration: float,
                   success: bool,
                   tokens: int = 0,
                   model: str = '',
                   error: Optional[str] = None) -> None
```

Record request metrics.

**Arguments**:

- `provider` - Provider name
- `method` - Method called
- `duration` - Request duration in seconds
- `success` - Whether request was successful
- `tokens` - Number of tokens used
- `model` - Model name
- `error` - Error message if failed

<a id="spoon_ai.llm.monitoring.MetricsCollector.get_provider_stats"></a>

#### `get_provider_stats`

```python
def get_provider_stats(provider: str) -> Optional[ProviderStats]
```

Get statistics for a specific provider.

**Arguments**:

- `provider` - Provider name
  

**Returns**:

- `Optional[ProviderStats]` - Provider statistics or None if not found

<a id="spoon_ai.llm.monitoring.MetricsCollector.get_all_stats"></a>

#### `get_all_stats`

```python
def get_all_stats() -> Dict[str, ProviderStats]
```

Get statistics for all providers.

**Returns**:

  Dict[str, ProviderStats]: Dictionary of provider statistics

<a id="spoon_ai.llm.monitoring.MetricsCollector.get_rolling_metrics"></a>

#### `get_rolling_metrics`

```python
def get_rolling_metrics(provider: Optional[str] = None,
                        method: Optional[str] = None) -> List[Dict[str, Any]]
```

Get rolling metrics with optional filtering.

**Arguments**:

- `provider` - Filter by provider (optional)
- `method` - Filter by method (optional)
  

**Returns**:

  List[Dict[str, Any]]: List of metrics

<a id="spoon_ai.llm.monitoring.MetricsCollector.get_summary"></a>

#### `get_summary`

```python
def get_summary() -> Dict[str, Any]
```

Get overall summary statistics.

**Returns**:

  Dict[str, Any]: Summary statistics

<a id="spoon_ai.llm.monitoring.MetricsCollector.reset_stats"></a>

#### `reset_stats`

```python
def reset_stats(provider: Optional[str] = None) -> None
```

Reset statistics.

**Arguments**:

- `provider` - Reset specific provider only (optional)

<a id="spoon_ai.llm.monitoring.get_debug_logger"></a>

#### `get_debug_logger`

```python
def get_debug_logger() -> DebugLogger
```

Get global debug logger instance.

**Returns**:

- `DebugLogger` - Global debug logger

<a id="spoon_ai.llm.monitoring.get_metrics_collector"></a>

#### `get_metrics_collector`

```python
def get_metrics_collector() -> MetricsCollector
```

Get global metrics collector instance.

**Returns**:

- `MetricsCollector` - Global metrics collector

<a id="spoon_ai.llm.response_normalizer"></a>

# Module `spoon_ai.llm.response_normalizer`

Response normalizer for ensuring consistent response formats across providers.

<a id="spoon_ai.llm.response_normalizer.ResponseNormalizer"></a>

## `ResponseNormalizer` Objects

```python
class ResponseNormalizer()
```

Normalizes responses from different providers to ensure consistency.

<a id="spoon_ai.llm.response_normalizer.ResponseNormalizer.normalize_response"></a>

#### `normalize_response`

```python
def normalize_response(response: LLMResponse) -> LLMResponse
```

Normalize a response from any provider.

**Arguments**:

- `response` - Raw LLM response
  

**Returns**:

- `LLMResponse` - Normalized response
  

**Raises**:

- `ValidationError` - If response cannot be normalized

<a id="spoon_ai.llm.response_normalizer.ResponseNormalizer.validate_response"></a>

#### `validate_response`

```python
def validate_response(response: LLMResponse) -> bool
```

Validate that a response meets minimum requirements.

**Arguments**:

- `response` - Response to validate
  

**Returns**:

- `bool` - True if response is valid
  

**Raises**:

- `ValidationError` - If response is invalid

<a id="spoon_ai.llm.response_normalizer.ResponseNormalizer.add_provider_mapping"></a>

#### `add_provider_mapping`

```python
def add_provider_mapping(provider_name: str, normalizer_func) -> None
```

Add a custom normalizer for a new provider.

**Arguments**:

- `provider_name` - Name of the provider
- `normalizer_func` - Function that takes and returns LLMResponse

<a id="spoon_ai.llm.response_normalizer.ResponseNormalizer.get_supported_providers"></a>

#### `get_supported_providers`

```python
def get_supported_providers() -> List[str]
```

Get list of providers with custom normalizers.

**Returns**:

- `List[str]` - List of provider names

<a id="spoon_ai.llm.response_normalizer.get_response_normalizer"></a>

#### `get_response_normalizer`

```python
def get_response_normalizer() -> ResponseNormalizer
```

Get global response normalizer instance.

**Returns**:

- `ResponseNormalizer` - Global normalizer instance

<a id="spoon_ai.llm.cache"></a>

# Module `spoon_ai.llm.cache`

Caching system for LLM responses to improve performance.

<a id="spoon_ai.llm.cache.CacheEntry"></a>

## `CacheEntry` Objects

```python
@dataclass
class CacheEntry()
```

Cache entry for LLM responses.

<a id="spoon_ai.llm.cache.CacheEntry.is_expired"></a>

#### `is_expired`

```python
def is_expired(ttl: float) -> bool
```

Check if cache entry is expired.

**Arguments**:

- `ttl` - Time to live in seconds
  

**Returns**:

- `bool` - True if expired

<a id="spoon_ai.llm.cache.CacheEntry.touch"></a>

#### `touch`

```python
def touch() -> None
```

Update access information.

<a id="spoon_ai.llm.cache.LLMResponseCache"></a>

## `LLMResponseCache` Objects

```python
class LLMResponseCache()
```

Cache for LLM responses with TTL and size limits.

<a id="spoon_ai.llm.cache.LLMResponseCache.__init__"></a>

#### `__init__`

```python
def __init__(max_size: int = 1000, default_ttl: float = 3600)
```

Initialize cache.

**Arguments**:

- `max_size` - Maximum number of entries
- `default_ttl` - Default time to live in seconds

<a id="spoon_ai.llm.cache.LLMResponseCache.get"></a>

#### `get`

```python
def get(messages: List[Message],
        provider: str,
        ttl: Optional[float] = None,
        **kwargs) -> Optional[LLMResponse]
```

Get cached response if available and not expired.

**Arguments**:

- `messages` - List of messages
- `provider` - Provider name
- `ttl` - Time to live override
- `**kwargs` - Additional parameters
  

**Returns**:

- `Optional[LLMResponse]` - Cached response if available

<a id="spoon_ai.llm.cache.LLMResponseCache.put"></a>

#### `put`

```python
def put(messages: List[Message], provider: str, response: LLMResponse,
        **kwargs) -> None
```

Store response in cache.

**Arguments**:

- `messages` - List of messages
- `provider` - Provider name
- `response` - Response to cache
- `**kwargs` - Additional parameters

<a id="spoon_ai.llm.cache.LLMResponseCache.clear"></a>

#### `clear`

```python
def clear() -> None
```

Clear all cache entries.

<a id="spoon_ai.llm.cache.LLMResponseCache.get_stats"></a>

#### `get_stats`

```python
def get_stats() -> Dict[str, Any]
```

Get cache statistics.

**Returns**:

  Dict[str, Any]: Cache statistics

<a id="spoon_ai.llm.cache.LLMResponseCache.cleanup_expired"></a>

#### `cleanup_expired`

```python
def cleanup_expired() -> int
```

Remove expired entries.

**Returns**:

- `int` - Number of entries removed

<a id="spoon_ai.llm.cache.get_global_cache"></a>

#### `get_global_cache`

```python
def get_global_cache() -> LLMResponseCache
```

Get global cache instance.

**Returns**:

- `LLMResponseCache` - Global cache instance

<a id="spoon_ai.llm.cache.set_global_cache"></a>

#### `set_global_cache`

```python
def set_global_cache(cache: LLMResponseCache) -> None
```

Set global cache instance.

**Arguments**:

- `cache` - Cache instance to set as global

<a id="spoon_ai.llm.cache.CachedLLMManager"></a>

## `CachedLLMManager` Objects

```python
class CachedLLMManager()
```

LLM Manager wrapper with caching support.

<a id="spoon_ai.llm.cache.CachedLLMManager.__init__"></a>

#### `__init__`

```python
def __init__(manager, cache: Optional[LLMResponseCache] = None)
```

Initialize cached manager.

**Arguments**:

- `manager` - LLM manager instance
- `cache` - Cache instance (optional)

<a id="spoon_ai.llm.cache.CachedLLMManager.chat"></a>

#### `chat`

```python
async def chat(messages: List[Message],
               provider: Optional[str] = None,
               use_cache: bool = True,
               **kwargs) -> LLMResponse
```

Chat with caching support.

**Arguments**:

- `messages` - List of messages
- `provider` - Provider name
- `use_cache` - Whether to use cache
- `**kwargs` - Additional parameters
  

**Returns**:

- `LLMResponse` - Response (cached or fresh)

<a id="spoon_ai.llm.cache.CachedLLMManager.chat_with_tools"></a>

#### `chat_with_tools`

```python
async def chat_with_tools(messages: List[Message],
                          tools: List[Dict],
                          provider: Optional[str] = None,
                          use_cache: bool = True,
                          **kwargs) -> LLMResponse
```

Chat with tools and caching support.

**Arguments**:

- `messages` - List of messages
- `tools` - List of tools
- `provider` - Provider name
- `use_cache` - Whether to use cache
- `**kwargs` - Additional parameters
  

**Returns**:

- `LLMResponse` - Response (cached or fresh)

<a id="spoon_ai.llm.cache.CachedLLMManager.enable_cache"></a>

#### `enable_cache`

```python
def enable_cache() -> None
```

Enable caching.

<a id="spoon_ai.llm.cache.CachedLLMManager.disable_cache"></a>

#### `disable_cache`

```python
def disable_cache() -> None
```

Disable caching.

<a id="spoon_ai.llm.cache.CachedLLMManager.clear_cache"></a>

#### `clear_cache`

```python
def clear_cache() -> None
```

Clear cache.

<a id="spoon_ai.llm.cache.CachedLLMManager.get_cache_stats"></a>

#### `get_cache_stats`

```python
def get_cache_stats() -> Dict[str, Any]
```

Get cache statistics.

**Returns**:

  Dict[str, Any]: Cache statistics

<a id="spoon_ai.llm.cache.CachedLLMManager.__getattr__"></a>

#### `__getattr__`

```python
def __getattr__(name)
```

Delegate other methods to the underlying manager.

<a id="spoon_ai.llm.interface"></a>

# Module `spoon_ai.llm.interface`

LLM Provider Interface - Abstract base class defining the unified interface for all LLM providers.

<a id="spoon_ai.llm.interface.ProviderCapability"></a>

## `ProviderCapability` Objects

```python
class ProviderCapability(Enum)
```

Enumeration of capabilities that LLM providers can support.

<a id="spoon_ai.llm.interface.ProviderMetadata"></a>

## `ProviderMetadata` Objects

```python
@dataclass
class ProviderMetadata()
```

Metadata describing a provider's capabilities and limits.

<a id="spoon_ai.llm.interface.LLMResponse"></a>

## `LLMResponse` Objects

```python
@dataclass
class LLMResponse()
```

Enhanced LLM response with comprehensive metadata and debugging information.

<a id="spoon_ai.llm.interface.LLMProviderInterface"></a>

## `LLMProviderInterface` Objects

```python
class LLMProviderInterface(ABC)
```

Abstract base class defining the unified interface for all LLM providers.

<a id="spoon_ai.llm.interface.LLMProviderInterface.initialize"></a>

#### `initialize`

```python
@abstractmethod
async def initialize(config: Dict[str, Any]) -> None
```

Initialize the provider with configuration.

**Arguments**:

- `config` - Provider-specific configuration dictionary
  

**Raises**:

- `ConfigurationError` - If configuration is invalid

<a id="spoon_ai.llm.interface.LLMProviderInterface.chat"></a>

#### `chat`

```python
@abstractmethod
async def chat(messages: List[Message], **kwargs) -> LLMResponse
```

Send chat request to the provider.

**Arguments**:

- `messages` - List of conversation messages
- `**kwargs` - Additional provider-specific parameters
  

**Returns**:

- `LLMResponse` - Standardized response object
  

**Raises**:

- `ProviderError` - If the request fails

<a id="spoon_ai.llm.interface.LLMProviderInterface.chat_stream"></a>

#### `chat_stream`

```python
@abstractmethod
async def chat_stream(messages: List[Message],
                      callbacks: Optional[List[BaseCallbackHandler]] = None,
                      **kwargs) -> AsyncIterator[LLMResponseChunk]
```

Send streaming chat request to the provider with callback support.

**Arguments**:

- `messages` - List of conversation messages
- `callbacks` - Optional list of callback handlers for real-time events
- `**kwargs` - Additional provider-specific parameters
  

**Yields**:

- `LLMResponseChunk` - Structured streaming response chunks
  

**Raises**:

- `ProviderError` - If the request fails

<a id="spoon_ai.llm.interface.LLMProviderInterface.completion"></a>

#### `completion`

```python
@abstractmethod
async def completion(prompt: str, **kwargs) -> LLMResponse
```

Send completion request to the provider.

**Arguments**:

- `prompt` - Text prompt for completion
- `**kwargs` - Additional provider-specific parameters
  

**Returns**:

- `LLMResponse` - Standardized response object
  

**Raises**:

- `ProviderError` - If the request fails

<a id="spoon_ai.llm.interface.LLMProviderInterface.chat_with_tools"></a>

#### `chat_with_tools`

```python
@abstractmethod
async def chat_with_tools(messages: List[Message], tools: List[Dict],
                          **kwargs) -> LLMResponse
```

Send chat request with tool support.

**Arguments**:

- `messages` - List of conversation messages
- `tools` - List of available tools
- `**kwargs` - Additional provider-specific parameters
  

**Returns**:

- `LLMResponse` - Standardized response object with potential tool calls
  

**Raises**:

- `ProviderError` - If the request fails

<a id="spoon_ai.llm.interface.LLMProviderInterface.get_metadata"></a>

#### `get_metadata`

```python
@abstractmethod
def get_metadata() -> ProviderMetadata
```

Get provider metadata and capabilities.

**Returns**:

- `ProviderMetadata` - Provider information and capabilities

<a id="spoon_ai.llm.interface.LLMProviderInterface.health_check"></a>

#### `health_check`

```python
@abstractmethod
async def health_check() -> bool
```

Check if provider is healthy and available.

**Returns**:

- `bool` - True if provider is healthy, False otherwise

<a id="spoon_ai.llm.interface.LLMProviderInterface.cleanup"></a>

#### `cleanup`

```python
@abstractmethod
async def cleanup() -> None
```

Cleanup resources and connections.

This method should be called when the provider is no longer needed.

<a id="spoon_ai.llm.providers.deepseek_provider"></a>

# Module `spoon_ai.llm.providers.deepseek_provider`

DeepSeek Provider implementation for the unified LLM interface.
DeepSeek provides access to their models through an OpenAI-compatible API.

<a id="spoon_ai.llm.providers.deepseek_provider.DeepSeekProvider"></a>

## `DeepSeekProvider` Objects

```python
@register_provider("deepseek", [
    ProviderCapability.CHAT,
    ProviderCapability.COMPLETION,
    ProviderCapability.TOOLS,
    ProviderCapability.STREAMING
])
class DeepSeekProvider(OpenAICompatibleProvider)
```

DeepSeek provider implementation using OpenAI-compatible API.

<a id="spoon_ai.llm.providers.deepseek_provider.DeepSeekProvider.get_metadata"></a>

#### `get_metadata`

```python
def get_metadata() -> ProviderMetadata
```

Get DeepSeek provider metadata.

<a id="spoon_ai.llm.providers.gemini_provider"></a>

# Module `spoon_ai.llm.providers.gemini_provider`

Gemini Provider implementation for the unified LLM interface.

<a id="spoon_ai.llm.providers.gemini_provider.GeminiProvider"></a>

## `GeminiProvider` Objects

```python
@register_provider("gemini", [
    ProviderCapability.CHAT,
    ProviderCapability.COMPLETION,
    ProviderCapability.STREAMING,
    ProviderCapability.TOOLS,
    ProviderCapability.IMAGE_GENERATION,
    ProviderCapability.VISION
])
class GeminiProvider(LLMProviderInterface)
```

Gemini provider implementation.

<a id="spoon_ai.llm.providers.gemini_provider.GeminiProvider.initialize"></a>

#### `initialize`

```python
async def initialize(config: Dict[str, Any]) -> None
```

Initialize the Gemini provider with configuration.

<a id="spoon_ai.llm.providers.gemini_provider.GeminiProvider.chat"></a>

#### `chat`

```python
async def chat(messages: List[Message], **kwargs) -> LLMResponse
```

Send chat request to Gemini.

<a id="spoon_ai.llm.providers.gemini_provider.GeminiProvider.chat_stream"></a>

#### `chat_stream`

```python
async def chat_stream(messages: List[Message],
                      callbacks: Optional[List] = None,
                      **kwargs) -> AsyncIterator[LLMResponseChunk]
```

Send streaming chat request to Gemini with callback support.

**Yields**:

- `LLMResponseChunk` - Structured streaming response chunks

<a id="spoon_ai.llm.providers.gemini_provider.GeminiProvider.completion"></a>

#### `completion`

```python
async def completion(prompt: str, **kwargs) -> LLMResponse
```

Send completion request to Gemini.

<a id="spoon_ai.llm.providers.gemini_provider.GeminiProvider.chat_with_tools"></a>

#### `chat_with_tools`

```python
async def chat_with_tools(messages: List[Message], tools: List[Dict],
                          **kwargs) -> LLMResponse
```

Send chat request with tools to Gemini using native function calling.

<a id="spoon_ai.llm.providers.gemini_provider.GeminiProvider.get_metadata"></a>

#### `get_metadata`

```python
def get_metadata() -> ProviderMetadata
```

Get Gemini provider metadata.

<a id="spoon_ai.llm.providers.gemini_provider.GeminiProvider.health_check"></a>

#### `health_check`

```python
async def health_check() -> bool
```

Check if Gemini provider is healthy.

<a id="spoon_ai.llm.providers.gemini_provider.GeminiProvider.cleanup"></a>

#### `cleanup`

```python
async def cleanup() -> None
```

Cleanup Gemini provider resources.

<a id="spoon_ai.llm.providers.openai_compatible_provider"></a>

# Module `spoon_ai.llm.providers.openai_compatible_provider`

OpenAI Compatible Provider base class for providers that use OpenAI-compatible APIs.
This includes OpenAI, OpenRouter, DeepSeek, and other providers with similar interfaces.

<a id="spoon_ai.llm.providers.openai_compatible_provider.OpenAICompatibleProvider"></a>

## `OpenAICompatibleProvider` Objects

```python
class OpenAICompatibleProvider(LLMProviderInterface)
```

Base class for OpenAI-compatible providers.

<a id="spoon_ai.llm.providers.openai_compatible_provider.OpenAICompatibleProvider.get_provider_name"></a>

#### `get_provider_name`

```python
def get_provider_name() -> str
```

Get the provider name. Should be overridden by subclasses.

<a id="spoon_ai.llm.providers.openai_compatible_provider.OpenAICompatibleProvider.get_default_base_url"></a>

#### `get_default_base_url`

```python
def get_default_base_url() -> str
```

Get the default base URL. Should be overridden by subclasses.

<a id="spoon_ai.llm.providers.openai_compatible_provider.OpenAICompatibleProvider.get_default_model"></a>

#### `get_default_model`

```python
def get_default_model() -> str
```

Get the default model. Should be overridden by subclasses.

<a id="spoon_ai.llm.providers.openai_compatible_provider.OpenAICompatibleProvider.get_additional_headers"></a>

#### `get_additional_headers`

```python
def get_additional_headers(config: Dict[str, Any]) -> Dict[str, str]
```

Get additional headers for the provider. Can be overridden by subclasses.

<a id="spoon_ai.llm.providers.openai_compatible_provider.OpenAICompatibleProvider.initialize"></a>

#### `initialize`

```python
async def initialize(config: Dict[str, Any]) -> None
```

Initialize the provider with configuration.

<a id="spoon_ai.llm.providers.openai_compatible_provider.OpenAICompatibleProvider.chat"></a>

#### `chat`

```python
async def chat(messages: List[Message], **kwargs) -> LLMResponse
```

Send chat request to the provider.

<a id="spoon_ai.llm.providers.openai_compatible_provider.OpenAICompatibleProvider.chat_stream"></a>

#### `chat_stream`

```python
async def chat_stream(messages: List[Message],
                      callbacks: Optional[List[BaseCallbackHandler]] = None,
                      **kwargs) -> AsyncIterator[LLMResponseChunk]
```

Send streaming chat request with full callback support.

**Yields**:

- `LLMResponseChunk` - Structured streaming response chunks

<a id="spoon_ai.llm.providers.openai_compatible_provider.OpenAICompatibleProvider.completion"></a>

#### `completion`

```python
async def completion(prompt: str, **kwargs) -> LLMResponse
```

Send completion request to the provider.

<a id="spoon_ai.llm.providers.openai_compatible_provider.OpenAICompatibleProvider.chat_with_tools"></a>

#### `chat_with_tools`

```python
async def chat_with_tools(messages: List[Message], tools: List[Dict],
                          **kwargs) -> LLMResponse
```

Send chat request with tools to the provider.

<a id="spoon_ai.llm.providers.openai_compatible_provider.OpenAICompatibleProvider.get_metadata"></a>

#### `get_metadata`

```python
def get_metadata() -> ProviderMetadata
```

Get provider metadata. Should be overridden by subclasses.

<a id="spoon_ai.llm.providers.openai_compatible_provider.OpenAICompatibleProvider.health_check"></a>

#### `health_check`

```python
async def health_check() -> bool
```

Check if provider is healthy.

<a id="spoon_ai.llm.providers.openai_compatible_provider.OpenAICompatibleProvider.cleanup"></a>

#### `cleanup`

```python
async def cleanup() -> None
```

Cleanup provider resources.

<a id="spoon_ai.llm.providers.openai_provider"></a>

# Module `spoon_ai.llm.providers.openai_provider`

OpenAI Provider implementation for the unified LLM interface.

<a id="spoon_ai.llm.providers.openai_provider.OpenAIProvider"></a>

## `OpenAIProvider` Objects

```python
@register_provider("openai", [
    ProviderCapability.CHAT,
    ProviderCapability.COMPLETION,
    ProviderCapability.TOOLS,
    ProviderCapability.STREAMING
])
class OpenAIProvider(OpenAICompatibleProvider)
```

OpenAI provider implementation.

<a id="spoon_ai.llm.providers.openai_provider.OpenAIProvider.get_metadata"></a>

#### `get_metadata`

```python
def get_metadata() -> ProviderMetadata
```

Get OpenAI provider metadata.

<a id="spoon_ai.llm.providers.anthropic_provider"></a>

# Module `spoon_ai.llm.providers.anthropic_provider`

Anthropic Provider implementation for the unified LLM interface.

<a id="spoon_ai.llm.providers.anthropic_provider.AnthropicProvider"></a>

## `AnthropicProvider` Objects

```python
@register_provider("anthropic", [
    ProviderCapability.CHAT,
    ProviderCapability.COMPLETION,
    ProviderCapability.TOOLS,
    ProviderCapability.STREAMING
])
class AnthropicProvider(LLMProviderInterface)
```

Anthropic provider implementation.

<a id="spoon_ai.llm.providers.anthropic_provider.AnthropicProvider.initialize"></a>

#### `initialize`

```python
async def initialize(config: Dict[str, Any]) -> None
```

Initialize the Anthropic provider with configuration.

<a id="spoon_ai.llm.providers.anthropic_provider.AnthropicProvider.get_cache_metrics"></a>

#### `get_cache_metrics`

```python
def get_cache_metrics() -> Dict[str, int]
```

Get current cache performance metrics.

<a id="spoon_ai.llm.providers.anthropic_provider.AnthropicProvider.chat"></a>

#### `chat`

```python
async def chat(messages: List[Message], **kwargs) -> LLMResponse
```

Send chat request to Anthropic.

<a id="spoon_ai.llm.providers.anthropic_provider.AnthropicProvider.chat_stream"></a>

#### `chat_stream`

```python
async def chat_stream(messages: List[Message],
                      callbacks: Optional[List] = None,
                      **kwargs) -> AsyncIterator[LLMResponseChunk]
```

Send streaming chat request to Anthropic with callback support.

**Yields**:

- `LLMResponseChunk` - Structured streaming response chunks

<a id="spoon_ai.llm.providers.anthropic_provider.AnthropicProvider.completion"></a>

#### `completion`

```python
async def completion(prompt: str, **kwargs) -> LLMResponse
```

Send completion request to Anthropic.

<a id="spoon_ai.llm.providers.anthropic_provider.AnthropicProvider.chat_with_tools"></a>

#### `chat_with_tools`

```python
async def chat_with_tools(messages: List[Message], tools: List[Dict],
                          **kwargs) -> LLMResponse
```

Send chat request with tools to Anthropic.

<a id="spoon_ai.llm.providers.anthropic_provider.AnthropicProvider.get_metadata"></a>

#### `get_metadata`

```python
def get_metadata() -> ProviderMetadata
```

Get Anthropic provider metadata.

<a id="spoon_ai.llm.providers.anthropic_provider.AnthropicProvider.health_check"></a>

#### `health_check`

```python
async def health_check() -> bool
```

Check if Anthropic provider is healthy.

<a id="spoon_ai.llm.providers.anthropic_provider.AnthropicProvider.cleanup"></a>

#### `cleanup`

```python
async def cleanup() -> None
```

Cleanup Anthropic provider resources.

<a id="spoon_ai.llm.providers.openrouter_provider"></a>

# Module `spoon_ai.llm.providers.openrouter_provider`

OpenRouter Provider implementation for the unified LLM interface.
OpenRouter provides access to multiple LLM models through a unified API compatible with OpenAI.

<a id="spoon_ai.llm.providers.openrouter_provider.OpenRouterProvider"></a>

## `OpenRouterProvider` Objects

```python
@register_provider("openrouter", [
    ProviderCapability.CHAT,
    ProviderCapability.COMPLETION,
    ProviderCapability.TOOLS,
    ProviderCapability.STREAMING
])
class OpenRouterProvider(OpenAICompatibleProvider)
```

OpenRouter provider implementation using OpenAI-compatible API.

<a id="spoon_ai.llm.providers.openrouter_provider.OpenRouterProvider.get_additional_headers"></a>

#### `get_additional_headers`

```python
def get_additional_headers(config: Dict[str, Any]) -> Dict[str, str]
```

Get OpenRouter-specific headers.

<a id="spoon_ai.llm.providers.openrouter_provider.OpenRouterProvider.get_metadata"></a>

#### `get_metadata`

```python
def get_metadata() -> ProviderMetadata
```

Get OpenRouter provider metadata.

<a id="spoon_ai.llm.providers"></a>

# Module `spoon_ai.llm.providers`

LLM Provider implementations.

<a id="spoon_ai.llm.manager"></a>

# Module `spoon_ai.llm.manager`

LLM Manager - Central orchestrator for managing providers, fallback, and load balancing.

<a id="spoon_ai.llm.manager.ProviderState"></a>

## `ProviderState` Objects

```python
@dataclass
class ProviderState()
```

Track provider initialization and health state.

<a id="spoon_ai.llm.manager.ProviderState.can_retry_initialization"></a>

#### `can_retry_initialization`

```python
def can_retry_initialization() -> bool
```

Check if provider initialization can be retried.

<a id="spoon_ai.llm.manager.ProviderState.record_initialization_failure"></a>

#### `record_initialization_failure`

```python
def record_initialization_failure(error: Exception) -> None
```

Record initialization failure with exponential backoff.

<a id="spoon_ai.llm.manager.ProviderState.record_initialization_success"></a>

#### `record_initialization_success`

```python
def record_initialization_success() -> None
```

Record successful initialization.

<a id="spoon_ai.llm.manager.FallbackStrategy"></a>

## `FallbackStrategy` Objects

```python
class FallbackStrategy()
```

Handles fallback logic between providers.

<a id="spoon_ai.llm.manager.FallbackStrategy.execute_with_fallback"></a>

#### `execute_with_fallback`

```python
async def execute_with_fallback(providers: List[str], operation, *args,
                                **kwargs) -> LLMResponse
```

Execute operation with fallback chain.

**Arguments**:

- `providers` - List of provider names in fallback order
- `operation` - Async operation to execute
  *args, **kwargs: Arguments for the operation
  

**Returns**:

- `LLMResponse` - Response from successful provider
  

**Raises**:

- `ProviderError` - If all providers fail

<a id="spoon_ai.llm.manager.LoadBalancer"></a>

## `LoadBalancer` Objects

```python
class LoadBalancer()
```

Handles load balancing between multiple provider instances.

<a id="spoon_ai.llm.manager.LoadBalancer.select_provider"></a>

#### `select_provider`

```python
def select_provider(providers: List[str],
                    strategy: str = "round_robin") -> str
```

Select a provider based on load balancing strategy.

**Arguments**:

- `providers` - List of available providers
- `strategy` - Load balancing strategy ('round_robin', 'weighted', 'random')
  

**Returns**:

- `str` - Selected provider name

<a id="spoon_ai.llm.manager.LoadBalancer.update_provider_health"></a>

#### `update_provider_health`

```python
def update_provider_health(provider: str, is_healthy: bool) -> None
```

Update provider health status.

<a id="spoon_ai.llm.manager.LoadBalancer.set_provider_weight"></a>

#### `set_provider_weight`

```python
def set_provider_weight(provider: str, weight: float) -> None
```

Set provider weight for weighted load balancing.

<a id="spoon_ai.llm.manager.LLMManager"></a>

## `LLMManager` Objects

```python
class LLMManager()
```

Central orchestrator for LLM providers with fallback and load balancing.

<a id="spoon_ai.llm.manager.LLMManager.__init__"></a>

#### `__init__`

```python
def __init__(config_manager: Optional[ConfigurationManager] = None,
             debug_logger: Optional[DebugLogger] = None,
             metrics_collector: Optional[MetricsCollector] = None,
             response_normalizer: Optional[ResponseNormalizer] = None,
             registry: Optional[LLMProviderRegistry] = None)
```

Initialize LLM Manager with enhanced provider state tracking.

<a id="spoon_ai.llm.manager.LLMManager.cleanup"></a>

#### `cleanup`

```python
async def cleanup() -> None
```

Enhanced cleanup with proper resource management.

<a id="spoon_ai.llm.manager.LLMManager.get_provider_status"></a>

#### `get_provider_status`

```python
def get_provider_status() -> Dict[str, Dict[str, Any]]
```

Get detailed status of all providers.

<a id="spoon_ai.llm.manager.LLMManager.reset_provider"></a>

#### `reset_provider`

```python
async def reset_provider(provider_name: str) -> bool
```

Reset a provider's state and force reinitialization.

**Arguments**:

- `provider_name` - Name of provider to reset
  

**Returns**:

- `bool` - True if reset successful

<a id="spoon_ai.llm.manager.LLMManager.chat"></a>

#### `chat`

```python
async def chat(messages: List[Message],
               provider: Optional[str] = None,
               **kwargs) -> LLMResponse
```

Send chat request with automatic provider selection and fallback.

**Arguments**:

- `messages` - List of conversation messages
- `provider` - Specific provider to use (optional)
- `**kwargs` - Additional parameters
  

**Returns**:

- `LLMResponse` - Normalized response

<a id="spoon_ai.llm.manager.LLMManager.chat_stream"></a>

#### `chat_stream`

```python
async def chat_stream(messages: List[Message],
                      provider: Optional[str] = None,
                      callbacks: Optional[List[BaseCallbackHandler]] = None,
                      **kwargs) -> AsyncGenerator[LLMResponseChunk, None]
```

Send streaming chat request with callback support.

**Arguments**:

- `messages` - List of conversation messages
- `provider` - Specific provider to use (optional)
- `callbacks` - Optional callback handlers for monitoring
- `**kwargs` - Additional parameters
  

**Yields**:

- `LLMResponseChunk` - Structured streaming response chunks

<a id="spoon_ai.llm.manager.LLMManager.completion"></a>

#### `completion`

```python
async def completion(prompt: str,
                     provider: Optional[str] = None,
                     **kwargs) -> LLMResponse
```

Send completion request.

**Arguments**:

- `prompt` - Text prompt
- `provider` - Specific provider to use (optional)
- `**kwargs` - Additional parameters
  

**Returns**:

- `LLMResponse` - Normalized response

<a id="spoon_ai.llm.manager.LLMManager.chat_with_tools"></a>

#### `chat_with_tools`

```python
async def chat_with_tools(messages: List[Message],
                          tools: List[Dict],
                          provider: Optional[str] = None,
                          **kwargs) -> LLMResponse
```

Send tool-enabled chat request.

**Arguments**:

- `messages` - List of conversation messages
- `tools` - List of available tools
- `provider` - Specific provider to use (optional)
- `**kwargs` - Additional parameters
  

**Returns**:

- `LLMResponse` - Normalized response

<a id="spoon_ai.llm.manager.LLMManager.set_fallback_chain"></a>

#### `set_fallback_chain`

```python
def set_fallback_chain(providers: List[str]) -> None
```

Set fallback provider chain.

**Arguments**:

- `providers` - List of provider names in fallback order

<a id="spoon_ai.llm.manager.LLMManager.enable_load_balancing"></a>

#### `enable_load_balancing`

```python
def enable_load_balancing(strategy: str = "round_robin") -> None
```

Enable load balancing with specified strategy.

**Arguments**:

- `strategy` - Load balancing strategy ('round_robin', 'weighted', 'random')

<a id="spoon_ai.llm.manager.LLMManager.disable_load_balancing"></a>

#### `disable_load_balancing`

```python
def disable_load_balancing() -> None
```

Disable load balancing.

<a id="spoon_ai.llm.manager.LLMManager.health_check_all"></a>

#### `health_check_all`

```python
async def health_check_all() -> Dict[str, bool]
```

Check health of all registered providers.

**Returns**:

  Dict[str, bool]: Provider health status

<a id="spoon_ai.llm.manager.LLMManager.cleanup"></a>

#### `cleanup`

```python
async def cleanup() -> None
```

Cleanup all provider resources.

<a id="spoon_ai.llm.manager.LLMManager.get_stats"></a>

#### `get_stats`

```python
def get_stats() -> Dict[str, Any]
```

Get comprehensive statistics.

**Returns**:

  Dict[str, Any]: Manager and provider statistics

<a id="spoon_ai.llm.manager.get_llm_manager"></a>

#### `get_llm_manager`

```python
def get_llm_manager() -> LLMManager
```

Get global LLM manager instance.

**Returns**:

- `LLMManager` - Global manager instance

<a id="spoon_ai.llm.manager.set_llm_manager"></a>

#### `set_llm_manager`

```python
def set_llm_manager(manager: LLMManager) -> None
```

Set global LLM manager instance.

**Arguments**:

- `manager` - Manager instance to set as global

<a id="spoon_ai.llm.registry"></a>

# Module `spoon_ai.llm.registry`

LLM Provider Registry for dynamic provider registration and discovery.

<a id="spoon_ai.llm.registry.LLMProviderRegistry"></a>

## `LLMProviderRegistry` Objects

```python
class LLMProviderRegistry()
```

Registry for managing LLM provider classes and instances.

<a id="spoon_ai.llm.registry.LLMProviderRegistry.register"></a>

#### `register`

```python
def register(name: str, provider_class: Type[LLMProviderInterface]) -> None
```

Register a provider class.

**Arguments**:

- `name` - Unique provider name
- `provider_class` - Provider class implementing LLMProviderInterface
  

**Raises**:

- `ConfigurationError` - If provider name already exists or class is invalid

<a id="spoon_ai.llm.registry.LLMProviderRegistry.get_provider"></a>

#### `get_provider`

```python
def get_provider(
        name: str,
        config: Optional[Dict[str, Any]] = None) -> LLMProviderInterface
```

Get or create provider instance.

**Arguments**:

- `name` - Provider name
- `config` - Provider configuration (optional if already configured)
  

**Returns**:

- `LLMProviderInterface` - Provider instance
  

**Raises**:

- `ConfigurationError` - If provider not found or configuration invalid

<a id="spoon_ai.llm.registry.LLMProviderRegistry.list_providers"></a>

#### `list_providers`

```python
def list_providers() -> List[str]
```

List all registered provider names.

**Returns**:

- `List[str]` - List of provider names

<a id="spoon_ai.llm.registry.LLMProviderRegistry.get_capabilities"></a>

#### `get_capabilities`

```python
def get_capabilities(name: str) -> List[ProviderCapability]
```

Get provider capabilities.

**Arguments**:

- `name` - Provider name
  

**Returns**:

- `List[ProviderCapability]` - List of supported capabilities
  

**Raises**:

- `ConfigurationError` - If provider not found

<a id="spoon_ai.llm.registry.LLMProviderRegistry.is_registered"></a>

#### `is_registered`

```python
def is_registered(name: str) -> bool
```

Check if a provider is registered.

**Arguments**:

- `name` - Provider name
  

**Returns**:

- `bool` - True if provider is registered

<a id="spoon_ai.llm.registry.LLMProviderRegistry.unregister"></a>

#### `unregister`

```python
def unregister(name: str) -> None
```

Unregister a provider.

**Arguments**:

- `name` - Provider name

<a id="spoon_ai.llm.registry.LLMProviderRegistry.clear"></a>

#### `clear`

```python
def clear() -> None
```

Clear all registered providers and instances.

<a id="spoon_ai.llm.registry.register_provider"></a>

#### `register_provider`

```python
def register_provider(name: str,
                      capabilities: Optional[List[ProviderCapability]] = None)
```

Decorator for automatic provider registration.

**Arguments**:

- `name` - Provider name
- `capabilities` - List of supported capabilities (optional)
  

**Returns**:

  Decorator function

<a id="spoon_ai.llm.registry.get_global_registry"></a>

#### `get_global_registry`

```python
def get_global_registry() -> LLMProviderRegistry
```

Get the global provider registry instance.

**Returns**:

- `LLMProviderRegistry` - Global registry instance

<a id="spoon_ai.llm.config"></a>

# Module `spoon_ai.llm.config`

Configuration management for LLM providers using environment variables.

<a id="spoon_ai.llm.config.ProviderConfig"></a>

## `ProviderConfig` Objects

```python
@dataclass
class ProviderConfig()
```

Configuration for a specific LLM provider.

<a id="spoon_ai.llm.config.ProviderConfig.__post_init__"></a>

#### `__post_init__`

```python
def __post_init__()
```

Validate configuration after initialization.

<a id="spoon_ai.llm.config.ProviderConfig.model_dump"></a>

#### `model_dump`

```python
def model_dump() -> Dict[str, Any]
```

Convert the configuration to a dictionary.

**Returns**:

  Dict[str, Any]: Configuration as dictionary

<a id="spoon_ai.llm.config.ConfigurationManager"></a>

## `ConfigurationManager` Objects

```python
class ConfigurationManager()
```

Manages environment-driven configuration for LLM providers.

<a id="spoon_ai.llm.config.ConfigurationManager.__init__"></a>

#### `__init__`

```python
def __init__() -> None
```

Initialize configuration manager and load environment variables.

<a id="spoon_ai.llm.config.ConfigurationManager.load_provider_config"></a>

#### `load_provider_config`

```python
def load_provider_config(provider_name: str) -> ProviderConfig
```

Load and validate provider configuration.

**Arguments**:

- `provider_name` - Name of the provider
  

**Returns**:

- `ProviderConfig` - Validated provider configuration
  

**Raises**:

- `ConfigurationError` - If configuration is invalid or missing

<a id="spoon_ai.llm.config.ConfigurationManager.validate_config"></a>

#### `validate_config`

```python
def validate_config(config: ProviderConfig) -> bool
```

Validate provider configuration.

**Arguments**:

- `config` - Provider configuration to validate
  

**Returns**:

- `bool` - True if configuration is valid
  

**Raises**:

- `ConfigurationError` - If configuration is invalid

<a id="spoon_ai.llm.config.ConfigurationManager.get_default_provider"></a>

#### `get_default_provider`

```python
def get_default_provider() -> str
```

Get default provider from configuration with intelligent selection.

**Returns**:

- `str` - Default provider name

<a id="spoon_ai.llm.config.ConfigurationManager.get_fallback_chain"></a>

#### `get_fallback_chain`

```python
def get_fallback_chain() -> List[str]
```

Get fallback chain from configuration.

**Returns**:

- `List[str]` - List of provider names in fallback order

<a id="spoon_ai.llm.config.ConfigurationManager.list_configured_providers"></a>

#### `list_configured_providers`

```python
def list_configured_providers() -> List[str]
```

List all configured providers.

**Returns**:

- `List[str]` - List of provider names that have configuration

<a id="spoon_ai.llm.config.ConfigurationManager.get_available_providers_by_priority"></a>

#### `get_available_providers_by_priority`

```python
def get_available_providers_by_priority() -> List[str]
```

Get available providers ordered by priority and quality.

**Returns**:

- `List[str]` - List of available provider names in priority order

<a id="spoon_ai.llm.config.ConfigurationManager.get_provider_info"></a>

#### `get_provider_info`

```python
def get_provider_info() -> Dict[str, Dict[str, Any]]
```

Get information about all providers and their availability.

**Returns**:

  Dict[str, Dict[str, Any]]: Provider information including availability

<a id="spoon_ai.llm.config.ConfigurationManager.reload_config"></a>

#### `reload_config`

```python
def reload_config() -> None
```

Reload configuration from file.

<a id="spoon_ai.llm"></a>

# Module `spoon_ai.llm`

Unified LLM infrastructure package.

This package provides a unified interface for working with different LLM providers,
including comprehensive configuration management, monitoring, and error handling.

<a id="spoon_ai.llm.errors"></a>

# Module `spoon_ai.llm.errors`

Standardized error hierarchy for LLM operations.

<a id="spoon_ai.llm.errors.LLMError"></a>

## `LLMError` Objects

```python
class LLMError(Exception)
```

Base exception for all LLM-related errors.

<a id="spoon_ai.llm.errors.ProviderError"></a>

## `ProviderError` Objects

```python
class ProviderError(LLMError)
```

Provider-specific error with detailed context.

<a id="spoon_ai.llm.errors.ConfigurationError"></a>

## `ConfigurationError` Objects

```python
class ConfigurationError(LLMError)
```

Configuration validation or loading error.

<a id="spoon_ai.llm.errors.RateLimitError"></a>

## `RateLimitError` Objects

```python
class RateLimitError(ProviderError)
```

Rate limit exceeded error.

<a id="spoon_ai.llm.errors.AuthenticationError"></a>

## `AuthenticationError` Objects

```python
class AuthenticationError(ProviderError)
```

Authentication failed error.

<a id="spoon_ai.llm.errors.ModelNotFoundError"></a>

## `ModelNotFoundError` Objects

```python
class ModelNotFoundError(ProviderError)
```

Model not found or not available error.

<a id="spoon_ai.llm.errors.TokenLimitError"></a>

## `TokenLimitError` Objects

```python
class TokenLimitError(ProviderError)
```

Token limit exceeded error.

<a id="spoon_ai.llm.errors.NetworkError"></a>

## `NetworkError` Objects

```python
class NetworkError(ProviderError)
```

Network connectivity or timeout error.

<a id="spoon_ai.llm.errors.ProviderUnavailableError"></a>

## `ProviderUnavailableError` Objects

```python
class ProviderUnavailableError(ProviderError)
```

Provider service unavailable error.

<a id="spoon_ai.llm.errors.ValidationError"></a>

## `ValidationError` Objects

```python
class ValidationError(LLMError)
```

Input validation error.

<a id="spoon_ai.llm.base"></a>

# Module `spoon_ai.llm.base`

<a id="spoon_ai.llm.base.LLMBase"></a>

## `LLMBase` Objects

```python
class LLMBase(ABC)
```

Base abstract class for LLM, defining interfaces that all LLM providers must implement

<a id="spoon_ai.llm.base.LLMBase.__init__"></a>

#### `__init__`

```python
def __init__(config_path: str = "config/config.toml",
             config_name: str = "llm")
```

Initialize LLM interface

**Arguments**:

- `config_path` - Configuration file path
- `config_name` - Configuration name

<a id="spoon_ai.llm.base.LLMBase.chat"></a>

#### `chat`

```python
@abstractmethod
async def chat(messages: List[Message],
               system_msgs: Optional[List[Message]] = None,
               **kwargs) -> LLMResponse
```

Send chat request to LLM and get response

**Arguments**:

- `messages` - List of messages
- `system_msgs` - List of system messages
- `**kwargs` - Other parameters
  

**Returns**:

- `LLMResponse` - LLM response

<a id="spoon_ai.llm.base.LLMBase.completion"></a>

#### `completion`

```python
@abstractmethod
async def completion(prompt: str, **kwargs) -> LLMResponse
```

Send text completion request to LLM and get response

**Arguments**:

- `prompt` - Prompt text
- `**kwargs` - Other parameters
  

**Returns**:

- `LLMResponse` - LLM response

<a id="spoon_ai.llm.base.LLMBase.chat_with_tools"></a>

#### `chat_with_tools`

```python
@abstractmethod
async def chat_with_tools(messages: List[Message],
                          system_msgs: Optional[List[Message]] = None,
                          tools: Optional[List[Dict]] = None,
                          tool_choice: Literal["none", "auto",
                                               "required"] = "auto",
                          **kwargs) -> LLMResponse
```

Send chat request that may contain tool calls to LLM and get response

**Arguments**:

- `messages` - List of messages
- `system_msgs` - List of system messages
- `tools` - List of tools
- `tool_choice` - Tool selection mode
- `**kwargs` - Other parameters
  

**Returns**:

- `LLMResponse` - LLM response

<a id="spoon_ai.llm.base.LLMBase.generate_image"></a>

#### `generate_image`

```python
async def generate_image(prompt: str, **kwargs) -> Union[str, List[str]]
```

Generate image (optional implementation)

**Arguments**:

- `prompt` - Prompt text
- `**kwargs` - Other parameters
  

**Returns**:

  Union[str, List[str]]: Image URL or list of URLs

<a id="spoon_ai.llm.base.LLMBase.reset_output_handler"></a>

#### `reset_output_handler`

```python
def reset_output_handler()
```

Reset output handler

<a id="spoon_ai.utils.utils"></a>

# Module `spoon_ai.utils.utils`

<a id="spoon_ai.utils.config_manager"></a>

# Module `spoon_ai.utils.config_manager`

<a id="spoon_ai.utils.config_manager.ConfigManager"></a>

## `ConfigManager` Objects

```python
class ConfigManager()
```

Environment-based configuration helper for core usage.

<a id="spoon_ai.utils.config_manager.ConfigManager.__init__"></a>

#### `__init__`

```python
def __init__() -> None
```

Initialize manager with environment-backed cache.

<a id="spoon_ai.utils.config_manager.ConfigManager.refresh"></a>

#### `refresh`

```python
def refresh() -> None
```

Reload configuration snapshot from environment variables.

<a id="spoon_ai.utils.config_manager.ConfigManager.get"></a>

#### `get`

```python
def get(key: str, default: Any = None) -> Any
```

Get configuration item from environment snapshot.

<a id="spoon_ai.utils.config_manager.ConfigManager.set"></a>

#### `set`

```python
def set(key: str, value: Any) -> None
```

Set configuration item by exporting to environment variables.

<a id="spoon_ai.utils.config_manager.ConfigManager.list_config"></a>

#### `list_config`

```python
def list_config() -> Dict[str, Any]
```

List configuration snapshot without persisting secrets.

<a id="spoon_ai.utils.config_manager.ConfigManager.get_api_key"></a>

#### `get_api_key`

```python
def get_api_key(provider: str) -> Optional[str]
```

Get API key for specified provider with environment priority.

<a id="spoon_ai.utils.config_manager.ConfigManager.set_api_key"></a>

#### `set_api_key`

```python
def set_api_key(provider: str, api_key: str) -> None
```

Set API key by exporting to environment variables.

<a id="spoon_ai.utils.config_manager.ConfigManager.get_model_name"></a>

#### `get_model_name`

```python
def get_model_name() -> Optional[str]
```

Get model name override from environment.

<a id="spoon_ai.utils.config_manager.ConfigManager.get_base_url"></a>

#### `get_base_url`

```python
def get_base_url() -> Optional[str]
```

Get base URL override from environment.

<a id="spoon_ai.utils.config_manager.ConfigManager.get_llm_provider"></a>

#### `get_llm_provider`

```python
def get_llm_provider() -> Optional[str]
```

Determine LLM provider from environment variables.

<a id="spoon_ai.utils.config"></a>

# Module `spoon_ai.utils.config`

<a id="spoon_ai.utils"></a>

# Module `spoon_ai.utils`

<a id="spoon_ai.utils.streaming"></a>

# Module `spoon_ai.utils.streaming`

<a id="spoon_ai.utils.streaming.StreamOutcome"></a>

## `StreamOutcome` Objects

```python
@dataclass
class StreamOutcome()
```

Accumulator for streaming output state.

<a id="spoon_ai.runnables.events"></a>

# Module `spoon_ai.runnables.events`

<a id="spoon_ai.runnables.events.StreamEventBuilder"></a>

## `StreamEventBuilder` Objects

```python
class StreamEventBuilder()
```

<a id="spoon_ai.runnables.events.StreamEventBuilder.chain_start"></a>

#### `chain_start`

```python
@staticmethod
def chain_start(run_id: UUID, name: str, inputs: Any,
                **kwargs: Any) -> StreamEvent
```

Build chain start event.

<a id="spoon_ai.runnables.events.StreamEventBuilder.chain_stream"></a>

#### `chain_stream`

```python
@staticmethod
def chain_stream(run_id: UUID, name: str, chunk: Any,
                 **kwargs: Any) -> StreamEvent
```

Build chain stream event.

<a id="spoon_ai.runnables.events.StreamEventBuilder.chain_end"></a>

#### `chain_end`

```python
@staticmethod
def chain_end(run_id: UUID, name: str, output: Any,
              **kwargs: Any) -> StreamEvent
```

Build chain end event.

<a id="spoon_ai.runnables.events.StreamEventBuilder.chain_error"></a>

#### `chain_error`

```python
@staticmethod
def chain_error(run_id: UUID, name: str, error: Exception,
                **kwargs: Any) -> StreamEvent
```

Build chain error event.

<a id="spoon_ai.runnables.events.StreamEventBuilder.llm_stream"></a>

#### `llm_stream`

```python
@staticmethod
def llm_stream(run_id: UUID,
               name: str,
               token: str,
               chunk: Optional[Any] = None,
               **kwargs: Any) -> StreamEvent
```

Build LLM stream event.

<a id="spoon_ai.runnables"></a>

# Module `spoon_ai.runnables`

Runnable interface and utilities for composable AI components.

This module provides the foundational Runnable interface that all Spoon AI
components implement, enabling streaming, composition, and standardized execution.

<a id="spoon_ai.runnables.base"></a>

# Module `spoon_ai.runnables.base`

<a id="spoon_ai.runnables.base.log_patches_from_events"></a>

#### `log_patches_from_events`

```python
async def log_patches_from_events(
        event_iter: AsyncIterator[Dict[str, Any]],
        *,
        diff: bool = True) -> AsyncIterator[RunLogPatch]
```

Convert a stream of events into run log patches.

<a id="spoon_ai.runnables.base.Runnable"></a>

## `Runnable` Objects

```python
class Runnable(ABC, Generic[Input, Output])
```

<a id="spoon_ai.runnables.base.Runnable.astream_log"></a>

#### `astream_log`

```python
async def astream_log(input: Input,
                      config: Optional[RunnableConfig] = None,
                      *,
                      diff: bool = True) -> AsyncIterator[RunLogPatch]
```

Asynchronously stream structured log patches derived from execution events.

<a id="spoon_ai.runnables.base.Runnable.astream_events"></a>

#### `astream_events`

```python
async def astream_events(
        input: Input,
        config: Optional[RunnableConfig] = None
) -> AsyncIterator[Dict[str, Any]]
```

Asynchronously stream structured execution events.

<a id="spoon_ai.payments.server"></a>

# Module `spoon_ai.payments.server`

<a id="spoon_ai.payments.server.create_paywalled_router"></a>

#### `create_paywalled_router`

```python
def create_paywalled_router(
    service: Optional[X402PaymentService] = None,
    agent_factory: AgentFactory = _default_agent_factory,
    payment_message: str = "Payment required to invoke this agent."
) -> APIRouter
```

Build a FastAPI router that protects agent invocations behind an x402 paywall.

**Arguments**:

- `service` - Optional pre-configured payment service.
- `agent_factory` - Coroutine that returns an initialized agent given its name.
- `payment_message` - Message displayed when payment is required.
  

**Returns**:

- `APIRouter` - Router with `/invoke/&#123;agent_name&#125;` endpoint ready to mount.

<a id="spoon_ai.payments.cli"></a>

# Module `spoon_ai.payments.cli`

<a id="spoon_ai.payments.facilitator_client"></a>

# Module `spoon_ai.payments.facilitator_client`

<a id="spoon_ai.payments.facilitator_client.X402FacilitatorClient"></a>

## `X402FacilitatorClient` Objects

```python
class X402FacilitatorClient()
```

Thin wrapper over the upstream facilitator client with async header hooks.

<a id="spoon_ai.payments.exceptions"></a>

# Module `spoon_ai.payments.exceptions`

<a id="spoon_ai.payments.exceptions.X402PaymentError"></a>

## `X402PaymentError` Objects

```python
class X402PaymentError(Exception)
```

Base exception for x402 payment operations.

<a id="spoon_ai.payments.exceptions.X402ConfigurationError"></a>

## `X402ConfigurationError` Objects

```python
class X402ConfigurationError(X402PaymentError)
```

Raised when integration configuration is invalid or incomplete.

<a id="spoon_ai.payments.exceptions.X402VerificationError"></a>

## `X402VerificationError` Objects

```python
class X402VerificationError(X402PaymentError)
```

Raised when a payment header fails verification against the facilitator.

<a id="spoon_ai.payments.exceptions.X402SettlementError"></a>

## `X402SettlementError` Objects

```python
class X402SettlementError(X402PaymentError)
```

Raised when settlement fails or returns an error response.

<a id="spoon_ai.payments.x402_service"></a>

# Module `spoon_ai.payments.x402_service`

<a id="spoon_ai.payments.x402_service.X402PaymentService"></a>

## `X402PaymentService` Objects

```python
class X402PaymentService()
```

High level service that aligns the x402 SDK with SpoonOS conventions.

<a id="spoon_ai.payments.x402_service.X402PaymentService.discover_resources"></a>

#### `discover_resources`

```python
async def discover_resources(
        *,
        resource_type: Optional[str] = None,
        limit: Optional[int] = None,
        offset: Optional[int] = None) -> ListDiscoveryResourcesResponse
```

Query the facilitator discovery endpoint for registered paywalled resources.

<a id="spoon_ai.payments.x402_service.X402PaymentService.render_paywall_html"></a>

#### `render_paywall_html`

```python
def render_paywall_html(error: str,
                        request: Optional[X402PaymentRequest] = None,
                        headers: Optional[Dict[str, Any]] = None) -> str
```

Render the embedded paywall HTML with payment requirements.

<a id="spoon_ai.payments.x402_service.X402PaymentService.build_payment_header"></a>

#### `build_payment_header`

```python
def build_payment_header(requirements: PaymentRequirements,
                         *,
                         max_value: Optional[int] = None) -> str
```

Create a signed X-PAYMENT header for outbound requests.

<a id="spoon_ai.payments.x402_service.X402PaymentService.decode_payment_response"></a>

#### `decode_payment_response`

```python
def decode_payment_response(header_value: str) -> X402PaymentReceipt
```

Decode an X-PAYMENT-RESPONSE header into a structured receipt.

<a id="spoon_ai.payments.config"></a>

# Module `spoon_ai.payments.config`

<a id="spoon_ai.payments.config.X402ConfigurationError"></a>

## `X402ConfigurationError` Objects

```python
class X402ConfigurationError(Exception)
```

Raised when required x402 configuration is missing or invalid.

<a id="spoon_ai.payments.config.X402PaywallBranding"></a>

## `X402PaywallBranding` Objects

```python
class X402PaywallBranding(BaseModel)
```

Optional branding customisations for the embedded paywall template.

<a id="spoon_ai.payments.config.X402ClientConfig"></a>

## `X402ClientConfig` Objects

```python
class X402ClientConfig(BaseModel)
```

Holds client-side signing configuration used for outbound payments.

<a id="spoon_ai.payments.config.X402Settings"></a>

## `X402Settings` Objects

```python
class X402Settings(BaseModel)
```

Resolved configuration view for x402 payments inside SpoonOS.

<a id="spoon_ai.payments.config.X402Settings.amount_in_atomic_units"></a>

#### `amount_in_atomic_units`

```python
@property
def amount_in_atomic_units() -> str
```

Return the configured maximum amount encoded as atomic units (string).

<a id="spoon_ai.payments.config.X402Settings.build_asset_extra"></a>

#### `build_asset_extra`

```python
def build_asset_extra() -> Dict[str, Any]
```

Construct the `extra` payload for the payment requirements.

<a id="spoon_ai.payments.config.X402Settings.load"></a>

#### `load`

```python
@classmethod
def load(cls,
         config_manager: Optional[ConfigManager] = None) -> "X402Settings"
```

Load settings from config.json with .env fallbacks.

<a id="spoon_ai.payments.app"></a>

# Module `spoon_ai.payments.app`

<a id="spoon_ai.payments"></a>

# Module `spoon_ai.payments`

Payment utilities for integrating the SpoonOS core with the x402 payments protocol.

This package wraps the upstream `x402` Python SDK with configuration and service
abstractions that align to SpoonOS conventions (config.json priority, .env overrides,
and async-friendly helper utilities).

<a id="spoon_ai.payments.models"></a>

# Module `spoon_ai.payments.models`

<a id="spoon_ai.payments.models.X402PaymentRequest"></a>

## `X402PaymentRequest` Objects

```python
class X402PaymentRequest(BaseModel)
```

Describes a payment requirement that should be issued for a resource.

<a id="spoon_ai.payments.models.X402VerifyResult"></a>

## `X402VerifyResult` Objects

```python
class X402VerifyResult(BaseModel)
```

Captures the facilitator verification response.

<a id="spoon_ai.payments.models.X402SettleResult"></a>

## `X402SettleResult` Objects

```python
class X402SettleResult(BaseModel)
```

Captures settlement details.

<a id="spoon_ai.payments.models.X402PaymentOutcome"></a>

## `X402PaymentOutcome` Objects

```python
class X402PaymentOutcome(BaseModel)
```

Aggregates verification and settlement outcomes.

<a id="spoon_ai.payments.models.X402PaymentReceipt"></a>

## `X402PaymentReceipt` Objects

```python
class X402PaymentReceipt(BaseModel)
```

Decoded representation of the X-PAYMENT-RESPONSE header.

<a id="spoon_ai.chat"></a>

# Module `spoon_ai.chat`

<a id="spoon_ai.chat.ShortTermMemoryConfig"></a>

## `ShortTermMemoryConfig` Objects

```python
class ShortTermMemoryConfig(BaseModel)
```

Configuration for short-term memory management.

<a id="spoon_ai.chat.ShortTermMemoryConfig.enabled"></a>

#### `enabled`

Enable automatic short-term memory management.

<a id="spoon_ai.chat.ShortTermMemoryConfig.max_tokens"></a>

#### `max_tokens`

Maximum token count before triggering trimming/summarization.

<a id="spoon_ai.chat.ShortTermMemoryConfig.strategy"></a>

#### `strategy`

Strategy to use when exceeding max_tokens: 'summarize' or 'trim'.

<a id="spoon_ai.chat.ShortTermMemoryConfig.messages_to_keep"></a>

#### `messages_to_keep`

Number of recent messages to keep when summarizing.

<a id="spoon_ai.chat.ShortTermMemoryConfig.trim_strategy"></a>

#### `trim_strategy`

Trimming strategy when using 'trim' mode.

<a id="spoon_ai.chat.ShortTermMemoryConfig.keep_system_messages"></a>

#### `keep_system_messages`

Always keep system messages during trimming.

<a id="spoon_ai.chat.ShortTermMemoryConfig.auto_checkpoint"></a>

#### `auto_checkpoint`

Automatically save checkpoints before trimming/summarization.

<a id="spoon_ai.chat.ShortTermMemoryConfig.checkpoint_thread_id"></a>

#### `checkpoint_thread_id`

Thread ID for checkpoint management.

<a id="spoon_ai.chat.ShortTermMemoryConfig.summary_model"></a>

#### `summary_model`

Model to use for summarization (defaults to ChatBot's model).

<a id="spoon_ai.chat.ChatBot"></a>

## `ChatBot` Objects

```python
class ChatBot()
```

<a id="spoon_ai.chat.ChatBot.__init__"></a>

#### `__init__`

```python
def __init__(use_llm_manager: bool = True,
             model_name: str = None,
             llm_provider: str = None,
             api_key: str = None,
             base_url: str = None,
             enable_short_term_memory: bool = True,
             short_term_memory_config: Optional[Union[Dict[
                 str, Any], ShortTermMemoryConfig]] = None,
             token_counter: Optional[MessageTokenCounter] = None,
             enable_long_term_memory: bool = False,
             mem0_config: Optional[Dict[str, Any]] = None,
             callbacks: Optional[List[BaseCallbackHandler]] = None,
             **kwargs)
```

Initialize ChatBot with hierarchical configuration priority system.

Configuration Priority System:
1. Full manual override (highest priority) - all params provided
2. Partial override with config fallback - llm_provider provided, credentials pulled from environment (or config files if explicitly enabled)
3. Full environment-based loading - only use_llm_manager=True, reads from environment variables

**Arguments**:

- `use_llm_manager` - Enable LLM manager architecture (default: True)
- `model_name` - Model name override
- `llm_provider` - Provider name override
- `api_key` - API key override
- `base_url` - Base URL override
- `enable_short_term_memory` - Enable short-term memory management (default: True)
- `short_term_memory_config` - Configuration dict or ShortTermMemoryConfig instance
- `token_counter` - Optional custom token counter instance
- `enable_long_term_memory` - Enable Mem0-backed long-term memory retrieval/storage
- `mem0_config` - Configuration dict for Mem0 (api_key, user_id/agent_id, collection, etc.)
- `callbacks` - Optional list of callback handlers for monitoring
- `**kwargs` - Additional parameters

<a id="spoon_ai.chat.ChatBot.update_mem0_config"></a>

#### `update_mem0_config`

```python
def update_mem0_config(config: Optional[Dict[str, Any]] = None,
                       enable: Optional[bool] = None) -> None
```

Update Mem0 configuration and re-initialize the client if needed.

<a id="spoon_ai.chat.ChatBot.ask"></a>

#### `ask`

```python
async def ask(messages: List[Union[dict, Message]],
              system_msg: Optional[str] = None,
              output_queue: Optional[asyncio.Queue] = None) -> str
```

Ask method using the LLM manager architecture.

Automatically applies short-term memory strategy if enabled.

<a id="spoon_ai.chat.ChatBot.ask_tool"></a>

#### `ask_tool`

```python
async def ask_tool(messages: List[Union[dict, Message]],
                   system_msg: Optional[str] = None,
                   tools: Optional[List[dict]] = None,
                   tool_choice: Optional[str] = None,
                   output_queue: Optional[asyncio.Queue] = None,
                   **kwargs) -> LLMResponse
```

Ask tool method using the LLM manager architecture.

Automatically applies short-term memory strategy if enabled.

<a id="spoon_ai.chat.ChatBot.trim_messages"></a>

#### `trim_messages`

```python
async def trim_messages(messages: List[Message],
                        max_tokens: int,
                        strategy: TrimStrategy = TrimStrategy.FROM_END,
                        keep_system: bool = True,
                        model: Optional[str] = None) -> List[Message]
```

Trim messages to stay within the token budget.

**Arguments**:

- `messages` - List of messages to trim
- `max_tokens` - Maximum token count to retain
- `strategy` - Trimming strategy (from_start or from_end)
- `keep_system` - Whether to always keep the leading system message
- `model` - Model name for token counting
  

**Returns**:

- `List[Message]` - Trimmed messages list

<a id="spoon_ai.chat.ChatBot.remove_message"></a>

#### `remove_message`

```python
def remove_message(message_id: str, **kwargs: Any) -> "RemoveMessage"
```

Construct a removal instruction for the message with the given ID.

<a id="spoon_ai.chat.ChatBot.remove_all_messages"></a>

#### `remove_all_messages`

```python
def remove_all_messages() -> "RemoveMessage"
```

Construct a removal instruction that clears the entire history.

<a id="spoon_ai.chat.ChatBot.summarize_messages"></a>

#### `summarize_messages`

```python
async def summarize_messages(
    messages: List[Message],
    max_tokens_before_summary: int,
    messages_to_keep: int = 5,
    summary_model: Optional[str] = None,
    existing_summary: str = ""
) -> Tuple[List[Message], List[RemoveMessage], Optional[str]]
```

Summarize earlier messages and emit removal directives.

Returns a tuple ``(messages_for_llm, removals, summary_text)`` where
``messages_for_llm`` are the messages that should be sent to the language
model for the next turn, ``removals`` contains ``RemoveMessage``
directives that should be applied to the stored history, and
``summary_text`` is the newly generated summary (if any).

**Arguments**:

- `messages` - List of messages to process
- `max_tokens_before_summary` - Token threshold for triggering summary
- `messages_to_keep` - Number of recent messages to keep uncompressed
- `summary_model` - Model to use for summarization
- `existing_summary` - Previously stored summary text

<a id="spoon_ai.chat.ChatBot.latest_summary"></a>

#### `latest_summary`

```python
@property
def latest_summary() -> Optional[str]
```

Return the most recent summary generated by short-term memory.

<a id="spoon_ai.chat.ChatBot.latest_removals"></a>

#### `latest_removals`

```python
@property
def latest_removals() -> List[RemoveMessage]
```

Return the most recent removal directives emitted by summarization.

<a id="spoon_ai.chat.ChatBot.save_checkpoint"></a>

#### `save_checkpoint`

```python
def save_checkpoint(thread_id: str,
                    messages: List[Message],
                    metadata: Optional[dict] = None) -> str
```

Save current message state to checkpoint.

**Arguments**:

- `thread_id` - Thread identifier
- `messages` - Messages to save
- `metadata` - Optional metadata to store
  

**Returns**:

- `str` - Checkpoint ID

<a id="spoon_ai.chat.ChatBot.restore_checkpoint"></a>

#### `restore_checkpoint`

```python
def restore_checkpoint(
        thread_id: str,
        checkpoint_id: Optional[str] = None) -> Optional[List[Message]]
```

Restore messages from checkpoint.

**Arguments**:

- `thread_id` - Thread identifier
- `checkpoint_id` - Optional specific checkpoint ID
  

**Returns**:

- `Optional[List[Message]]` - Restored messages, or None if checkpoint not found

<a id="spoon_ai.chat.ChatBot.list_checkpoints"></a>

#### `list_checkpoints`

```python
def list_checkpoints(thread_id: str) -> List[dict]
```

List all checkpoints for a thread.

**Arguments**:

- `thread_id` - Thread identifier
  

**Returns**:

- `List[dict]` - List of checkpoint metadata

<a id="spoon_ai.chat.ChatBot.clear_checkpoints"></a>

#### `clear_checkpoints`

```python
def clear_checkpoints(thread_id: str) -> None
```

Clear all checkpoints for a thread.

**Arguments**:

- `thread_id` - Thread identifier

<a id="spoon_ai.chat.ChatBot.astream"></a>

#### `astream`

```python
async def astream(messages: List[Union[dict, Message]],
                  system_msg: Optional[str] = None,
                  callbacks: Optional[List[BaseCallbackHandler]] = None,
                  **kwargs: Any) -> AsyncIterator[LLMResponseChunk]
```

Stream LLM responses chunk by chunk.

<a id="spoon_ai.chat.ChatBot.astream_events"></a>

#### `astream_events`

```python
async def astream_events(messages: List[Union[dict, Message]],
                         system_msg: Optional[str] = None,
                         callbacks: Optional[List[BaseCallbackHandler]] = None,
                         **kwargs) -> AsyncIterator[dict]
```

Stream structured events during LLM execution.

This method yields detailed events tracking the execution flow,
useful for monitoring and debugging.

**Arguments**:

- `messages` - List of messages or dicts
- `system_msg` - Optional system message
- `callbacks` - Optional callback handlers
- `**kwargs` - Additional provider parameters
  

**Yields**:

  Event dictionaries with structure:
  &#123;
- `"event"` - event_type,
- `"run_id"` - str,
- `"timestamp"` - ISO datetime string,
- `"data"` - &#123;event-specific data&#125;
  &#125;

<a id="spoon_ai.chat.ChatBot.astream_log"></a>

#### `astream_log`

```python
async def astream_log(messages: List[Union[dict, Message]],
                      system_msg: Optional[str] = None,
                      callbacks: Optional[List[BaseCallbackHandler]] = None,
                      *,
                      diff: bool = True,
                      **kwargs: Any) -> AsyncIterator[RunLogPatch]
```

Stream run log patches describing ChatBot execution.

<a id="spoon_ai.tools.turnkey_tools"></a>

# Module `spoon_ai.tools.turnkey_tools`

Turnkey Tools - Secure Blockchain Operations

This module provides Turnkey SDK tools for secure blockchain operations including:
- Transaction signing and broadcasting
- Message and EIP-712 signing
- Multi-account management
- Activity audit and monitoring
- Wallet and account operations

<a id="spoon_ai.tools.turnkey_tools.TurnkeyBaseTool"></a>

## `TurnkeyBaseTool` Objects

```python
class TurnkeyBaseTool(BaseTool)
```

Base class for Turnkey tools with shared client initialization

<a id="spoon_ai.tools.turnkey_tools.TurnkeyBaseTool.client"></a>

#### `client`

```python
@property
def client()
```

Lazy initialization of Turnkey client

<a id="spoon_ai.tools.turnkey_tools.SignEVMTransactionTool"></a>

## `SignEVMTransactionTool` Objects

```python
class SignEVMTransactionTool(TurnkeyBaseTool)
```

Sign EVM transaction using Turnkey

<a id="spoon_ai.tools.turnkey_tools.SignEVMTransactionTool.execute"></a>

#### `execute`

```python
async def execute(sign_with: str, unsigned_tx: str, **kwargs) -> str
```

Sign EVM transaction

<a id="spoon_ai.tools.turnkey_tools.SignMessageTool"></a>

## `SignMessageTool` Objects

```python
class SignMessageTool(TurnkeyBaseTool)
```

Sign arbitrary message using Turnkey

<a id="spoon_ai.tools.turnkey_tools.SignMessageTool.execute"></a>

#### `execute`

```python
async def execute(sign_with: str,
                  message: str,
                  use_keccak256: bool = True,
                  **kwargs) -> str
```

Sign message

<a id="spoon_ai.tools.turnkey_tools.SignTypedDataTool"></a>

## `SignTypedDataTool` Objects

```python
class SignTypedDataTool(TurnkeyBaseTool)
```

Sign EIP-712 structured data using Turnkey

<a id="spoon_ai.tools.turnkey_tools.SignTypedDataTool.execute"></a>

#### `execute`

```python
async def execute(sign_with: str, typed_data: dict, **kwargs) -> str
```

Sign EIP-712 typed data

<a id="spoon_ai.tools.turnkey_tools.BroadcastTransactionTool"></a>

## `BroadcastTransactionTool` Objects

```python
class BroadcastTransactionTool(TurnkeyBaseTool)
```

Broadcast signed transaction to blockchain

<a id="spoon_ai.tools.turnkey_tools.BroadcastTransactionTool.execute"></a>

#### `execute`

```python
async def execute(signed_tx: str, rpc_url: str = None, **kwargs) -> str
```

Broadcast transaction

<a id="spoon_ai.tools.turnkey_tools.ListWalletsTool"></a>

## `ListWalletsTool` Objects

```python
class ListWalletsTool(TurnkeyBaseTool)
```

List all wallets in the organization

<a id="spoon_ai.tools.turnkey_tools.ListWalletsTool.execute"></a>

#### `execute`

```python
async def execute(**kwargs) -> str
```

List wallets

<a id="spoon_ai.tools.turnkey_tools.ListWalletAccountsTool"></a>

## `ListWalletAccountsTool` Objects

```python
class ListWalletAccountsTool(TurnkeyBaseTool)
```

List accounts for a specific wallet

<a id="spoon_ai.tools.turnkey_tools.ListWalletAccountsTool.execute"></a>

#### `execute`

```python
async def execute(wallet_id: str,
                  limit: str = None,
                  before: str = None,
                  after: str = None,
                  **kwargs) -> str
```

List wallet accounts

<a id="spoon_ai.tools.turnkey_tools.GetActivityTool"></a>

## `GetActivityTool` Objects

```python
class GetActivityTool(TurnkeyBaseTool)
```

Get activity details by ID

<a id="spoon_ai.tools.turnkey_tools.GetActivityTool.execute"></a>

#### `execute`

```python
async def execute(activity_id: str, **kwargs) -> str
```

Get activity details

<a id="spoon_ai.tools.turnkey_tools.ListActivitiesTool"></a>

## `ListActivitiesTool` Objects

```python
class ListActivitiesTool(TurnkeyBaseTool)
```

List recent activities in the organization

<a id="spoon_ai.tools.turnkey_tools.ListActivitiesTool.execute"></a>

#### `execute`

```python
async def execute(limit: str = "10",
                  before: str = None,
                  after: str = None,
                  filter_by_status: list = None,
                  filter_by_type: list = None,
                  **kwargs) -> str
```

List activities

<a id="spoon_ai.tools.turnkey_tools.WhoAmITool"></a>

## `WhoAmITool` Objects

```python
class WhoAmITool(TurnkeyBaseTool)
```

Get organization information

<a id="spoon_ai.tools.turnkey_tools.WhoAmITool.execute"></a>

#### `execute`

```python
async def execute(**kwargs) -> str
```

Get organization info

<a id="spoon_ai.tools.turnkey_tools.BuildUnsignedEIP1559TxTool"></a>

## `BuildUnsignedEIP1559TxTool` Objects

```python
class BuildUnsignedEIP1559TxTool(BaseTool)
```

Build unsigned EIP-1559 transaction (supports NeoX)

<a id="spoon_ai.tools.turnkey_tools.BuildUnsignedEIP1559TxTool.execute"></a>

#### `execute`

```python
async def execute(from_addr: str,
                  to_addr: str = None,
                  value_wei: str = "0",
                  data_hex: str = "0x",
                  priority_gwei: str = "1",
                  max_fee_gwei: str = None,
                  gas_limit: str = None,
                  rpc_url: str = None,
                  **kwargs) -> str
```

Build unsigned transaction (auto-detects NeoX)

<a id="spoon_ai.tools.turnkey_tools.ListAllAccountsTool"></a>

## `ListAllAccountsTool` Objects

```python
class ListAllAccountsTool(TurnkeyBaseTool)
```

List all accounts across all wallets in the organization

<a id="spoon_ai.tools.turnkey_tools.ListAllAccountsTool.execute"></a>

#### `execute`

```python
async def execute(limit: str = "50", **kwargs) -> str
```

List all accounts across all wallets

<a id="spoon_ai.tools.turnkey_tools.BatchSignTransactionsTool"></a>

## `BatchSignTransactionsTool` Objects

```python
class BatchSignTransactionsTool(TurnkeyBaseTool)
```

Batch sign transactions for multiple accounts

<a id="spoon_ai.tools.turnkey_tools.BatchSignTransactionsTool.execute"></a>

#### `execute`

```python
async def execute(to_address: str,
                  value_wei: str,
                  data_hex: str = "0x",
                  max_accounts: str = "3",
                  enable_broadcast: bool = False,
                  rpc_url: str = None,
                  **kwargs) -> str
```

Batch sign transactions for multiple accounts

<a id="spoon_ai.tools.turnkey_tools.CreateWalletTool"></a>

## `CreateWalletTool` Objects

```python
class CreateWalletTool(TurnkeyBaseTool)
```

Create a new wallet

<a id="spoon_ai.tools.turnkey_tools.CreateWalletTool.execute"></a>

#### `execute`

```python
async def execute(wallet_name: str,
                  accounts_json: str = None,
                  mnemonic_length: str = "24",
                  **kwargs) -> str
```

Create a new wallet

<a id="spoon_ai.tools.turnkey_tools.GetWalletTool"></a>

## `GetWalletTool` Objects

```python
class GetWalletTool(TurnkeyBaseTool)
```

Get wallet information by wallet ID

<a id="spoon_ai.tools.turnkey_tools.GetWalletTool.execute"></a>

#### `execute`

```python
async def execute(wallet_id: str, **kwargs) -> str
```

Get wallet information

<a id="spoon_ai.tools.turnkey_tools.CreateWalletAccountsTool"></a>

## `CreateWalletAccountsTool` Objects

```python
class CreateWalletAccountsTool(TurnkeyBaseTool)
```

Add accounts to an existing wallet

<a id="spoon_ai.tools.turnkey_tools.CreateWalletAccountsTool.execute"></a>

#### `execute`

```python
async def execute(wallet_id: str, accounts_json: str, **kwargs) -> str
```

Add accounts to existing wallet

<a id="spoon_ai.tools.turnkey_tools.CompleteTransactionWorkflowTool"></a>

## `CompleteTransactionWorkflowTool` Objects

```python
class CompleteTransactionWorkflowTool(TurnkeyBaseTool)
```

Complete transaction workflow: build, sign, and optionally broadcast

<a id="spoon_ai.tools.turnkey_tools.CompleteTransactionWorkflowTool.execute"></a>

#### `execute`

```python
async def execute(sign_with: str,
                  to_address: str,
                  value_wei: str,
                  data_hex: str = "0x",
                  enable_broadcast: bool = False,
                  rpc_url: str = None,
                  **kwargs) -> str
```

Complete transaction workflow

<a id="spoon_ai.tools.turnkey_tools.get_turnkey_tools"></a>

#### `get_turnkey_tools`

```python
def get_turnkey_tools() -> List[BaseTool]
```

Get all Turnkey tools

<a id="spoon_ai.tools.tool_manager"></a>

# Module `spoon_ai.tools.tool_manager`

<a id="spoon_ai.tools.tool_manager.ToolManager"></a>

## `ToolManager` Objects

```python
class ToolManager()
```

<a id="spoon_ai.tools.tool_manager.ToolManager.reindex"></a>

#### `reindex`

```python
def reindex() -> None
```

Rebuild the internal name-&gt;tool mapping. Useful if tools have been renamed dynamically.

<a id="spoon_ai.tools.neofs_tools"></a>

# Module `spoon_ai.tools.neofs_tools`

NeoFS Tools for spoon_ai framework

Simple wrappers around NeoFS client methods.
Tools do NOT auto-create bearer tokens - Agent manages tokens.
All parameters map directly to client method parameters.

<a id="spoon_ai.tools.neofs_tools.get_shared_neofs_client"></a>

#### `get_shared_neofs_client`

```python
def get_shared_neofs_client() -> NeoFSClient
```

Get shared NeoFSClient instance for all NeoFS tools.

Returns the same client instance across all tool calls to ensure
bearer token authentication works correctly.

<a id="spoon_ai.tools.neofs_tools.CreateBearerTokenTool"></a>

## `CreateBearerTokenTool` Objects

```python
class CreateBearerTokenTool(BaseTool)
```

Create a bearer token for NeoFS operations

<a id="spoon_ai.tools.neofs_tools.CreateContainerTool"></a>

## `CreateContainerTool` Objects

```python
class CreateContainerTool(BaseTool)
```

Create a NeoFS container

<a id="spoon_ai.tools.neofs_tools.UploadObjectTool"></a>

## `UploadObjectTool` Objects

```python
class UploadObjectTool(BaseTool)
```

Upload object to container

<a id="spoon_ai.tools.neofs_tools.DownloadObjectByIdTool"></a>

## `DownloadObjectByIdTool` Objects

```python
class DownloadObjectByIdTool(BaseTool)
```

Download object by ID

<a id="spoon_ai.tools.neofs_tools.GetObjectHeaderByIdTool"></a>

## `GetObjectHeaderByIdTool` Objects

```python
class GetObjectHeaderByIdTool(BaseTool)
```

Get object header by ID

<a id="spoon_ai.tools.neofs_tools.DownloadObjectByAttributeTool"></a>

## `DownloadObjectByAttributeTool` Objects

```python
class DownloadObjectByAttributeTool(BaseTool)
```

Download object by attribute

<a id="spoon_ai.tools.neofs_tools.GetObjectHeaderByAttributeTool"></a>

## `GetObjectHeaderByAttributeTool` Objects

```python
class GetObjectHeaderByAttributeTool(BaseTool)
```

Get object header by attribute

<a id="spoon_ai.tools.neofs_tools.DeleteObjectTool"></a>

## `DeleteObjectTool` Objects

```python
class DeleteObjectTool(BaseTool)
```

Delete an object

<a id="spoon_ai.tools.neofs_tools.SearchObjectsTool"></a>

## `SearchObjectsTool` Objects

```python
class SearchObjectsTool(BaseTool)
```

Search objects in container

<a id="spoon_ai.tools.neofs_tools.SetContainerEaclTool"></a>

## `SetContainerEaclTool` Objects

```python
class SetContainerEaclTool(BaseTool)
```

Set eACL for container

<a id="spoon_ai.tools.neofs_tools.GetContainerEaclTool"></a>

## `GetContainerEaclTool` Objects

```python
class GetContainerEaclTool(BaseTool)
```

Get eACL for container

<a id="spoon_ai.tools.neofs_tools.ListContainersTool"></a>

## `ListContainersTool` Objects

```python
class ListContainersTool(BaseTool)
```

List all containers

<a id="spoon_ai.tools.neofs_tools.GetContainerInfoTool"></a>

## `GetContainerInfoTool` Objects

```python
class GetContainerInfoTool(BaseTool)
```

Get container info

<a id="spoon_ai.tools.neofs_tools.DeleteContainerTool"></a>

## `DeleteContainerTool` Objects

```python
class DeleteContainerTool(BaseTool)
```

Delete container

<a id="spoon_ai.tools.neofs_tools.GetNetworkInfoTool"></a>

## `GetNetworkInfoTool` Objects

```python
class GetNetworkInfoTool(BaseTool)
```

Get network info

<a id="spoon_ai.tools.neofs_tools.GetBalanceTool"></a>

## `GetBalanceTool` Objects

```python
class GetBalanceTool(BaseTool)
```

Get balance for an address

<a id="spoon_ai.tools.x402_payment"></a>

# Module `spoon_ai.tools.x402_payment`

<a id="spoon_ai.tools.x402_payment.X402PaymentHeaderTool"></a>

## `X402PaymentHeaderTool` Objects

```python
class X402PaymentHeaderTool(BaseTool)
```

Create a signed X-PAYMENT header for a given resource.

<a id="spoon_ai.tools.x402_payment.X402PaywalledRequestTool"></a>

## `X402PaywalledRequestTool` Objects

```python
class X402PaywalledRequestTool(BaseTool)
```

Fetch a paywalled resource, handling the x402 402 negotiation automatically.

<a id="spoon_ai.tools"></a>

# Module `spoon_ai.tools`

<a id="spoon_ai.tools.mcp_tool"></a>

# Module `spoon_ai.tools.mcp_tool`

<a id="spoon_ai.tools.mcp_tool.MCPTool"></a>

## `MCPTool` Objects

```python
class MCPTool(BaseTool, MCPClientMixin)
```

<a id="spoon_ai.tools.mcp_tool.MCPTool.call_mcp_tool"></a>

#### `call_mcp_tool`

```python
async def call_mcp_tool(tool_name: str, **kwargs)
```

Override the mixin method to add tool-specific error handling.

<a id="spoon_ai.tools.mcp_tool.MCPTool.list_available_tools"></a>

#### `list_available_tools`

```python
async def list_available_tools() -> list
```

List available tools from the MCP server.

<a id="spoon_ai.tools.base"></a>

# Module `spoon_ai.tools.base`

<a id="spoon_ai.tools.base.ToolFailure"></a>

## `ToolFailure` Objects

```python
class ToolFailure(Exception)
```

Exception to indicate a tool execution failure.

<a id="spoon_ai.graph.agent"></a>

# Module `spoon_ai.graph.agent`

GraphAgent implementation for the graph package.

<a id="spoon_ai.graph.agent.Memory"></a>

## `Memory` Objects

```python
class Memory()
```

Memory implementation with persistent storage

<a id="spoon_ai.graph.agent.Memory.clear"></a>

#### `clear`

```python
def clear()
```

Clear all messages and reset memory

<a id="spoon_ai.graph.agent.Memory.add_message"></a>

#### `add_message`

```python
def add_message(msg)
```

Add a message to memory

<a id="spoon_ai.graph.agent.Memory.get_messages"></a>

#### `get_messages`

```python
def get_messages(limit: Optional[int] = None) -> List[Dict[str, Any]]
```

Get messages from memory

<a id="spoon_ai.graph.agent.Memory.get_recent_messages"></a>

#### `get_recent_messages`

```python
def get_recent_messages(hours: int = 24) -> List[Dict[str, Any]]
```

Get messages from the last N hours

<a id="spoon_ai.graph.agent.Memory.search_messages"></a>

#### `search_messages`

```python
def search_messages(query: str, limit: int = 10) -> List[Dict[str, Any]]
```

Search messages containing the query

<a id="spoon_ai.graph.agent.Memory.get_statistics"></a>

#### `get_statistics`

```python
def get_statistics() -> Dict[str, Any]
```

Get memory statistics

<a id="spoon_ai.graph.agent.Memory.set_metadata"></a>

#### `set_metadata`

```python
def set_metadata(key: str, value: Any)
```

Set metadata

<a id="spoon_ai.graph.agent.Memory.get_metadata"></a>

#### `get_metadata`

```python
def get_metadata(key: str, default: Any = None) -> Any
```

Get metadata

<a id="spoon_ai.graph.agent.MockMemory"></a>

## `MockMemory` Objects

```python
class MockMemory(Memory)
```

Alias for backward compatibility - now uses persistent memory

<a id="spoon_ai.graph.agent.GraphAgent"></a>

## `GraphAgent` Objects

```python
class GraphAgent()
```

<a id="spoon_ai.graph.agent.GraphAgent.search_memory"></a>

#### `search_memory`

```python
def search_memory(query: str, limit: int = 10) -> List[Dict[str, Any]]
```

Search memory for messages containing the query

<a id="spoon_ai.graph.agent.GraphAgent.get_recent_memory"></a>

#### `get_recent_memory`

```python
def get_recent_memory(hours: int = 24) -> List[Dict[str, Any]]
```

Get recent messages from memory

<a id="spoon_ai.graph.agent.GraphAgent.get_memory_statistics"></a>

#### `get_memory_statistics`

```python
def get_memory_statistics() -> Dict[str, Any]
```

Get memory statistics

<a id="spoon_ai.graph.agent.GraphAgent.set_memory_metadata"></a>

#### `set_memory_metadata`

```python
def set_memory_metadata(key: str, value: Any)
```

Set memory metadata

<a id="spoon_ai.graph.agent.GraphAgent.get_memory_metadata"></a>

#### `get_memory_metadata`

```python
def get_memory_metadata(key: str, default: Any = None) -> Any
```

Get memory metadata

<a id="spoon_ai.graph.agent.GraphAgent.save_session"></a>

#### `save_session`

```python
def save_session()
```

Manually save current session

<a id="spoon_ai.graph.agent.GraphAgent.load_session"></a>

#### `load_session`

```python
def load_session(session_id: str)
```

Load a specific session

<a id="spoon_ai.graph.types"></a>

# Module `spoon_ai.graph.types`

Typed structures for the graph package.

<a id="spoon_ai.graph.checkpointer"></a>

# Module `spoon_ai.graph.checkpointer`

In-memory checkpointer for the graph package.

<a id="spoon_ai.graph.checkpointer.InMemoryCheckpointer"></a>

## `InMemoryCheckpointer` Objects

```python
class InMemoryCheckpointer()
```

<a id="spoon_ai.graph.checkpointer.InMemoryCheckpointer.iter_checkpoint_history"></a>

#### `iter_checkpoint_history`

```python
def iter_checkpoint_history(
        config: Dict[str, Any]) -> Iterable[CheckpointTuple]
```

Return checkpoint tuples for the specified thread, newest last.

<a id="spoon_ai.graph.builder"></a>

# Module `spoon_ai.graph.builder`

Declarative builders and helpers for SpoonAI graphs.

<a id="spoon_ai.graph.builder.Intent"></a>

## `Intent` Objects

```python
@dataclass
class Intent()
```

Result of intent analysis.

<a id="spoon_ai.graph.builder.IntentAnalyzer"></a>

## `IntentAnalyzer` Objects

```python
class IntentAnalyzer()
```

LLM-powered intent analyzer.

Core stays generic; concrete prompts/parsers are supplied by callers.

<a id="spoon_ai.graph.builder.AdaptiveStateBuilder"></a>

## `AdaptiveStateBuilder` Objects

```python
class AdaptiveStateBuilder()
```

Construct initial graph state using query intent and optional parameters.

<a id="spoon_ai.graph.builder.ParameterInferenceEngine"></a>

## `ParameterInferenceEngine` Objects

```python
class ParameterInferenceEngine()
```

LLM delegator for parameter extraction.

Core keeps this generic; applications provide formatting/parsing via options.

<a id="spoon_ai.graph.builder.NodeSpec"></a>

## `NodeSpec` Objects

```python
@dataclass
class NodeSpec()
```

Declarative node specification.

<a id="spoon_ai.graph.builder.EdgeSpec"></a>

## `EdgeSpec` Objects

```python
@dataclass
class EdgeSpec()
```

Declarative edge specification.

<a id="spoon_ai.graph.builder.EdgeSpec.end"></a>

#### `end`

target name or callable router

<a id="spoon_ai.graph.builder.ParallelGroupSpec"></a>

## `ParallelGroupSpec` Objects

```python
@dataclass
class ParallelGroupSpec()
```

Parallel group specification.

<a id="spoon_ai.graph.builder.GraphTemplate"></a>

## `GraphTemplate` Objects

```python
@dataclass
class GraphTemplate()
```

Complete declarative template for a graph.

<a id="spoon_ai.graph.builder.DeclarativeGraphBuilder"></a>

## `DeclarativeGraphBuilder` Objects

```python
class DeclarativeGraphBuilder()
```

Build StateGraph instances from declarative templates.

<a id="spoon_ai.graph.builder.NodePlugin"></a>

## `NodePlugin` Objects

```python
class NodePlugin()
```

Pluggable node provider.

<a id="spoon_ai.graph.builder.NodePluginSystem"></a>

## `NodePluginSystem` Objects

```python
class NodePluginSystem()
```

Registry and discovery for node plugins.

<a id="spoon_ai.graph.builder.HighLevelGraphAPI"></a>

## `HighLevelGraphAPI` Objects

```python
class HighLevelGraphAPI()
```

Convenience facade for building graphs per query.

<a id="spoon_ai.graph.mcp_integration"></a>

# Module `spoon_ai.graph.mcp_integration`

Utility classes for intelligent MCP tool discovery and configuration.

Core graph components no longer hard-code external tools; instead, user code
registers tool specifications and optional transport/configuration details via
these helpers.

<a id="spoon_ai.graph.mcp_integration.MCPToolSpec"></a>

## `MCPToolSpec` Objects

```python
@dataclass
class MCPToolSpec()
```

Specification describing a desired MCP tool.

<a id="spoon_ai.graph.mcp_integration.MCPConfigManager"></a>

## `MCPConfigManager` Objects

```python
class MCPConfigManager()
```

Centralised configuration loader for MCP tools.

<a id="spoon_ai.graph.mcp_integration.MCPToolDiscoveryEngine"></a>

## `MCPToolDiscoveryEngine` Objects

```python
class MCPToolDiscoveryEngine()
```

Discover MCP tools based on registered intent mappings.

<a id="spoon_ai.graph.mcp_integration.MCPIntegrationManager"></a>

## `MCPIntegrationManager` Objects

```python
class MCPIntegrationManager()
```

High level coordinator for MCP tool usage within graphs.

<a id="spoon_ai.graph.exceptions"></a>

# Module `spoon_ai.graph.exceptions`

Graph engine exception definitions (public within graph package).

<a id="spoon_ai.graph.reducers"></a>

# Module `spoon_ai.graph.reducers`

Reducers and validators for the graph package.

<a id="spoon_ai.graph.decorators"></a>

# Module `spoon_ai.graph.decorators`

Decorators and executor for the graph package.

<a id="spoon_ai.graph.config"></a>

# Module `spoon_ai.graph.config`

Configuration primitives for the SpoonAI graph engine.

<a id="spoon_ai.graph.config.RouterConfig"></a>

## `RouterConfig` Objects

```python
@dataclass
class RouterConfig()
```

Controls how the graph chooses the next node after each execution step.

<a id="spoon_ai.graph.config.ParallelRetryPolicy"></a>

## `ParallelRetryPolicy` Objects

```python
@dataclass
class ParallelRetryPolicy()
```

Retry policy for individual nodes inside a parallel group.

<a id="spoon_ai.graph.config.ParallelGroupConfig"></a>

## `ParallelGroupConfig` Objects

```python
@dataclass
class ParallelGroupConfig()
```

Controls how a parallel group executes and aggregates results.

<a id="spoon_ai.graph.config.ParallelGroupConfig.quorum"></a>

#### `quorum`

floats in (0, 1] treated as ratio, ints as absolute

<a id="spoon_ai.graph.config.ParallelGroupConfig.error_strategy"></a>

#### `error_strategy`

fail_fast, collect_errors, ignore_errors

<a id="spoon_ai.graph.config.GraphConfig"></a>

## `GraphConfig` Objects

```python
@dataclass
class GraphConfig()
```

Top-level configuration applied to an entire graph instance.

<a id="spoon_ai.graph.engine"></a>

# Module `spoon_ai.graph.engine`

Graph engine: StateGraph, CompiledGraph, and interrupt API implementation.

<a id="spoon_ai.graph.engine.BaseNode"></a>

## `BaseNode` Objects

```python
class BaseNode(ABC, Generic[State])
```

Base class for all graph nodes

<a id="spoon_ai.graph.engine.BaseNode.__call__"></a>

#### `__call__`

```python
@abstractmethod
async def __call__(state: State,
                   config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]
```

Execute the node logic

<a id="spoon_ai.graph.engine.RunnableNode"></a>

## `RunnableNode` Objects

```python
class RunnableNode(BaseNode[State])
```

Runnable node that wraps a function

<a id="spoon_ai.graph.engine.RunnableNode.__call__"></a>

#### `__call__`

```python
async def __call__(state: State,
                   config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]
```

Execute the wrapped function

<a id="spoon_ai.graph.engine.ToolNode"></a>

## `ToolNode` Objects

```python
class ToolNode(BaseNode[State])
```

Tool node for executing tools

<a id="spoon_ai.graph.engine.ToolNode.__call__"></a>

#### `__call__`

```python
async def __call__(state: State,
                   config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]
```

Execute tools based on state

<a id="spoon_ai.graph.engine.ConditionNode"></a>

## `ConditionNode` Objects

```python
class ConditionNode(BaseNode[State])
```

Conditional node for routing decisions

<a id="spoon_ai.graph.engine.ConditionNode.__call__"></a>

#### `__call__`

```python
async def __call__(state: State,
                   config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]
```

Execute condition and return routing decision

<a id="spoon_ai.graph.engine.interrupt"></a>

#### `interrupt`

```python
def interrupt(data: Dict[str, Any]) -> Any
```

Interrupt execution and wait for human input.

<a id="spoon_ai.graph.engine.RouteRule"></a>

## `RouteRule` Objects

```python
class RouteRule()
```

Advanced routing rule for automatic path selection

<a id="spoon_ai.graph.engine.RouteRule.matches"></a>

#### `matches`

```python
def matches(state: Dict[str, Any], query: str = "") -> bool
```

Check if this rule matches the current state/query

<a id="spoon_ai.graph.engine.RunningSummary"></a>

## `RunningSummary` Objects

```python
@dataclass
class RunningSummary()
```

Rolling conversation summary used by the summarisation node.

<a id="spoon_ai.graph.engine.SummarizationNode"></a>

## `SummarizationNode` Objects

```python
class SummarizationNode(BaseNode[Dict[str, Any]])
```

Node that summarises conversation history before model invocation.

<a id="spoon_ai.graph.engine.StateGraph"></a>

## `StateGraph` Objects

```python
class StateGraph(Generic[State])
```

<a id="spoon_ai.graph.engine.StateGraph.add_node"></a>

#### `add_node`

```python
def add_node(
        node_name: str, node: Union[BaseNode[State],
                                    Callable[[State], Any]]) -> "StateGraph"
```

Add a node to the graph

<a id="spoon_ai.graph.engine.StateGraph.add_edge"></a>

#### `add_edge`

```python
def add_edge(
        start_node: str,
        end_node: str,
        condition: Optional[Callable[[State], bool]] = None) -> "StateGraph"
```

Add an edge. When condition is provided, edge becomes conditional.

<a id="spoon_ai.graph.engine.StateGraph.add_conditional_edges"></a>

#### `add_conditional_edges`

```python
def add_conditional_edges(start_node: str, condition: Callable[[State], str],
                          path_map: Dict[str, str]) -> "StateGraph"
```

Add conditional edges

<a id="spoon_ai.graph.engine.StateGraph.set_entry_point"></a>

#### `set_entry_point`

```python
def set_entry_point(node_name: str) -> "StateGraph"
```

Set the entry point

<a id="spoon_ai.graph.engine.StateGraph.add_tool_node"></a>

#### `add_tool_node`

```python
def add_tool_node(tools: List[Any], name: str = "tools") -> "StateGraph"
```

Add a tool node

<a id="spoon_ai.graph.engine.StateGraph.add_conditional_node"></a>

#### `add_conditional_node`

```python
def add_conditional_node(condition_func: Callable[[State], str],
                         name: str = "condition") -> "StateGraph"
```

Add a conditional node

<a id="spoon_ai.graph.engine.StateGraph.add_parallel_group"></a>

#### `add_parallel_group`

```python
def add_parallel_group(
    group_name: str,
    nodes: List[str],
    config: Optional[Union[Dict[str, Any], ParallelGroupConfig]] = None
) -> "StateGraph"
```

Add a parallel execution group

<a id="spoon_ai.graph.engine.StateGraph.add_routing_rule"></a>

#### `add_routing_rule`

```python
def add_routing_rule(source_node: str,
                     condition: Union[str, Callable[[State, str], bool]],
                     target_node: str,
                     priority: int = 0) -> "StateGraph"
```

Add an intelligent routing rule

<a id="spoon_ai.graph.engine.StateGraph.get_state"></a>

#### `get_state`

```python
def get_state(
        config: Optional[Dict[str, Any]] = None) -> Optional[StateSnapshot]
```

Fetch the latest (or specified) checkpoint snapshot for a thread.

<a id="spoon_ai.graph.engine.StateGraph.get_state_history"></a>

#### `get_state_history`

```python
def get_state_history(
        config: Optional[Dict[str, Any]] = None) -> Iterable[StateSnapshot]
```

Return all checkpoints for the given thread, ordered by creation time.

<a id="spoon_ai.graph.engine.StateGraph.add_pattern_routing"></a>

#### `add_pattern_routing`

```python
def add_pattern_routing(source_node: str,
                        pattern: str,
                        target_node: str,
                        priority: int = 0) -> "StateGraph"
```

Add pattern-based routing rule

<a id="spoon_ai.graph.engine.StateGraph.set_intelligent_router"></a>

#### `set_intelligent_router`

```python
def set_intelligent_router(
        router_func: Callable[[Dict[str, Any], str], str]) -> "StateGraph"
```

Set the intelligent router function

<a id="spoon_ai.graph.engine.StateGraph.set_llm_router"></a>

#### `set_llm_router`

```python
def set_llm_router(router_func: Optional[Callable[[Dict[str, Any], str],
                                                  str]] = None,
                   config: Optional[Dict[str, Any]] = None) -> "StateGraph"
```

Set the LLM-powered router function

**Arguments**:

- `router_func` - Custom LLM router function. If None, uses default LLM router.
- `config` - Configuration for LLM router (model, temperature, max_tokens, etc.)

<a id="spoon_ai.graph.engine.StateGraph.enable_llm_routing"></a>

#### `enable_llm_routing`

```python
def enable_llm_routing(
        config: Optional[Dict[str, Any]] = None) -> "StateGraph"
```

Enable LLM-powered natural language routing

This automatically sets up LLM routing for the graph entry point.

<a id="spoon_ai.graph.engine.StateGraph.compile"></a>

#### `compile`

```python
def compile(checkpointer: Optional[Any] = None) -> "CompiledGraph"
```

Compile the graph

<a id="spoon_ai.graph.engine.StateGraph.get_graph"></a>

#### `get_graph`

```python
def get_graph() -> Dict[str, Any]
```

Get graph structure for visualization/debugging

<a id="spoon_ai.graph.engine.CompiledGraph"></a>

## `CompiledGraph` Objects

```python
class CompiledGraph(Generic[State])
```

Compiled graph for execution

<a id="spoon_ai.graph.engine.CompiledGraph.get_execution_metrics"></a>

#### `get_execution_metrics`

```python
def get_execution_metrics() -> Dict[str, Any]
```

Get aggregated execution metrics

<a id="spoon_ai.neofs.utils"></a>

# Module `spoon_ai.neofs.utils`

<a id="spoon_ai.neofs.utils.SignatureError"></a>

## `SignatureError` Objects

```python
class SignatureError(Exception)
```

Raised when signature payload construction fails.

<a id="spoon_ai.neofs.utils.sign_bearer_token"></a>

#### `sign_bearer_token`

```python
def sign_bearer_token(bearer_token: str,
                      private_key_wif: str,
                      *,
                      wallet_connect: bool = True) -> tuple[str, str]
```

Returns (signature_hex, compressed_pubkey_hex)

- wallet_connect=True:
    msg = WC format (with prefix/len/salt/postfix), hash=SHA-256
    X-Bearer-Signature = &lt;DER signature hex&gt; + &lt;16B salt hex&gt;
    X-Bearer-Signature-Key = &lt;compressed public key hex&gt;
    URL needs to append ?walletConnect=true

<a id="spoon_ai.neofs.client"></a>

# Module `spoon_ai.neofs.client`

<a id="spoon_ai.neofs.client.NeoFSClient"></a>

## `NeoFSClient` Objects

```python
class NeoFSClient()
```

<a id="spoon_ai.neofs.client.NeoFSClient.set_container_eacl"></a>

#### `set_container_eacl`

```python
def set_container_eacl(container_id: str,
                       eacl: Eacl,
                       *,
                       bearer_token: Optional[str] = None,
                       wallet_connect: bool = True) -> SuccessResponse
```

Set container eACL.

**Arguments**:

- `container_id` - Container ID
- `eacl` - eACL object
- `bearer_token` - Optional Bearer Token (recommended for eACL operations)
- `wallet_connect` - Whether to use wallet_connect mode (default True)

<a id="spoon_ai.neofs.client.NeoFSClient.download_object_by_id"></a>

#### `download_object_by_id`

```python
def download_object_by_id(container_id: str,
                          object_id: str,
                          *,
                          bearer_token: Optional[str] = None,
                          download: bool | None = None,
                          range_header: str | None = None) -> httpx.Response
```

Download object by ID. Bearer token is optional for public containers.

<a id="spoon_ai.neofs.client.NeoFSClient.get_object_header_by_id"></a>

#### `get_object_header_by_id`

```python
def get_object_header_by_id(container_id: str,
                            object_id: str,
                            *,
                            bearer_token: Optional[str] = None,
                            range_header: str | None = None) -> httpx.Response
```

Get object header by ID. Bearer token is optional for public containers.

<a id="spoon_ai.neofs.client.NeoFSClient.download_object_by_attribute"></a>

#### `download_object_by_attribute`

```python
def download_object_by_attribute(
        container_id: str,
        attr_key: str,
        attr_val: str,
        *,
        bearer_token: Optional[str] = None,
        download: bool | None = None,
        range_header: str | None = None) -> httpx.Response
```

Download object by attribute. Bearer token is optional for public containers.

<a id="spoon_ai.neofs.client.NeoFSClient.get_object_header_by_attribute"></a>

#### `get_object_header_by_attribute`

```python
def get_object_header_by_attribute(
        container_id: str,
        attr_key: str,
        attr_val: str,
        *,
        bearer_token: Optional[str] = None,
        range_header: str | None = None) -> httpx.Response
```

Get object header by attribute. Bearer token is optional for public containers.

<a id="spoon_ai.neofs.client.NeoFSClient.search_objects"></a>

#### `search_objects`

```python
def search_objects(container_id: str,
                   search_request: SearchRequest,
                   *,
                   bearer_token: Optional[str] = None,
                   cursor: str = "",
                   limit: int = 100) -> ObjectListV2
```

Search objects. Bearer token is optional for public containers.

<a id="spoon_ai.neofs.client.NeoFSException"></a>

## `NeoFSException` Objects

```python
class NeoFSException(Exception)
```

Base exception for the NeoFS client.

<a id="spoon_ai.neofs.client.NeoFSAPIException"></a>

## `NeoFSAPIException` Objects

```python
class NeoFSAPIException(NeoFSException)
```

Raised when the API returns an error.

<a id="spoon_ai.neofs"></a>

# Module `spoon_ai.neofs`

NeoFS integration for Spoon Core.

<a id="spoon_ai.neofs.models"></a>

# Module `spoon_ai.neofs.models`

Pydantic models describing NeoFS REST API payloads.

<a id="spoon_ai.neofs.models.NetworkInfo"></a>

## `NetworkInfo` Objects

```python
class NetworkInfo(BaseModel)
```

Describes network configuration fees reported by the gateway.

<a id="spoon_ai.agents.toolcall"></a>

# Module `spoon_ai.agents.toolcall`

<a id="spoon_ai.agents.toolcall.ToolCallAgent"></a>

## `ToolCallAgent` Objects

```python
class ToolCallAgent(ReActAgent)
```

<a id="spoon_ai.agents.toolcall.ToolCallAgent.tool_choices"></a>

#### `tool_choices`

type: ignore

<a id="spoon_ai.agents.toolcall.ToolCallAgent.mcp_tools_cache_ttl"></a>

#### `mcp_tools_cache_ttl`

5 minutes TTL

<a id="spoon_ai.agents.toolcall.ToolCallAgent.run"></a>

#### `run`

```python
async def run(request: Optional[str] = None) -> str
```

Override run method to handle finish_reason termination specially.

<a id="spoon_ai.agents.toolcall.ToolCallAgent.step"></a>

#### `step`

```python
async def step() -> str
```

Override the step method to handle finish_reason termination properly.

<a id="spoon_ai.agents.react"></a>

# Module `spoon_ai.agents.react`

<a id="spoon_ai.agents.mcp_client_mixin"></a>

# Module `spoon_ai.agents.mcp_client_mixin`

<a id="spoon_ai.agents.mcp_client_mixin.MCPClientMixin"></a>

## `MCPClientMixin` Objects

```python
class MCPClientMixin()
```

<a id="spoon_ai.agents.mcp_client_mixin.MCPClientMixin.get_session"></a>

#### `get_session`

```python
@asynccontextmanager
async def get_session()
```

Get a session with robust resource management and cleanup.

Features:
- Automatic session reuse per task
- Resource limits to prevent exhaustion
- Proper cleanup on cancellation/failure
- Periodic cleanup of stale sessions

<a id="spoon_ai.agents.mcp_client_mixin.MCPClientMixin.list_mcp_tools"></a>

#### `list_mcp_tools`

```python
async def list_mcp_tools()
```

Get the list of available tools from the MCP server

<a id="spoon_ai.agents.mcp_client_mixin.MCPClientMixin.call_mcp_tool"></a>

#### `call_mcp_tool`

```python
async def call_mcp_tool(tool_name: str, **kwargs)
```

Call a tool on the MCP server

<a id="spoon_ai.agents.mcp_client_mixin.MCPClientMixin.send_mcp_message"></a>

#### `send_mcp_message`

```python
async def send_mcp_message(recipient: str,
                           message: Union[str, Dict[str, Any]],
                           topic: Optional[str] = None,
                           metadata: Optional[Dict[str, Any]] = None) -> bool
```

Send a message to the MCP system

**Arguments**:

- `recipient` - Recipient ID
- `message` - Message content (string or dictionary)
- `topic` - Message topic
- `metadata` - Additional metadata
  

**Returns**:

- `bool` - Whether the message was sent successfully

<a id="spoon_ai.agents.mcp_client_mixin.MCPClientMixin.cleanup"></a>

#### `cleanup`

```python
async def cleanup()
```

Enhanced cleanup method with comprehensive resource cleanup.

<a id="spoon_ai.agents.mcp_client_mixin.MCPClientMixin.get_session_stats"></a>

#### `get_session_stats`

```python
def get_session_stats() -> Dict[str, Any]
```

Get session statistics for monitoring.

<a id="spoon_ai.agents.spoon_react_mcp"></a>

# Module `spoon_ai.agents.spoon_react_mcp`

<a id="spoon_ai.agents.spoon_react_mcp.SpoonReactMCP"></a>

## `SpoonReactMCP` Objects

```python
class SpoonReactMCP(SpoonReactAI)
```

<a id="spoon_ai.agents.spoon_react_mcp.SpoonReactMCP.list_mcp_tools"></a>

#### `list_mcp_tools`

```python
async def list_mcp_tools()
```

Return MCP tools from available_tools manager

<a id="spoon_ai.agents.monitor"></a>

# Module `spoon_ai.agents.monitor`

<a id="spoon_ai.agents.rag"></a>

# Module `spoon_ai.agents.rag`

<a id="spoon_ai.agents.rag.RetrievalMixin"></a>

## `RetrievalMixin` Objects

```python
class RetrievalMixin()
```

Mixin class for retrieval-augmented generation functionality

<a id="spoon_ai.agents.rag.RetrievalMixin.initialize_retrieval_client"></a>

#### `initialize_retrieval_client`

```python
def initialize_retrieval_client(backend: str = 'chroma', **kwargs)
```

Initialize the retrieval client if it doesn't exist

<a id="spoon_ai.agents.rag.RetrievalMixin.add_documents"></a>

#### `add_documents`

```python
def add_documents(documents, backend: str = 'chroma', **kwargs)
```

Add documents to the retrieval system

<a id="spoon_ai.agents.rag.RetrievalMixin.retrieve_relevant_documents"></a>

#### `retrieve_relevant_documents`

```python
def retrieve_relevant_documents(query, k=5, backend: str = 'chroma', **kwargs)
```

Retrieve relevant documents for a query

<a id="spoon_ai.agents.rag.RetrievalMixin.get_context_from_query"></a>

#### `get_context_from_query`

```python
def get_context_from_query(query)
```

Get context string from relevant documents for a query

<a id="spoon_ai.agents.custom_agent"></a>

# Module `spoon_ai.agents.custom_agent`

<a id="spoon_ai.agents.custom_agent.CustomAgent"></a>

## `CustomAgent` Objects

```python
class CustomAgent(ToolCallAgent)
```

Custom Agent class allowing users to create their own agents and add custom tools

Usage:
Create custom agent and add tools:
   agent = CustomAgent(name="my_agent", description="My custom agent")
   agent.add_tool(MyCustomTool())
   result = await agent.run("Use my custom tool")

<a id="spoon_ai.agents.custom_agent.CustomAgent.add_tool"></a>

#### `add_tool`

```python
def add_tool(tool: BaseTool) -> None
```

Add a tool to the agent with validation.

**Arguments**:

- `tool` - Tool instance to add
  

**Raises**:

- `ValueError` - If tool is invalid or already exists

<a id="spoon_ai.agents.custom_agent.CustomAgent.add_tools"></a>

#### `add_tools`

```python
def add_tools(tools: List[BaseTool]) -> None
```

Add multiple tools to the agent with atomic operation.

**Arguments**:

- `tools` - List of tool instances to add
  

**Raises**:

- `ValueError` - If any tool is invalid

<a id="spoon_ai.agents.custom_agent.CustomAgent.remove_tool"></a>

#### `remove_tool`

```python
def remove_tool(tool_name: str) -> bool
```

Remove a tool from the agent.

**Arguments**:

- `tool_name` - Name of the tool to remove
  

**Returns**:

- `bool` - True if tool was removed, False if not found

<a id="spoon_ai.agents.custom_agent.CustomAgent.list_tools"></a>

#### `list_tools`

```python
def list_tools() -> List[str]
```

List all available tools in the agent.

**Returns**:

  List of tool names, empty list if no tools

<a id="spoon_ai.agents.custom_agent.CustomAgent.get_tool_info"></a>

#### `get_tool_info`

```python
def get_tool_info() -> Dict[str, Dict[str, Any]]
```

Get detailed information about all tools.

**Returns**:

  Dictionary with tool names as keys and tool info as values

<a id="spoon_ai.agents.custom_agent.CustomAgent.validate_tools"></a>

#### `validate_tools`

```python
def validate_tools() -> Dict[str, Any]
```

Validate all current tools and return validation report.

**Returns**:

  Dictionary with validation results

<a id="spoon_ai.agents.custom_agent.CustomAgent.run"></a>

#### `run`

```python
async def run(request: Optional[str] = None) -> str
```

Run the agent with enhanced tool validation.

**Arguments**:

- `request` - User request
  

**Returns**:

  Processing result

<a id="spoon_ai.agents.custom_agent.CustomAgent.clear"></a>

#### `clear`

```python
def clear()
```

Enhanced clear method with proper tool state management.

<a id="spoon_ai.agents.graph_agent"></a>

# Module `spoon_ai.agents.graph_agent`

Graph-based agent implementation for SpoonOS.

This module provides the GraphAgent class that executes StateGraph workflows,
integrating the graph execution system with the existing agent architecture.

<a id="spoon_ai.agents.graph_agent.GraphAgent"></a>

## `GraphAgent` Objects

```python
class GraphAgent(BaseAgent)
```

An agent that executes StateGraph workflows.

This agent provides a bridge between the existing SpoonOS agent architecture
and the new graph-based execution system. It allows complex, stateful workflows
to be defined as graphs and executed with proper state management.

Key Features:
- Executes StateGraph workflows
- Maintains compatibility with existing agent interfaces
- Provides detailed execution logging and error handling
- Supports both sync and async node functions

<a id="spoon_ai.agents.graph_agent.GraphAgent.__init__"></a>

#### `__init__`

```python
def __init__(**kwargs)
```

Initialize the GraphAgent.

**Arguments**:

- `graph` - StateGraph instance to execute
- `**kwargs` - Additional arguments passed to BaseAgent
  

**Raises**:

- `ValueError` - If no graph is provided

<a id="spoon_ai.agents.graph_agent.GraphAgent.validate_graph"></a>

#### `validate_graph`

```python
@validator('graph')
def validate_graph(cls, v)
```

Validate that the provided graph is a StateGraph instance.

<a id="spoon_ai.agents.graph_agent.GraphAgent.run"></a>

#### `run`

```python
async def run(request: Optional[str] = None) -> str
```

Execute the graph workflow.

This method overrides the base run method to invoke the compiled graph
instead of the traditional step-based execution loop.

**Arguments**:

- `request` - Optional input request to include in initial state
  

**Returns**:

  String representation of the execution result
  

**Raises**:

- `RuntimeError` - If agent is not in IDLE state
- `GraphExecutionError` - If graph execution fails

<a id="spoon_ai.agents.graph_agent.GraphAgent.step"></a>

#### `step`

```python
async def step() -> str
```

Step method for compatibility with BaseAgent.

Since GraphAgent uses graph execution instead of step-based execution,
this method is not used in normal operation but is required by the
BaseAgent interface.

**Returns**:

  Status message indicating graph-based execution

<a id="spoon_ai.agents.graph_agent.GraphAgent.get_execution_history"></a>

#### `get_execution_history`

```python
def get_execution_history() -> list
```

Get the execution history from the last graph run.

**Returns**:

  List of execution steps with metadata

<a id="spoon_ai.agents.graph_agent.GraphAgent.get_execution_metadata"></a>

#### `get_execution_metadata`

```python
def get_execution_metadata() -> Dict[str, Any]
```

Get metadata from the last execution.

**Returns**:

  Dictionary containing execution metadata

<a id="spoon_ai.agents.graph_agent.GraphAgent.clear_state"></a>

#### `clear_state`

```python
def clear_state()
```

Clear preserved state and execution history.

<a id="spoon_ai.agents.graph_agent.GraphAgent.update_initial_state"></a>

#### `update_initial_state`

```python
def update_initial_state(updates: Dict[str, Any])
```

Update the initial state for future executions.

**Arguments**:

- `updates` - Dictionary of state updates to merge

<a id="spoon_ai.agents.graph_agent.GraphAgent.set_preserve_state"></a>

#### `set_preserve_state`

```python
def set_preserve_state(preserve: bool)
```

Enable or disable state preservation between runs.

**Arguments**:

- `preserve` - Whether to preserve state between runs

<a id="spoon_ai.agents"></a>

# Module `spoon_ai.agents`

<a id="spoon_ai.agents.spoon_react"></a>

# Module `spoon_ai.agents.spoon_react`

<a id="spoon_ai.agents.spoon_react.create_configured_chatbot"></a>

#### `create_configured_chatbot`

```python
def create_configured_chatbot()
```

Create a ChatBot instance with intelligent provider selection.

<a id="spoon_ai.agents.spoon_react.SpoonReactAI"></a>

## `SpoonReactAI` Objects

```python
class SpoonReactAI(ToolCallAgent)
```

<a id="spoon_ai.agents.spoon_react.SpoonReactAI.__init__"></a>

#### `__init__`

```python
def __init__(**kwargs)
```

Initialize SpoonReactAI with both ToolCallAgent and MCPClientMixin initialization

<a id="spoon_ai.agents.spoon_react.SpoonReactAI.initialize"></a>

#### `initialize`

```python
async def initialize(__context: Any = None)
```

Initialize async components and subscribe to topics

<a id="spoon_ai.agents.spoon_react.SpoonReactAI.run"></a>

#### `run`

```python
async def run(request: Optional[str] = None) -> str
```

Ensure prompts reflect current tools before running.

<a id="spoon_ai.agents.base"></a>

# Module `spoon_ai.agents.base`

<a id="spoon_ai.agents.base.ThreadSafeOutputQueue"></a>

## `ThreadSafeOutputQueue` Objects

```python
class ThreadSafeOutputQueue()
```

Thread-safe output queue with fair access and timeout protection

<a id="spoon_ai.agents.base.ThreadSafeOutputQueue.get"></a>

#### `get`

```python
async def get(timeout: Optional[float] = 30.0) -> Any
```

Get item with timeout and fair access

<a id="spoon_ai.agents.base.BaseAgent"></a>

## `BaseAgent` Objects

```python
class BaseAgent(BaseModel, ABC)
```

Thread-safe base class for all agents with proper concurrency handling.

<a id="spoon_ai.agents.base.BaseAgent.add_message"></a>

#### `add_message`

```python
async def add_message(role: Literal["user", "assistant", "tool"],
                      content: str,
                      tool_call_id: Optional[str] = None,
                      tool_calls: Optional[List[ToolCall]] = None,
                      tool_name: Optional[str] = None,
                      timeout: Optional[float] = None) -> None
```

Thread-safe message addition with timeout protection

<a id="spoon_ai.agents.base.BaseAgent.state_context"></a>

#### `state_context`

```python
@asynccontextmanager
async def state_context(new_state: AgentState,
                        timeout: Optional[float] = None)
```

Thread-safe state context manager with deadlock prevention.
Acquires the state lock only to perform quick transitions, not for the
duration of the work inside the context, avoiding long-held locks and
false timeouts during network calls.

<a id="spoon_ai.agents.base.BaseAgent.run"></a>

#### `run`

```python
async def run(request: Optional[str] = None,
              timeout: Optional[float] = None) -> str
```

Thread-safe run method with proper concurrency control and callback support.

<a id="spoon_ai.agents.base.BaseAgent.step"></a>

#### `step`

```python
async def step(run_id: Optional[uuid.UUID] = None) -> str
```

Override this method in subclasses - now with step-level locking and callback support.

<a id="spoon_ai.agents.base.BaseAgent.is_stuck"></a>

#### `is_stuck`

```python
async def is_stuck() -> bool
```

Thread-safe stuck detection

<a id="spoon_ai.agents.base.BaseAgent.handle_stuck_state"></a>

#### `handle_stuck_state`

```python
async def handle_stuck_state()
```

Thread-safe stuck state handling

<a id="spoon_ai.agents.base.BaseAgent.add_documents"></a>

#### `add_documents`

```python
def add_documents(documents) -> None
```

Store documents on the agent so CLI load-docs works without RAG mixin.

This default implementation keeps the documents in-memory under
self._loaded_documents. Agents that support retrieval should override
this method to index documents into their vector store.

<a id="spoon_ai.agents.base.BaseAgent.save_chat_history"></a>

#### `save_chat_history`

```python
def save_chat_history()
```

Thread-safe chat history saving

<a id="spoon_ai.agents.base.BaseAgent.stream"></a>

#### `stream`

```python
async def stream(timeout: Optional[float] = None)
```

Thread-safe streaming with proper cleanup and timeout

<a id="spoon_ai.agents.base.BaseAgent.process_mcp_message"></a>

#### `process_mcp_message`

```python
async def process_mcp_message(content: Any,
                              sender: str,
                              message: Dict[str, Any],
                              agent_id: str,
                              timeout: Optional[float] = None)
```

Thread-safe MCP message processing with timeout protection

<a id="spoon_ai.agents.base.BaseAgent.shutdown"></a>

#### `shutdown`

```python
async def shutdown(timeout: float = 30.0)
```

Graceful shutdown with cleanup of active operations

<a id="spoon_ai.agents.base.BaseAgent.get_diagnostics"></a>

#### `get_diagnostics`

```python
def get_diagnostics() -> Dict[str, Any]
```

Get diagnostic information about the agent's state

<a id="spoon_ai.prompts.toolcall"></a>

# Module `spoon_ai.prompts.toolcall`

<a id="spoon_ai.prompts"></a>

# Module `spoon_ai.prompts`

<a id="spoon_ai.prompts.spoon_react"></a>

# Module `spoon_ai.prompts.spoon_react`

<a id="spoon_ai.turnkey.client"></a>

# Module `spoon_ai.turnkey.client`

<a id="spoon_ai.turnkey.client.Turnkey"></a>

## `Turnkey` Objects

```python
class Turnkey()
```

Turnkey API client class for managing blockchain private keys and wallet operations.

<a id="spoon_ai.turnkey.client.Turnkey.__init__"></a>

#### `__init__`

```python
def __init__(base_url=None,
             api_public_key=None,
             api_private_key=None,
             org_id=None)
```

Initialize Turnkey client.

**Arguments**:

- `base_url` _str_ - Turnkey API base URL (defaults from .env or default value).
- `api_public_key` _str_ - Turnkey API public key.
- `api_private_key` _str_ - Turnkey API private key.
- `org_id` _str_ - Turnkey organization ID.
  

**Raises**:

- `ValueError` - If required configuration parameters are missing.

<a id="spoon_ai.turnkey.client.Turnkey.whoami"></a>

#### `whoami`

```python
def whoami()
```

Call whoami API to get organization information.

**Returns**:

- `dict` - JSON response containing organization information.

<a id="spoon_ai.turnkey.client.Turnkey.import_private_key"></a>

#### `import_private_key`

```python
def import_private_key(user_id,
                       private_key_name,
                       encrypted_bundle,
                       curve="CURVE_SECP256K1",
                       address_formats=["ADDRESS_FORMAT_ETHEREUM"])
```

Import private key to Turnkey.

**Arguments**:

- `user_id` _str_ - User ID.
- `private_key_name` _str_ - Private key name.
- `encrypted_bundle` _str_ - Encrypted private key bundle.
- `curve` _str_ - Elliptic curve type, defaults to CURVE_SECP256K1.
- `address_formats` _list_ - Address format list, defaults to ["ADDRESS_FORMAT_ETHEREUM"].
  

**Returns**:

- `dict` - JSON response containing imported private key information.

<a id="spoon_ai.turnkey.client.Turnkey.sign_evm_transaction"></a>

#### `sign_evm_transaction`

```python
def sign_evm_transaction(sign_with, unsigned_tx)
```

Sign EVM transaction using Turnkey.

**Arguments**:

- `sign_with` _str_ - Signing identity (wallet account address / private key address / private key ID).
- `unsigned_tx` _str_ - Raw unsigned transaction (hex string).
  

**Returns**:

- `dict` - JSON response containing signing result, see signTransactionResult.signedTransaction.
  
  Reference:
  https://docs.turnkey.com/api-reference/activities/sign-transaction

<a id="spoon_ai.turnkey.client.Turnkey.sign_typed_data"></a>

#### `sign_typed_data`

```python
def sign_typed_data(sign_with, typed_data)
```

Sign EIP-712 structured data.

**Arguments**:

- `sign_with` _str_ - Signing identity (wallet account address / private key address / private key ID).
- `typed_data` _dict|str_ - EIP-712 structure (domain/types/message) or its JSON string.
  

**Returns**:

- `dict` - Activity response, result contains r/s/v.
  

**Notes**:

  - encoding uses PAYLOAD_ENCODING_EIP712
  - hashFunction uses HASH_FUNCTION_NOT_APPLICABLE (server completes EIP-712 spec hashing)

<a id="spoon_ai.turnkey.client.Turnkey.sign_message"></a>

#### `sign_message`

```python
def sign_message(sign_with, message, use_keccak256=True)
```

Sign arbitrary message (defaults to KECCAK256 following Ethereum convention).

**Arguments**:

- `sign_with` _str_ - Signing identity (wallet account address / private key address / private key ID).
- `message` _str|bytes_ - Text to be signed; bytes will be decoded as UTF-8.
- `use_keccak256` _bool_ - Whether to use KECCAK256 as hash function (default True).
  

**Returns**:

- `dict` - Activity response, result contains r/s/v.

<a id="spoon_ai.turnkey.client.Turnkey.get_activity"></a>

#### `get_activity`

```python
def get_activity(activity_id)
```

Query Activity details.

**Arguments**:

- `activity_id` _str_ - Activity ID.
  

**Returns**:

- `dict` - Activity details.
  
  Reference:
  https://docs.turnkey.com/api-reference/queries/get-activity

<a id="spoon_ai.turnkey.client.Turnkey.list_activities"></a>

#### `list_activities`

```python
def list_activities(limit=None,
                    before=None,
                    after=None,
                    filter_by_status=None,
                    filter_by_type=None)
```

List activities within organization (paginated).

**Arguments**:

- `limit` _str|None_ - Number per page.
- `before` _str|None_ - Pagination cursor (before).
- `after` _str|None_ - Pagination cursor (after).
- `filter_by_status` _list|None_ - Filter by activity status (e.g., ['ACTIVITY_STATUS_COMPLETED']).
- `filter_by_type` _list|None_ - Filter by activity type (e.g., ['ACTIVITY_TYPE_SIGN_TRANSACTION_V2']).
  

**Returns**:

- `dict` - Activity list.
  
  Reference:
  https://docs.turnkey.com/api-reference/queries/list-activities

<a id="spoon_ai.turnkey.client.Turnkey.get_policy_evaluations"></a>

#### `get_policy_evaluations`

```python
def get_policy_evaluations(activity_id)
```

Query policy evaluation results for an Activity (if available).

**Arguments**:

- `activity_id` _str_ - Activity ID.
  

**Returns**:

- `dict` - Policy evaluation details.
  
  Reference:
  https://docs.turnkey.com/api-reference/queries/get-policy-evaluations

<a id="spoon_ai.turnkey.client.Turnkey.get_private_key"></a>

#### `get_private_key`

```python
def get_private_key(private_key_id)
```

Query information for specified private key.

**Arguments**:

- `private_key_id` _str_ - Private key ID.
  

**Returns**:

- `dict` - JSON response containing private key information.

<a id="spoon_ai.turnkey.client.Turnkey.create_wallet"></a>

#### `create_wallet`

```python
def create_wallet(wallet_name, accounts, mnemonic_length=24)
```

Create new wallet.

**Arguments**:

- `wallet_name` _str_ - Wallet name.
- `accounts` _list_ - Account configuration list, each account contains curve, pathFormat, path, addressFormat.
- `mnemonic_length` _int_ - Mnemonic length (default 24).
  

**Returns**:

- `dict` - JSON response containing new wallet information.

<a id="spoon_ai.turnkey.client.Turnkey.create_wallet_accounts"></a>

#### `create_wallet_accounts`

```python
def create_wallet_accounts(wallet_id, accounts)
```

Add accounts to existing wallet.

**Arguments**:

- `wallet_id` _str_ - Wallet ID.
- `accounts` _list_ - New account configuration list, each account contains curve, pathFormat, path, addressFormat.
  

**Returns**:

- `dict` - JSON response containing new account information.

<a id="spoon_ai.turnkey.client.Turnkey.get_wallet"></a>

#### `get_wallet`

```python
def get_wallet(wallet_id)
```

Query information for specified wallet.

**Arguments**:

- `wallet_id` _str_ - Wallet ID.
  

**Returns**:

- `dict` - JSON response containing wallet information.

<a id="spoon_ai.turnkey.client.Turnkey.get_wallet_account"></a>

#### `get_wallet_account`

```python
def get_wallet_account(wallet_id, address=None, path=None)
```

Query information for specified wallet account.

**Arguments**:

- `wallet_id` _str_ - Wallet ID.
- `address` _str, optional_ - Account address.
- `path` _str, optional_ - Account path (e.g., m/44'/60'/0'/0/0).
  

**Returns**:

- `dict` - JSON response containing account information.
  

**Raises**:

- `ValueError` - If neither address nor path is provided.

<a id="spoon_ai.turnkey.client.Turnkey.list_wallets"></a>

#### `list_wallets`

```python
def list_wallets()
```

List all wallets in the organization.

**Returns**:

- `dict` - JSON response containing wallet list.

<a id="spoon_ai.turnkey.client.Turnkey.list_wallet_accounts"></a>

#### `list_wallet_accounts`

```python
def list_wallet_accounts(wallet_id, limit=None, before=None, after=None)
```

List account list for specified wallet.

**Arguments**:

- `wallet_id` _str_ - Wallet ID.
- `limit` _str, optional_ - Number of accounts returned per page.
- `before` _str, optional_ - Pagination cursor, returns accounts before this ID.
- `after` _str, optional_ - Pagination cursor, returns accounts after this ID.
  

**Returns**:

- `dict` - JSON response containing account list.

<a id="spoon_ai.turnkey.client.Turnkey.init_import_wallet"></a>

#### `init_import_wallet`

```python
def init_import_wallet(user_id)
```

Initialize wallet import process, generate import_bundle.

**Arguments**:

- `user_id` _str_ - User ID.
  

**Returns**:

- `dict` - JSON response containing import_bundle.

<a id="spoon_ai.turnkey.client.Turnkey.encrypt_wallet"></a>

#### `encrypt_wallet`

```python
def encrypt_wallet(mnemonic,
                   user_id,
                   import_bundle,
                   encryption_key_name="demo-encryption-key")
```

Encrypt mnemonic using Turnkey CLI, generate encrypted_bundle.

**Arguments**:

- `mnemonic` _str_ - Mnemonic phrase (12/15/18/21/24 words).
- `user_id` _str_ - User ID.
- `import_bundle` _str_ - import_bundle obtained from init_import_wallet.
- `encryption_key_name` _str_ - Encryption key name, defaults to demo-encryption-key.
  

**Returns**:

- `str` - Encrypted encrypted_bundle.
  

**Raises**:

- `RuntimeError` - If CLI command fails or turnkey CLI is not installed.

<a id="spoon_ai.turnkey.client.Turnkey.encrypt_private_key"></a>

#### `encrypt_private_key`

```python
def encrypt_private_key(private_key,
                        user_id,
                        import_bundle,
                        key_format="hexadecimal",
                        encryption_key_name="demo-encryption-key")
```

Encrypt private key using Turnkey CLI, generate encrypted_bundle, equivalent to:
`turnkey encrypt --import-bundle-input "./import_bundle.txt" --plaintext-input /dev/fd/3 --key-format "hexadecimal" --encrypted-bundle-output "./encrypted_bundle.txt"`

**Arguments**:

- `private_key` _str_ - Private key string (hexadecimal or Solana format).
- `user_id` _str_ - User ID.
- `import_bundle` _str_ - import_bundle obtained from init_import_private_key.
- `key_format` _str_ - Private key format, defaults to "hexadecimal" (supports "hexadecimal", "solana").
- `encryption_key_name` _str_ - Encryption key name, defaults to "demo-encryption-key".
  

**Returns**:

- `str` - Encrypted encrypted_bundle (Base64 encoded string).
  

**Raises**:

- `ValueError` - If private_key, user_id, import_bundle is empty or key_format is invalid.
- `RuntimeError` - If CLI command fails or turnkey CLI is not installed.

<a id="spoon_ai.turnkey.client.Turnkey.init_import_private_key"></a>

#### `init_import_private_key`

```python
def init_import_private_key(user_id)
```

Initialize private key import process, generate import_bundle.

**Arguments**:

- `user_id` _str_ - User ID.
  

**Returns**:

- `dict` - JSON response containing import_bundle.

<a id="spoon_ai.turnkey.client.Turnkey.import_wallet"></a>

#### `import_wallet`

```python
def import_wallet(user_id, wallet_name, encrypted_bundle, accounts=None)
```

Import wallet to Turnkey.

**Arguments**:

- `user_id` _str_ - User ID.
- `wallet_name` _str_ - Wallet name.
- `encrypted_bundle` _str_ - Encrypted mnemonic bundle.
- `accounts` _list, optional_ - Account configuration list, each account contains curve, pathFormat, path, addressFormat.
  

**Returns**:

- `dict` - JSON response containing imported wallet information.

<a id="spoon_ai.turnkey"></a>

# Module `spoon_ai.turnkey`

Turnkey client integration for SpoonAI.

Provides `Turnkey` for secure signing via Turnkey API.

<a id="spoon_ai.callbacks.streaming_stdout"></a>

# Module `spoon_ai.callbacks.streaming_stdout`

<a id="spoon_ai.callbacks.streaming_stdout.StreamingStdOutCallbackHandler"></a>

## `StreamingStdOutCallbackHandler` Objects

```python
class StreamingStdOutCallbackHandler(BaseCallbackHandler)
```

Callback handler that streams tokens to standard output.

<a id="spoon_ai.callbacks.streaming_stdout.StreamingStdOutCallbackHandler.on_llm_new_token"></a>

#### `on_llm_new_token`

```python
def on_llm_new_token(token: str, **kwargs: Any) -> None
```

Print token to stdout immediately.

**Arguments**:

- `token` - The new token to print
- `**kwargs` - Additional context (ignored)

<a id="spoon_ai.callbacks.streaming_stdout.StreamingStdOutCallbackHandler.on_llm_end"></a>

#### `on_llm_end`

```python
def on_llm_end(response: Any, **kwargs: Any) -> None
```

Print newline after LLM completes.

**Arguments**:

- `response` - The complete LLM response (ignored)
- `**kwargs` - Additional context (ignored)

<a id="spoon_ai.callbacks.statistics"></a>

# Module `spoon_ai.callbacks.statistics`

<a id="spoon_ai.callbacks.statistics.StreamingStatisticsCallback"></a>

## `StreamingStatisticsCallback` Objects

```python
class StreamingStatisticsCallback(BaseCallbackHandler, LLMManagerMixin)
```

Collect simple throughput statistics during streaming runs.

By default, the callback prints summary metrics when the LLM finishes.
Consumers can provide a custom ``print_fn`` to redirect output, or disable
printing entirely and read the public attributes after execution.

<a id="spoon_ai.callbacks.stream_event"></a>

# Module `spoon_ai.callbacks.stream_event`

<a id="spoon_ai.callbacks.stream_event.StreamEventCallbackHandler"></a>

## `StreamEventCallbackHandler` Objects

```python
class StreamEventCallbackHandler(BaseCallbackHandler)
```

Translate callback invocations into standardized stream events.

<a id="spoon_ai.callbacks.manager"></a>

# Module `spoon_ai.callbacks.manager`

<a id="spoon_ai.callbacks.manager.CallbackManager"></a>

## `CallbackManager` Objects

```python
class CallbackManager()
```

Lightweight dispatcher for callback handlers.

<a id="spoon_ai.callbacks"></a>

# Module `spoon_ai.callbacks`

Callback system for streaming and event handling in Spoon AI.

This module provides a comprehensive callback system similar to LangChain's callbacks,
enabling real-time monitoring and event handling for LLM calls, agent execution,
tool invocation, and graph workflows.

<a id="spoon_ai.callbacks.base"></a>

# Module `spoon_ai.callbacks.base`

<a id="spoon_ai.callbacks.base.RetrieverManagerMixin"></a>

## `RetrieverManagerMixin` Objects

```python
class RetrieverManagerMixin()
```

Mixin providing retriever callback hooks.

<a id="spoon_ai.callbacks.base.RetrieverManagerMixin.on_retriever_start"></a>

#### `on_retriever_start`

```python
def on_retriever_start(run_id: UUID, query: Any, **kwargs: Any) -> Any
```

Run when a retriever begins execution.

<a id="spoon_ai.callbacks.base.RetrieverManagerMixin.on_retriever_end"></a>

#### `on_retriever_end`

```python
def on_retriever_end(run_id: UUID, documents: Any, **kwargs: Any) -> Any
```

Run when a retriever finishes successfully.

<a id="spoon_ai.callbacks.base.RetrieverManagerMixin.on_retriever_error"></a>

#### `on_retriever_error`

```python
def on_retriever_error(error: BaseException, *, run_id: UUID,
                       **kwargs: Any) -> Any
```

Run when a retriever raises an error.

<a id="spoon_ai.callbacks.base.LLMManagerMixin"></a>

## `LLMManagerMixin` Objects

```python
class LLMManagerMixin()
```

Mixin providing large language model callback hooks.

<a id="spoon_ai.callbacks.base.LLMManagerMixin.on_llm_start"></a>

#### `on_llm_start`

```python
def on_llm_start(run_id: UUID, messages: List[Message], **kwargs: Any) -> Any
```

Run when an LLM or chat model begins execution.

<a id="spoon_ai.callbacks.base.LLMManagerMixin.on_llm_new_token"></a>

#### `on_llm_new_token`

```python
def on_llm_new_token(token: str,
                     *,
                     chunk: Optional[LLMResponseChunk] = None,
                     run_id: Optional[UUID] = None,
                     **kwargs: Any) -> Any
```

Run for each streamed token emitted by an LLM.

<a id="spoon_ai.callbacks.base.LLMManagerMixin.on_llm_end"></a>

#### `on_llm_end`

```python
def on_llm_end(response: LLMResponse, *, run_id: UUID, **kwargs: Any) -> Any
```

Run when an LLM finishes successfully.

<a id="spoon_ai.callbacks.base.LLMManagerMixin.on_llm_error"></a>

#### `on_llm_error`

```python
def on_llm_error(error: BaseException, *, run_id: UUID, **kwargs: Any) -> Any
```

Run when an LLM raises an error.

<a id="spoon_ai.callbacks.base.ChainManagerMixin"></a>

## `ChainManagerMixin` Objects

```python
class ChainManagerMixin()
```

Mixin providing chain-level callback hooks.

<a id="spoon_ai.callbacks.base.ChainManagerMixin.on_chain_start"></a>

#### `on_chain_start`

```python
def on_chain_start(run_id: UUID, inputs: Any, **kwargs: Any) -> Any
```

Run when a chain (Runnable) starts executing.

<a id="spoon_ai.callbacks.base.ChainManagerMixin.on_chain_end"></a>

#### `on_chain_end`

```python
def on_chain_end(run_id: UUID, outputs: Any, **kwargs: Any) -> Any
```

Run when a chain finishes successfully.

<a id="spoon_ai.callbacks.base.ChainManagerMixin.on_chain_error"></a>

#### `on_chain_error`

```python
def on_chain_error(error: BaseException, *, run_id: UUID,
                   **kwargs: Any) -> Any
```

Run when a chain raises an error.

<a id="spoon_ai.callbacks.base.ToolManagerMixin"></a>

## `ToolManagerMixin` Objects

```python
class ToolManagerMixin()
```

Mixin providing tool callback hooks.

<a id="spoon_ai.callbacks.base.ToolManagerMixin.on_tool_start"></a>

#### `on_tool_start`

```python
def on_tool_start(tool_name: str, tool_input: Any, *, run_id: UUID,
                  **kwargs: Any) -> Any
```

Run when a tool invocation begins.

<a id="spoon_ai.callbacks.base.ToolManagerMixin.on_tool_end"></a>

#### `on_tool_end`

```python
def on_tool_end(tool_name: str, tool_output: Any, *, run_id: UUID,
                **kwargs: Any) -> Any
```

Run when a tool invocation succeeds.

<a id="spoon_ai.callbacks.base.ToolManagerMixin.on_tool_error"></a>

#### `on_tool_error`

```python
def on_tool_error(error: BaseException,
                  *,
                  run_id: UUID,
                  tool_name: Optional[str] = None,
                  **kwargs: Any) -> Any
```

Run when a tool invocation raises an error.

<a id="spoon_ai.callbacks.base.PromptManagerMixin"></a>

## `PromptManagerMixin` Objects

```python
class PromptManagerMixin()
```

Mixin providing prompt template callback hooks.

<a id="spoon_ai.callbacks.base.PromptManagerMixin.on_prompt_start"></a>

#### `on_prompt_start`

```python
def on_prompt_start(run_id: UUID, inputs: Any, **kwargs: Any) -> Any
```

Run when a prompt template begins formatting.

<a id="spoon_ai.callbacks.base.PromptManagerMixin.on_prompt_end"></a>

#### `on_prompt_end`

```python
def on_prompt_end(run_id: UUID, output: Any, **kwargs: Any) -> Any
```

Run when a prompt template finishes formatting.

<a id="spoon_ai.callbacks.base.PromptManagerMixin.on_prompt_error"></a>

#### `on_prompt_error`

```python
def on_prompt_error(error: BaseException, *, run_id: UUID,
                    **kwargs: Any) -> Any
```

Run when prompt formatting raises an error.

<a id="spoon_ai.callbacks.base.BaseCallbackHandler"></a>

## `BaseCallbackHandler` Objects

```python
class BaseCallbackHandler(LLMManagerMixin, ChainManagerMixin, ToolManagerMixin,
                          RetrieverManagerMixin, PromptManagerMixin, ABC)
```

Base class for SpoonAI callback handlers.

<a id="spoon_ai.callbacks.base.BaseCallbackHandler.raise_error"></a>

#### `raise_error`

Whether to re-raise exceptions originating from callbacks.

<a id="spoon_ai.callbacks.base.BaseCallbackHandler.run_inline"></a>

#### `run_inline`

Whether the callback prefers to run on the caller's event loop.

<a id="spoon_ai.callbacks.base.BaseCallbackHandler.ignore_llm"></a>

#### `ignore_llm`

```python
@property
def ignore_llm() -> bool
```

Return True to skip LLM callbacks.

<a id="spoon_ai.callbacks.base.BaseCallbackHandler.ignore_chain"></a>

#### `ignore_chain`

```python
@property
def ignore_chain() -> bool
```

Return True to skip chain callbacks.

<a id="spoon_ai.callbacks.base.BaseCallbackHandler.ignore_tool"></a>

#### `ignore_tool`

```python
@property
def ignore_tool() -> bool
```

Return True to skip tool callbacks.

<a id="spoon_ai.callbacks.base.BaseCallbackHandler.ignore_retriever"></a>

#### `ignore_retriever`

```python
@property
def ignore_retriever() -> bool
```

Return True to skip retriever callbacks.

<a id="spoon_ai.callbacks.base.BaseCallbackHandler.ignore_prompt"></a>

#### `ignore_prompt`

```python
@property
def ignore_prompt() -> bool
```

Return True to skip prompt callbacks.

<a id="spoon_ai.callbacks.base.AsyncCallbackHandler"></a>

## `AsyncCallbackHandler` Objects

```python
class AsyncCallbackHandler(BaseCallbackHandler)
```

Async version of the callback handler base class.

<a id="spoon_ai.schema"></a>

# Module `spoon_ai.schema`

<a id="spoon_ai.schema.Function"></a>

## `Function` Objects

```python
class Function(BaseModel)
```

<a id="spoon_ai.schema.Function.get_arguments_dict"></a>

#### `get_arguments_dict`

```python
def get_arguments_dict() -> dict
```

Parse arguments string to dictionary.

**Returns**:

- `dict` - Parsed arguments as dictionary

<a id="spoon_ai.schema.Function.create"></a>

#### `create`

```python
@classmethod
def create(cls, name: str, arguments: Union[str, dict]) -> "Function"
```

Create Function with arguments as string or dict.

**Arguments**:

- `name` - Function name
- `arguments` - Function arguments as string or dict
  

**Returns**:

- `Function` - Function instance with arguments as JSON string

<a id="spoon_ai.schema.AgentState"></a>

## `AgentState` Objects

```python
class AgentState(str, Enum)
```

The state of the agent.

<a id="spoon_ai.schema.ToolChoice"></a>

## `ToolChoice` Objects

```python
class ToolChoice(str, Enum)
```

Tool choice options

<a id="spoon_ai.schema.Role"></a>

## `Role` Objects

```python
class Role(str, Enum)
```

Message role options

<a id="spoon_ai.schema.ROLE_TYPE"></a>

#### `ROLE_TYPE`

type: ignore

<a id="spoon_ai.schema.Message"></a>

## `Message` Objects

```python
class Message(BaseModel)
```

Represents a chat message in the conversation

<a id="spoon_ai.schema.Message.role"></a>

#### `role`

type: ignore

<a id="spoon_ai.schema.SystemMessage"></a>

## `SystemMessage` Objects

```python
class SystemMessage(Message)
```

<a id="spoon_ai.schema.SystemMessage.role"></a>

#### `role`

type: ignore

<a id="spoon_ai.schema.TOOL_CHOICE_TYPE"></a>

#### `TOOL_CHOICE_TYPE`

type: ignore

<a id="spoon_ai.schema.LLMConfig"></a>

## `LLMConfig` Objects

```python
class LLMConfig(BaseModel)
```

Configuration for LLM providers

<a id="spoon_ai.schema.LLMResponse"></a>

## `LLMResponse` Objects

```python
class LLMResponse(BaseModel)
```

Unified LLM response model

<a id="spoon_ai.schema.LLMResponse.text"></a>

#### `text`

Original text response

<a id="spoon_ai.schema.LLMResponseChunk"></a>

## `LLMResponseChunk` Objects

```python
class LLMResponseChunk(BaseModel)
```

Enhanced LLM streaming response chunk.

<a id="spoon_ai.memory.utils"></a>

# Module `spoon_ai.memory.utils`

Memory helpers shared across Mem0 demos and utilities.

<a id="spoon_ai.memory.utils.extract_memories"></a>

#### `extract_memories`

```python
def extract_memories(result: Any) -> List[str]
```

Normalize Mem0 search/get responses into a list of memory strings.
Supports common shapes: &#123;"memories": [...]&#125;, &#123;"results": [...]&#125;, &#123;"data": [...]&#125;, list, or scalar.

<a id="spoon_ai.memory.utils.extract_first_memory_id"></a>

#### `extract_first_memory_id`

```python
def extract_first_memory_id(result: Any) -> Optional[str]
```

Pull the first memory id from Mem0 responses.
Supports common id fields: id, _id, memory_id, uuid.

<a id="spoon_ai.memory.short_term_manager"></a>

# Module `spoon_ai.memory.short_term_manager`

Short-term memory management for conversation history.

<a id="spoon_ai.memory.short_term_manager.TrimStrategy"></a>

## `TrimStrategy` Objects

```python
class TrimStrategy(str, Enum)
```

Strategy for trimming messages.

<a id="spoon_ai.memory.short_term_manager.TrimStrategy.FROM_START"></a>

#### `FROM_START`

Remove oldest messages first

<a id="spoon_ai.memory.short_term_manager.TrimStrategy.FROM_END"></a>

#### `FROM_END`

Remove newest messages first

<a id="spoon_ai.memory.short_term_manager.MessageTokenCounter"></a>

## `MessageTokenCounter` Objects

```python
class MessageTokenCounter()
```

Approximate token counter aligned with LangChain semantics.

<a id="spoon_ai.memory.short_term_manager.ShortTermMemoryManager"></a>

## `ShortTermMemoryManager` Objects

```python
class ShortTermMemoryManager()
```

Manager for short-term conversation memory with advanced operations.

<a id="spoon_ai.memory.short_term_manager.ShortTermMemoryManager.trim_messages"></a>

#### `trim_messages`

```python
async def trim_messages(messages: List[Message],
                        max_tokens: int,
                        strategy: TrimStrategy = TrimStrategy.FROM_END,
                        keep_system: bool = True,
                        model: Optional[str] = None) -> List[Message]
```

Trim messages using a LangChain-style heuristic.

<a id="spoon_ai.memory.short_term_manager.ShortTermMemoryManager.summarize_messages"></a>

#### `summarize_messages`

```python
async def summarize_messages(
    messages: List[Message],
    max_tokens_before_summary: int,
    messages_to_keep: int = 5,
    summary_model: Optional[str] = None,
    llm_manager=None,
    llm_provider: Optional[str] = None,
    existing_summary: str = ""
) -> Tuple[List[Message], List[RemoveMessage], Optional[str]]
```

Summarize earlier messages and emit removal directives.

<a id="spoon_ai.memory.remove_message"></a>

# Module `spoon_ai.memory.remove_message`

Helpers for emitting message-removal directives.

<a id="spoon_ai.memory.remove_message.RemoveMessage"></a>

## `RemoveMessage` Objects

```python
class RemoveMessage(BaseModel)
```

Lightweight message that signals another message should be removed.

<a id="spoon_ai.memory"></a>

# Module `spoon_ai.memory`

Short-term memory management for conversation history.

This module provides memory management utilities for maintaining and optimizing
conversation history in chat applications.

<a id="spoon_ai.memory.mem0_client"></a>

# Module `spoon_ai.memory.mem0_client`

<a id="spoon_ai.memory.mem0_client.SpoonMem0"></a>

## `SpoonMem0` Objects

```python
class SpoonMem0()
```

Lightweight wrapper around Mem0's MemoryClient with safe defaults.

<a id="spoon_ai.memory.mem0_client.SpoonMem0.add_text"></a>

#### `add_text`

```python
def add_text(data: str,
             user_id: Optional[str] = None,
             metadata: Optional[Dict[str, Any]] = None) -> None
```

Convenience helper for adding a single text memory.

<a id="spoon_ai.memory.mem0_client.SpoonMem0.get_all_memory"></a>

#### `get_all_memory`

```python
def get_all_memory(user_id: Optional[str] = None,
                   limit: Optional[int] = None) -> List[str]
```

Retrieve all memories for a user (subject to backend limits).

<a id="spoon_ai.retrieval.chroma"></a>

# Module `spoon_ai.retrieval.chroma`

<a id="spoon_ai.retrieval.qdrant"></a>

# Module `spoon_ai.retrieval.qdrant`

<a id="spoon_ai.retrieval.document_loader"></a>

# Module `spoon_ai.retrieval.document_loader`

<a id="spoon_ai.retrieval.document_loader.BasicTextSplitter"></a>

## `BasicTextSplitter` Objects

```python
class BasicTextSplitter()
```

Simple text splitter to replace langchain's RecursiveCharacterTextSplitter

<a id="spoon_ai.retrieval.document_loader.BasicTextSplitter.split_text"></a>

#### `split_text`

```python
def split_text(text: str) -> List[str]
```

Split text into chunks

<a id="spoon_ai.retrieval.document_loader.BasicTextSplitter.split_documents"></a>

#### `split_documents`

```python
def split_documents(documents: List[Document]) -> List[Document]
```

Split document collection into smaller document chunks

<a id="spoon_ai.retrieval.document_loader.DocumentLoader"></a>

## `DocumentLoader` Objects

```python
class DocumentLoader()
```

<a id="spoon_ai.retrieval.document_loader.DocumentLoader.load_directory"></a>

#### `load_directory`

```python
def load_directory(directory_path: str,
                   glob_pattern: Optional[str] = None) -> List[Document]
```

Load documents from a directory

<a id="spoon_ai.retrieval.document_loader.DocumentLoader.load_file"></a>

#### `load_file`

```python
def load_file(file_path: str) -> List[Document]
```

Load a single file and return the documents

<a id="spoon_ai.retrieval"></a>

# Module `spoon_ai.retrieval`

<a id="spoon_ai.retrieval.base"></a>

# Module `spoon_ai.retrieval.base`

<a id="spoon_ai.retrieval.base.BaseRetrievalClient"></a>

## `BaseRetrievalClient` Objects

```python
class BaseRetrievalClient()
```

Abstract base class for retrieval clients.

---

FILE: docs/cli/advanced-features.md

# Advanced CLI Features

This guide covers advanced  features including MCP integration, custom agents, scripting, and automation capabilities.

## Model Context Protocol (MCP) Integration

### MCP Overview

MCP allows  to integrate with external tools and services through standardized protocols.

### Configuring MCP Servers

Add MCP server configurations to your `config.json`:

```json
{
  "mcp_servers": {
    "filesystem": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-filesystem", "/Users/username/Documents"]
    },
    "github": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-github"],
      "env": {
        "GITHUB_PERSONAL_ACCESS_TOKEN": "ghp_your_token_here"
      }
    },
    "slack": {
      "command": "uvx",
      "args": ["mcp-server-slack"],
      "env": {
        "SLACK_BOT_TOKEN": "xoxb-your-bot-token"
      }
    }
  }
}
```

### MCP Agent Configuration

Create agents with MCP support:

```json
{
  "agents": {
    "mcp_agent": {
      "class_name": "SpoonReactMCP",
      "description": "Agent with full MCP integration",
      "mcp_servers": ["filesystem", "github", "slack"],
      "config": {
        "llm_provider": "anthropic",
        "model_name": "claude-3-sonnet-20240229"
      }
    }
  }
}
```

### Testing MCP Connections

```bash
# Start spoon-cli and run:
# Validate MCP server configurations
validate-config --check-servers

# Load MCP-enabled agent
load-agent mcp_agent

# List available MCP tools
action list_mcp_tools
```

## Custom Agent Development

### Agent Configuration Structure

```json
{
  "agents": {
    "custom_agent": {
      "class_name": "SpoonReactAI",
      "description": "Description of your custom agent",
      "aliases": ["ca", "custom"],
      "config": {
        "llm_provider": "anthropic",
        "model_name": "claude-3-sonnet-20240229",
        "temperature": 0.7,
        "max_tokens": 4000,
        "custom_parameter": "value"
      },
      "tools": [
        "web_search",
        "calculator",
        "custom_tool_1",
        "custom_tool_2"
      ],
      "system_prompt": "Custom system prompt for specialized behavior"
    }
  }
}
```

### Specialized Agent Types

#### Research Agent

```json
{
  "research_agent": {
    "class_name": "SpoonReactAI",
    "description": "Specialized research and analysis agent",
    "config": {
      "llm_provider": "openai",
      "model_name": "gpt-4-turbo-preview",
      "temperature": 0.3
    },
    "tools": [
      "web_search",
      "academic_search",
      "data_analysis",
      "citation_manager"
    ]
  }
}
```

#### Trading Agent

```json
{
  "trading_agent": {
    "class_name": "SpoonReactAI",
    "description": "Cryptocurrency trading assistant",
    "config": {
      "llm_provider": "anthropic",
      "model_name": "claude-3-sonnet-20240229",
      "temperature": 0.2
    },
    "tools": [
      "crypto_price_lookup",
      "dex_swap",
      "wallet_balance",
      "market_data",
      "technical_indicators"
    ]
  }
}
```

#### Code Review Agent

```json
{
  "code_review_agent": {
    "class_name": "SpoonReactAI",
    "description": "Code review and development assistant",
    "config": {
      "llm_provider": "openai",
      "model_name": "gpt-4",
      "temperature": 0.1
    },
    "tools": [
      "file_operations",
      "git_operations",
      "code_analysis",
      "testing_tools"
    ]
  }
}
```

## Scripting and Automation

### Command Chaining

Execute multiple commands in sequence:

```bash
# Setup and start session (these would be run in spoon-cli interactive mode)
# load-agent research_agent
# load-toolkit-tools web_search academic
# action chat
```

### Batch Operations

Create scripts for automated workflows:

```bash
#!/bin/bash
# research_workflow.sh

# Note: This script would need to be run in spoon-cli interactive mode
# Start spoon-cli first, then run these commands:
# load-agent research_agent
# load-toolkit-tools web_search data_analysis
# load-docs ./research_papers/
# action chat

# For automation, you would need to:
echo "Starting automated research analysis..."
# spoon-cli action chat << EOF
Please analyze the loaded documents and provide a comprehensive summary of the key findings, methodologies, and conclusions.
EOF
```

### Environment-Specific Configurations

Use different configurations for different environments:

```json
{
  "profiles": {
    "development": {
      "default_agent": "debug_agent",
      "llm": {
        "timeout": 60,
        "retry_attempts": 5
      },
      "logging": {
        "level": "DEBUG"
      }
    },
    "production": {
      "default_agent": "production_agent",
      "llm": {
        "timeout": 30,
        "retry_attempts": 2
      },
      "logging": {
        "level": "WARNING"      }
    }
  }
}
```

Activate profiles:

```bash
export SPOON_CLI_PROFILE="production"
# Start spoon-cli and run:
system-info
```

## Advanced Tool Integration

### Custom Tool Development

#### Tool Factory Pattern

```json
{
  "tool_factories": {
    "custom_api_tool": {
      "factory": "my_package.tools.CustomAPIToolFactory",
      "config": {
        "api_endpoint": "https://api.example.com",
        "api_key": "${API_KEY}"
      }
    }
  }
}
```

#### Dynamic Tool Loading

```bash
# Start spoon-cli and run:
# Load tools from specific categories
load-toolkit-tools crypto blockchain

# Load all available tools (use with caution)
load-toolkit-tools all

# Load tools with specific configuration
load-toolkit-tools web_search --timeout 30 --max-results 20
```

### Tool Configuration

Advanced tool configuration in `config.json`:

```json
{
  "tool_configs": {
    "web_search": {
      "max_results": 10,
      "timeout": 30,
      "user_agent": "SpoonAI/1.0",
      "proxies": {
        "http": "http://proxy.company.com:8080",
        "https": "https://proxy.company.com:8080"
      }
    },
    "crypto_price_lookup": {
      "cache_timeout": 300,
      "preferred_exchanges": ["binance", "coinbase", "kraken"]
    }
  }
}
```

## Performance Optimization

### LLM Configuration Tuning

```json
{
  "llm": {
    "default_provider": "anthropic",
    "fallback_chain": ["anthropic", "openai", "deepseek"],
    "timeout": 30,
    "retry_attempts": 3,
    "rate_limiting": {
      "requests_per_minute": 60,
      "burst_limit": 10
    },
    "caching": {
      "enabled": true,
      "ttl": 3600,
      "max_size": "1GB"
    }
  }
}
```

### Memory Management

```json
{
  "memory": {
    "max_chat_history": 100,
    "compress_old_messages": true,
    "auto_save_interval": 300,
    "cleanup_temp_files": true
  }
}
```

### Parallel Processing

```json
{
  "parallel_processing": {
    "max_concurrent_requests": 5,
    "thread_pool_size": 10,
    "async_timeout": 60
  }
}
```

## Logging and Debugging

### Advanced Logging Configuration

```json
{
  "logging": {
    "level": "INFO",
    "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    "file": "spoon_cli.log",
    "max_file_size": "10MB",
    "backup_count": 5,
    "handlers": {
      "console": {
        "level": "WARNING",
        "format": "%(levelname)s: %(message)s"
      },
      "file": {
        "level": "DEBUG",
        "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
      }
    }
  }
}
```

### Debug Mode

Enable detailed debugging:

```bash
export SPOON_CLI_DEBUG=1

# Or use command-line flag when starting spoon-cli
# spoon-cli --debug

# Then run:
# system-info
```

### Performance Profiling

```bash
# Enable performance logging
export SPOON_CLI_PROFILE=1

# Start spoon-cli and run:
# action chat

# Check performance metrics
# system-info --performance
```

## Security Features

### API Key Management

```json
{
  "security": {
    "api_key_rotation": {
      "enabled": true,
      "interval_days": 30
    },
    "key_validation": {
      "enabled": true,
      "cache_timeout": 3600
    },
    "audit_logging": {
      "enabled": true,
      "log_sensitive_operations": false
    }
  }
}
```

### Network Security

```json
{
  "network": {
    "ssl_verification": true,
    "proxy": "http://proxy.company.com:8080",
    "timeout": 30,
    "retry_on_failure": true,
    "trusted_domains": [
      "*.openai.com",
      "*.anthropic.com",
      "*.googleapis.com"
    ]
  }
}
```

## Integration Features

### Webhook Support

Configure webhooks for external integrations:

```json
{
  "webhooks": {
    "telegram_bot": {
      "enabled": true,
      "token": "${TELEGRAM_BOT_TOKEN}",
      "allowed_users": ["user1", "user2"]
    },
    "slack_integration": {
      "enabled": false,
      "webhook_url": "${SLACK_WEBHOOK_URL}",
      "channel": "#ai-notifications"
    }
  }
}
```

### API Server Mode

Run as an API server:

```bash
# Start spoon-cli and run:
serve --host 0.0.0.0 --port 8000
```

### Database Integration

```json
{
  "database": {
    "type": "postgresql",
    "url": "${DATABASE_URL}",
    "connection_pool": {
      "min_size": 5,
      "max_size": 20
    }
  }
}
```

## Custom Extensions

### Plugin System

Load custom plugins:

```json
{
  "plugins": {
    "my_custom_plugin": {
      "path": "/path/to/my/plugin",
      "enabled": true,
      "config": {
        "custom_setting": "value"
      }
    }
  }
}
```

### Custom Commands

Extend CLI with custom commands:

```python
# custom_commands.py
from spoon_cli.commands import SpoonCommand

def my_custom_command(input_list):
    # Custom command implementation
    print("Custom command executed!")

# Register command
custom_command = SpoonCommand(
    name="my-command",
    description="My custom command",
    handler=my_custom_command
)
```

## Monitoring and Analytics

### Usage Analytics

```json
{
  "analytics": {
    "enabled": true,
    "metrics": {
      "commands_used": true,
      "response_times": true,
      "error_rates": true
    },
    "reporting": {
      "interval": "1h",
      "destination": "console"
    }
  }
}
```

### Health Monitoring

```bash
# Start spoon-cli and run:
# Continuous health monitoring
monitor --interval 60

# Export metrics
export-metrics --format json --output metrics.json
```

## Next Steps

- [Troubleshooting](./troubleshooting.md) - Solve advanced issues
- [Configuration Guide](./configuration.md) - Master configuration options
- [API Reference](../api-reference/index) - Complete command reference

---

FILE: docs/cli/basic-usage.md

# Basic CLI Usage

This guide covers the fundamental operations of spoon-cli, from starting your first session to performing common tasks.

## Starting the CLI

### Interactive Mode

The most common way to use spoon-cli is through interactive mode:

```bash
spoon-cli
```

This starts the interactive command-line interface where you can enter commands and chat with agents.

### Interactive Commands

Once in the spoon-cli interactive mode, you can run various commands:

```bash
# Load an agent and start chat
action chat

# List available agents
list-agents

# Show system information
system-info
```

## Core Commands

### Help System

Get help on available commands:

```bash
# Show all commands
help

# Get help for specific command
help load-agent
```

### Agent Management

#### List Available Agents

```bash
list-agents
```

Shows all configured agents with their descriptions and aliases.

#### Load an Agent

```bash
load-agent react
# or using alias
load-agent r
```

Loads the specified agent for use in subsequent operations.

#### Check Current Agent

The prompt shows the currently loaded agent:

```
Spoon AI (react) >
```

### Chat Operations

#### Start Chat Mode

```bash
action chat
```

Starts an interactive chat session with the current agent.

#### Create New Chat

```bash
new-chat
```

Clears the current chat history and starts fresh.

#### List Chat Histories

```bash
list-chats
```

Shows available saved chat sessions.

#### Load Previous Chat

```bash
load-chat react_session_001
```

Loads a previously saved chat session.

### Configuration Management

#### View Configuration

```bash
config
```

Shows all current configuration settings.

#### Set Configuration Values

```bash
# Set API key
config api_key openai "sk-your-key-here"

# Set default agent
config default_agent "my_agent"

# Set LLM provider
config llm.default_provider "anthropic"
```

#### Reload Configuration

```bash
reload-config
```

Reloads configuration after making changes to config files.

## Interactive Chat Mode

### Starting Chat

Once you enter chat mode (`action chat`), you'll see:

```
Spoon AI (react) > action chat
Starting chat with react
Type your message and press Enter to send.
Press Ctrl+C or Ctrl+D to exit chat mode.
Chat log will be saved to: chat_logs/chat_react_20241201_143022.txt

You >
```

### Basic Chat Interaction

```bash
You > Hello, can you help me analyze some cryptocurrency data?

react: I'd be happy to help you analyze cryptocurrency data. What specific cryptocurrencies or data are you interested in? I have access to various crypto analysis tools including price lookups, market data, and technical indicators.
```

### Special Commands in Chat

While in chat mode, you can use special commands:

- `Ctrl+C` or `Ctrl+D`: Exit chat mode
- Multi-line input: Continue typing, press Enter twice to send

### Chat History

All conversations are automatically saved to timestamped files in the `chat_logs/` directory.

## Tool Integration

### List Available Toolkits

```bash
list-toolkit-categories
```

Shows all available tool categories.

### List Tools in Category

```bash
list-toolkit-tools crypto
```

Shows tools available in the crypto category.

### Load Toolkit Tools

```bash
load-toolkit-tools crypto web_search
```

Loads tools from specified categories into the current agent.

## Document Operations

### Load Documents

```bash
load-docs /path/to/documents
load-docs /path/to/file.pdf
load-docs /path/to/folder --glob "*.txt"
```

Loads documents into the current agent for analysis and querying.

## System Information

### Health Check

```bash
system-info
```

Provides comprehensive system information including:
- Environment variables status
- Configuration file status
- Available agents and tools
- API key configuration
- Health score and recommendations

### LLM Status

```bash
llm-status
```

Shows LLM provider configuration and availability status.

## Blockchain Operations

### Token Information

```bash
token-info 0xA0b86a33E6441e88C5F2712C3E9b74E39E9f6E5a
token-by-symbol ETH
```

Get information about specific tokens.

### Transfer Tokens

```bash
transfer 0x742d35Cc6634C0532925a3b844Bc454e4438f44e 1.5
transfer 0x742d35Cc6634C0532925a3b844Bc454e4438f44e 100 USDC
```

Transfer native tokens or ERC-20 tokens.

### Token Swapping

```bash
swap ETH USDC 1.0
swap UNI WETH 100 --slippage 0.5
```

Swap tokens using integrated DEX aggregator.

## Social Media Integration

### Telegram Bot

```bash
telegram
```

Starts the Telegram bot client for social media interactions.

## Configuration Validation

### Validate Configuration

```bash
validate-config
```

Checks configuration for issues and missing requirements.

### Check Migration Status

```bash
check-config
```

Checks if configuration needs migration to new format.

### Migrate Configuration

```bash
migrate-config
```

Migrates legacy configuration to the new unified format.

## Command-line Options

### Global Options

```bash
--help              # Show help
--version           # Show version
--config FILE       # Use specific config file
--debug             # Enable debug mode
```

### Command-specific Options

```bash
migrate-config --dry-run    # Preview migration
validate-config --check-env # Check environment variables
```

## Examples

### Complete Workflow

```bash
# Start spoon-cli
spoon-cli

# Then run these commands in interactive mode:
# 1. Check system status
system-info

# 2. Configure API keys
config api_key openai "sk-your-key"
config api_key anthropic "sk-ant-your-key"

# 3. List and load agent
list-agents
load-agent react

# 4. Load useful tools
load-toolkit-tools crypto web_search

# 5. Start chatting
action chat
```

### Crypto Analysis Session

```bash
# Load crypto-focused agent
load-agent crypto_analyzer

# Load crypto tools
load-toolkit-tools crypto

# Analyze specific token
action chat
# Then ask: "Analyze the current market sentiment for BTC and ETH"
```

### Document Analysis

```bash
# Load agent and documents
load-agent react
load-docs ./research_papers/

# Start analysis
action chat
# Then ask: "Summarize the key findings from the loaded documents"
```

## Keyboard Shortcuts

### Main CLI
- `Ctrl+C`: Interrupt current operation
- `Ctrl+D`: Exit CLI (at main prompt)
- `â†‘/â†“`: Navigate command history
- `Tab`: Auto-complete commands

### Chat Mode
- `Ctrl+C`: Exit chat mode
- `Ctrl+D`: Exit chat mode
- `â†‘/â†“`: Navigate message history

## Best Practices

### Session Management

1. **Use descriptive agent names** for different tasks
2. **Save important chats** with meaningful filenames
3. **Regularly update configuration** as needs change

### Performance

1. **Load only needed tools** to reduce startup time
2. **Use appropriate models** for your tasks
3. **Monitor system resources** with `system-info`

### Security

1. **Never share API keys** in chat sessions
2. **Use environment variables** for sensitive data
3. **Regularly rotate API keys**

## Next Steps

- [Advanced Features](./advanced-features.md) - Learn advanced CLI capabilities
- [Configuration Guide](./configuration.md) - Deep dive into configuration options
- [Troubleshooting](./troubleshooting.md) - Solve common issues

---

FILE: docs/cli/configuration.md

# CLI Configuration

The  uses a flexible, multi-layered configuration system that allows you to customize behavior through various methods. This guide covers all configuration options and methods.

## Configuration Hierarchy

Configurations are applied in the following order (later sources override earlier ones):

1. **Built-in defaults** - Predefined sensible defaults
2. **Global config file** - `~/.spoon/config.json` or `~/.config/spoon/config.json`
3. **Project config file** - `./config.json` in current directory
4. **Environment variables** - Shell environment variables
5. **Command-line arguments** - Flags and options passed to commands

## Configuration File

The primary configuration method is through a JSON configuration file. Create `config.json` in your project root or home directory.

### Basic Configuration

```json
{
  "default_agent": "react",
  "api_keys": {
    "openai": "sk-your-openai-key-here",
    "anthropic": "sk-ant-your-anthropic-key-here"
  },
  "agents": {
    "my_agent": {
      "class_name": "SpoonReactAI",
      "description": "My custom agent",
      "config": {
        "llm_provider": "openai",
        "model_name": "gpt-4",
        "temperature": 0.7
      },
      "tools": ["web_search", "calculator"]
    }
  }
}
```

### Advanced Configuration

```json
{
  "default_agent": "react",
  "api_keys": {
    "openai": "sk-...",
    "anthropic": "sk-ant-...",
    "deepseek": "sk-...",
    "gemini": "your-gemini-key"
  },
  "agents": {
    "react": {
      "class_name": "SpoonReactAI",
      "description": "Standard React agent",
      "aliases": ["r"],
      "config": {
        "llm_provider": "anthropic",
        "model_name": "claude-3-sonnet-20240229",
        "temperature": 0.7,
        "max_tokens": 4000
      },
      "tools": ["web_search", "file_operations", "calculator"]
    },
    "mcp_agent": {
      "class_name": "SpoonReactMCP",
      "description": "Agent with MCP support",
      "config": {
        "llm_provider": "openai",
        "model_name": "gpt-4-turbo-preview"
      },
      "mcp_servers": ["filesystem", "github"]
    }
  },
  "mcp_servers": {
    "filesystem": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-filesystem", "/Users/username/Documents"]
    },
    "github": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-github"],
      "env": {
        "GITHUB_PERSONAL_ACCESS_TOKEN": "ghp_..."
      }
    },
    "slack": {
      "command": "uvx",
      "args": ["mcp-server-slack"],
      "env": {
        "SLACK_BOT_TOKEN": "xoxb-..."
      }
    }
  },
  "llm": {
    "fallback_chain": ["anthropic", "openai", "deepseek"],
    "timeout": 30,
    "retry_attempts": 3
  },
  "logging": {
    "level": "INFO",
    "file": "spoon_cli.log",
    "max_file_size": "10MB",
    "backup_count": 5
  }
}
```

## Environment Variables

Environment variables provide an alternative way to configure sensitive information like API keys.

### LLM Provider API Keys

```bash
# Primary LLM providers
export OPENAI_API_KEY="sk-proj-..."
export ANTHROPIC_API_KEY="sk-ant-api03-..."
export DEEPSEEK_API_KEY="sk-..."

# Additional providers
export GEMINI_API_KEY="AIza..."
export OPENROUTER_API_KEY="sk-or-v1-..."
export TAVILY_API_KEY="tvly-..."
```

### Blockchain Configuration

```bash
export PRIVATE_KEY="0x..."
export RPC_URL="https://mainnet.infura.io/v3/YOUR_PROJECT_ID"
export CHAIN_ID="1"
export SCAN_URL="https://etherscan.io"
```

### Application Settings

```bash
export SPOON_CLI_CONFIG_FILE="/path/to/custom/config.json"
export SPOON_CLI_LOG_LEVEL="DEBUG"
export SPOON_CLI_DEFAULT_AGENT="my_agent"
```

## Configuration Management Commands

The CLI provides built-in commands to manage configuration:

### View Configuration

```bash
# Start spoon-cli and run:
# Show all configuration
config

# Show specific key
config api_keys.openai

# Show LLM status
llm-status
```

### Modify Configuration

```bash
# Start spoon-cli and run:
# Set API key
config api_key openai "sk-your-key"

# Set configuration value
config default_agent "my_custom_agent"

# Set nested configuration
config llm.timeout 60
```

### Configuration Validation

```bash
# Start spoon-cli and run:
# Validate current configuration
validate-config

# Check for migration needs
check-config

# Migrate legacy configuration
# Start spoon-cli and run:
migrate-config
```

## Configuration Sections

### Agents Configuration

Define custom agents with specific configurations:

```json
{
  "agents": {
    "crypto_trader": {
      "class_name": "SpoonReactAI",
      "description": "Cryptocurrency trading assistant",
      "config": {
        "llm_provider": "anthropic",
        "model_name": "claude-3-sonnet-20240229",
        "temperature": 0.3,
        "max_tokens": 2000
      },
      "tools": [
        "crypto_price_lookup",
        "dex_swap",
        "wallet_balance",
        "market_data"
      ]
    }
  }
}
```

### MCP Servers Configuration

Configure Model Context Protocol servers for extended capabilities:

```json
{
  "mcp_servers": {
    "filesystem": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-filesystem", "/allowed/path"],
      "env": {}
    },
    "git": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-git", "--repository", "/path/to/repo"]
    },
    "github": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-github"],
      "env": {
        "GITHUB_PERSONAL_ACCESS_TOKEN": "ghp_..."
      }
    }
  }
}
```

### LLM Configuration

Configure LLM provider settings and fallback chains:

```json
{
  "llm": {
    "default_provider": "anthropic",
    "fallback_chain": ["anthropic", "openai", "deepseek"],
    "timeout": 30,
    "retry_attempts": 3,
    "providers": {
      "openai": {
        "base_url": null,
        "organization": null
      },
      "anthropic": {
        "max_tokens": 4096
      }
    }
  }
}
```

## Configuration Profiles

Use different configurations for different environments:

```json
{
  "profiles": {
    "development": {
      "default_agent": "debug_agent",
      "llm": {
        "timeout": 60,
        "retry_attempts": 5
      }
    },
    "production": {
      "default_agent": "production_agent",
      "logging": {
        "level": "WARNING"
      }
    }
  }
}
```

Activate a profile:

```bash
export SPOON_CLI_PROFILE="production"

# Then start spoon-cli with the profile active
```

## Configuration File Locations

The CLI searches for configuration files in the following order:

1. **Project-specific**: `./config.json`
2. **User-specific**: `~/.spoon/config.json`
3. **System-wide**: `/etc/spoon/config.json`
4. **XDG Base Directory**: `~/.config/spoon/config.json`

## Security Best Practices

### API Keys Management

1. **Never commit API keys** to version control
2. **Use environment variables** for sensitive data
3. **Rotate keys regularly**
4. **Use different keys** for different environments

### File Permissions

```bash
# Set restrictive permissions on config files
chmod 600 config.json
chmod 600 .env
```

### Environment Separation

Use different configurations for different environments:

```bash
# Development
cp config.json config.dev.json
export SPOON_CLI_CONFIG_FILE="config.dev.json"

# Production
cp config.json config.prod.json
export SPOON_CLI_CONFIG_FILE="config.prod.json"
```

## Troubleshooting Configuration

### Common Issues

1. **Configuration not loading**
   ```bash
# Start spoon-cli and run:
validate-config
# Check file syntax and permissions
   ```

2. **API keys not working**
   ```bash
# Start spoon-cli and run:
llm-status
# Verify key format and validity
   ```

3. **MCP servers not connecting**
   ```bash
# Start spoon-cli and run:
validate-config --check-servers
# Check server commands and environment variables
   ```

### Debug Configuration

Enable debug logging to troubleshoot configuration issues:

```bash
export SPOON_CLI_DEBUG=1
# Start spoon-cli and run:
config
```

## Next Steps

After configuring :
- [Basic Usage](./basic-usage.md) - Learn basic CLI operations
- [Advanced Features](./advanced-features.md) - Explore advanced capabilities
- [Troubleshooting](./troubleshooting.md) - Solve common issues

---

FILE: docs/cli/installation.md

# CLI Installation

The  is the command-line interface for SpoonAI, providing an easy way to interact with AI agents, manage configurations, and perform various operations.

## Installation Methods

### Method 1: Install from PyPI (Recommended)

```bash
pip install spoon-cli
```

This installs the latest stable version of  along with all necessary dependencies.

### Method 2: Install from Source

If you want to install from source or contribute to development:

```bash
# Clone the repository
git clone https://github.com/XSpoonAi/spoon-cli.git
cd spoon-cli/
# Install in development mode
pip install -e .
```

## System Requirements

### Minimum Requirements

- **Python**: 3.12 or higher
- **Operating System**: Linux, macOS, or Windows
- **Memory**: 4GB RAM minimum, 8GB recommended
- **Disk Space**: 500MB for installation

### Recommended Requirements

- **Python**: 3.12+
- **Memory**: 16GB RAM
- **CPU**: Multi-core processor

## Dependencies

The  automatically installs the following core dependencies:

- **spoon-ai-sdk**: Core SpoonAI framework
- **spoon-toolkits**: Additional tool collections
- **prompt_toolkit**: Enhanced command-line interface
- **python-dotenv**: Environment variable management
- **fastmcp**: MCP (Model Context Protocol) support
- **pydantic**: Data validation
- **websockets**: WebSocket communication

## Verification

After installation, verify the installation by checking the version:

```bash
# Check version
spoon-cli --version
```

Or start the interactive CLI:

```bash
# Start spoon-cli
spoon-cli
```

## Post-Installation Setup

### 1. Configure Environment Variables

Create a `.env` file in your project directory or set environment variables:

```bash
# LLM API Keys (choose at least one)
export OPENAI_API_KEY="sk-your-openai-key"
export ANTHROPIC_API_KEY="sk-ant-your-anthropic-key"
export DEEPSEEK_API_KEY="your-deepseek-key"

# Optional: Blockchain operations
export PRIVATE_KEY="your-wallet-private-key"
export RPC_URL="https://mainnet.infura.io/v3/YOUR_PROJECT_ID"
```

### 2. Create Configuration File

Create a `config.json` file for advanced configuration (optional but recommended):

```json
{
  "default_agent": "react",
  "api_keys": {
    "openai": "sk-your-key-here"
  }
}
```

### 3. Test Installation

Run the system health check:

```bash
# Start spoon-cli and run:
system-info
```

This command will show:
- System information
- Environment variable status
- Configuration file status
- Available agents and tools

## Troubleshooting Installation

### Common Issues

1. **Python Version Too Old**

```bash
python --version
# Should show 3.12 or higher
```

2. **Permission Denied**

```bash
# Use user installation
pip install --user
# Or use virtual environment
python -m venv spoon_env
source spoon_env/bin/activate  # Linux/macOS
# spoon_env\Scripts\activate    # Windows
pip install
```

3. **Dependency Conflicts**

```bash
# Upgrade pip
pip install --upgrade pip

# Install in isolated environment
pip install --isolated
```

### Windows-Specific Issues

On Windows, you might need to:

1. **Add Python to PATH** during installation
2. **Use Command Prompt or PowerShell** instead of bash
3. **Install Microsoft Visual C++ Redistributable** if required

## Upgrading

To upgrade to the latest version:

```bash
pip install --upgrade spoon-cli
```

## Uninstalling

To remove spoon-cli:

```bash
pip uninstall spoon-cli
```

Note: This will not remove configuration files or chat histories you may have created.

## Next Steps

After installation, proceed to:
- [Configuration Guide](./configuration.md) - Learn how to configure
- [Basic Usage](./basic-usage.md) - Start using
- [Advanced Features](./advanced-features.md) - Explore advanced capabilities

---

FILE: docs/cli/troubleshooting.md

# CLI Troubleshooting

This guide helps you diagnose and resolve common issues with . Follow the systematic approach to identify and fix problems.

## Quick Diagnosis

### System Health Check

Run the comprehensive health check first:

```bash
# Start  and run:
system-info
```

This command shows:
- Environment variables status
- Configuration file validity
- API key configuration
- Agent and tool availability
- Overall health score

### LLM Provider Status

Check LLM provider configuration:

```bash
# Start  and run:
llm-status
```

This shows:
- Available providers
- API key status (masked)
- Default provider
- Fallback chain configuration

## Configuration Issues

### Configuration File Problems

#### Invalid JSON Syntax

**Symptoms:**
- Configuration loading errors
- CLI fails to start

**Diagnosis:**
```bash
# Start spoon-cli and run:
validate-config
```

**Solutions:**
1. Check JSON syntax with online validator
2. Ensure proper quoting of strings
3. Validate file encoding (should be UTF-8)

#### Configuration Not Loading

**Symptoms:**
- Settings not applied
- Default values used instead

**Diagnosis:**
```bash
# Check if config file exists and is readable
ls -la config.json

# Start spoon-cli and run:
check-config
```

**Solutions:**
1. Ensure `config.json` is in current directory or `~/.spoon/config.json`
2. Check file permissions: `chmod 644 config.json`
3. Validate JSON format

### Environment Variable Issues

#### API Keys Not Recognized

**Symptoms:**
- LLM provider unavailable
- Authentication errors

**Diagnosis:**
```bash
# Check environment variables
env | grep -E "(OPENAI|ANTHROPIC|DEEPSEEK)"

# Test specific provider
 llm-status
```

**Solutions:**
1. Set environment variables correctly:
   ```bash
   export OPENAI_API_KEY="sk-your-key-here"
   export ANTHROPIC_API_KEY="sk-ant-your-key-here"
   ```

2. Use `.env` file for persistent configuration:
   ```bash
   echo "OPENAI_API_KEY=sk-your-key-here" > .env
   ```

3. Restart shell or reload environment

### Migration Issues

#### Legacy Configuration Detected

**Symptoms:**
- Migration warnings
- Some features not working

**Diagnosis:**
```bash
# Start spoon-cli and run:
check-config
```

**Solutions:**
```bash
# Preview migration
# Start spoon-cli and run:
migrate-config --dry-run

# Perform migration
# Start spoon-cli and run:
migrate-config

# Validate after migration
# Start spoon-cli and run:
validate-config
```

## Agent Loading Issues

### Agent Not Found

**Symptoms:**
- "Agent not found" error
- Cannot load specific agent

**Diagnosis:**
```bash
# Start spoon-cli and run:
# List available agents
list-agents

# Check configuration
config agents
```

**Solutions:**
1. Verify agent name in configuration
2. Check agent class name is correct
3. Ensure required dependencies are installed

### Tool Loading Failures

**Symptoms:**
- Tools not available after loading
- Agent lacks expected capabilities

**Diagnosis:**
```bash
# Start spoon-cli and run:
# Check tool loading status
tool-status

# List available toolkits
list-toolkit-categories
```

**Solutions:**
1. Reload agent configuration:
   ```bash
# Start spoon-cli and run:
reload-config
   ```

2. Reinstall toolkits:
   ```bash
   pip install --upgrade spoon-toolkits
   ```

3. Check network connectivity for external tools

## Network and Connectivity Issues

### MCP Server Connection Problems

**Symptoms:**
- MCP tools unavailable
- Server connection timeouts

**Diagnosis:**
```bash
# Start spoon-cli and run:
# Validate MCP configuration
validate-config --check-servers

# Test specific server
# Start spoon-cli and run:
load-agent mcp_agent
```

**Solutions:**
1. Verify server commands are installed:
   ```bash
   which npx
   which uvx
   ```

2. Check server environment variables
3. Update server configurations in `config.json`

### API Rate Limiting

**Symptoms:**
- Requests failing with rate limit errors
- Intermittent connectivity issues

**Solutions:**
1. Implement request throttling:
   ```json
   {
     "llm": {
       "rate_limiting": {
         "requests_per_minute": 30,
         "burst_limit": 5
       }
     }
   }
   ```

2. Switch to different provider temporarily
3. Upgrade API plan for higher limits

## Performance Issues

### Slow Startup

**Symptoms:**
- CLI takes long time to start
- Agent loading is slow

**Diagnosis:**
```bash
# Enable profiling
export SPOON_CLI_PROFILE=1
 system-info
```

**Solutions:**
1. Load fewer tools initially
2. Use faster LLM providers
3. Enable caching:
   ```json
   {
     "llm": {
       "caching": {
         "enabled": true,
         "ttl": 3600
       }
     }
   }
   ```

### High Memory Usage

**Symptoms:**
- System running out of memory
- Performance degradation

**Solutions:**
1. Reduce chat history size:
   ```json
   {
     "memory": {
       "max_chat_history": 50,
       "compress_old_messages": true
     }
   }
   ```

2. Enable memory cleanup:
   ```json
   {
     "memory": {
       "cleanup_temp_files": true,
       "auto_save_interval": 300
     }
   }
   ```

### Slow Response Times

**Diagnosis:**
```bash
# Start spoon-cli and run:
# Check LLM provider status
llm-status

# Test response time
time spoon-cli action chat <<< "Hello"
```

**Solutions:**
1. Switch to faster models
2. Adjust timeout settings:
   ```json
   {
     "llm": {
       "timeout": 60,
       "retry_attempts": 2
     }
   }
   ```

3. Enable response streaming for better perceived performance

## Installation Issues

### Import Errors

**Symptoms:**
- "Module not found" errors
- CLI fails to start

**Diagnosis:**
```bash
# Check Python path
python -c "import spoon_ai; print(spoon_ai.__file__)"

# Verify installation
pip list | grep spoon
```

**Solutions:**
1. Reinstall packages:
   ```bash
   pip uninstall  spoon-ai-sdk spoon-toolkits
   pip install    ```

2. Check Python version compatibility
3. Use virtual environment to avoid conflicts

### Permission Issues

**Symptoms:**
- Cannot write configuration files
- Installation fails

**Solutions:**
1. Use user installation:
   ```bash
   pip install --user    ```

2. Fix directory permissions:
   ```bash
   chmod 755 ~/.spoon/
   chmod 644 ~/.spoon/config.json
   ```

## Platform-Specific Issues

### Windows Issues

#### Command Prompt Problems

**Symptoms:**
- Interactive features not working
- Display issues

**Solutions:**
1. Use PowerShell instead of CMD
2. Install Windows Terminal
3. Enable ANSI color support

#### Path Issues

**Solutions:**
1. Add Python to PATH during installation
2. Use absolute paths in configuration
3. Avoid spaces in installation paths

### macOS Issues

#### SIP (System Integrity Protection)

**Solutions:**
1. Install in user directory:
   ```bash
   pip install --user    ```

2. Use Homebrew for Python:
   ```bash
   brew install python
   ```

### Linux Issues

#### Shared Library Problems

**Diagnosis:**
```bash
ldd $(which python) | grep "not found"
```

**Solutions:**
1. Install missing system libraries
2. Use system package manager:
   ```bash
   # Ubuntu/Debian
   sudo apt-get install python3-dev build-essential

   # CentOS/RHEL
   sudo yum install python3-devel gcc
   ```

## Debugging Techniques

### Enable Debug Mode

```bash
# Enable debug logging
export SPOON_CLI_DEBUG=1

# Or use flag
 --debug system-info
```

### Log Analysis

```bash
# Check log files
tail -f spoon_cli.log

# Increase log verbosity
export SPOON_CLI_LOG_LEVEL=DEBUG
```

### Network Debugging

```bash
# Test network connectivity
curl -I https://api.openai.com/v1/models

# Check proxy settings
env | grep -i proxy
```

### Configuration Debugging

```bash
# Show full configuration with debug
 --debug config

# Validate with detailed output
 validate-config --verbose
```

## Common Error Messages

### "Configuration validation failed"

**Cause:** Invalid configuration syntax or missing required fields

**Solution:**
```bash
 validate-config
# Fix reported issues
 reload-config
```

### "Agent class not found"

**Cause:** Invalid agent class name in configuration

**Solution:**
- Check class name spelling
- Ensure correct module imports
- Update to valid class names

### "Tool loading failed"

**Cause:** Tool dependencies not available

**Solution:**
```bash
pip install --upgrade spoon-toolkits
 reload-config
```

### "MCP server connection timeout"

**Cause:** Network issues or server configuration problems

**Solution:**
```bash
 validate-config --check-servers
# Fix server configurations
 reload-config
```

## Getting Help

### Community Resources

1. **GitHub Issues:** Report bugs and request features
2. **Documentation:** Check the full documentation
3. **Discord/Forum:** Community discussions

### Debug Information Collection

When reporting issues, include:

```bash
# System information
 system-info

# Configuration (mask sensitive data)
 config

# Error logs
tail -n 50 spoon_cli.log

# Python environment
python --version
pip list | grep spoon
```

## Preventive Maintenance

### Regular Updates

```bash
# Update CLI and dependencies
pip install --upgrade  spoon-ai-sdk spoon-toolkits

# Update configuration if needed
 migrate-config
```

### Configuration Backup

```bash
# Backup configuration
cp config.json config.json.backup

# Backup environment
env | grep SPOON > spoon_env.backup
```

### Health Monitoring

```bash
# Regular health checks
 system-info

# Monitor for issues
 validate-config
```

## Emergency Recovery

### Reset Configuration

```bash
# Remove corrupted config
rm config.json

# Start with minimal config
echo '{"default_agent": "react"}' > config.json

# Test basic functionality
 list-agents
```

### Clean Reinstall

```bash
# Complete cleanup
pip uninstall  spoon-ai-sdk spoon-toolkits
rm -rf ~/.spoon/
rm -rf chat_logs/

# Fresh install
pip install
# Basic setup
 config api_key openai "your-key"
```

## Next Steps

- [Installation](./installation.md) - Reinstall if needed
- [Configuration](./configuration.md) - Review configuration options
- [Basic Usage](./basic-usage.md) - Verify basic functionality works

---

FILE: docs/core-concepts/agents-detailed.md

---
sidebar_position: 5
---

# Agents (Detailed Guide)

This guide provides a comprehensive walkthrough for developing and configuring agents in SpoonOS. Agents combine language models with tools to create intelligent, autonomous systems that reason about problems, plan action sequences, execute tools, and adapt based on feedback.

## Agent Architecture

SpoonOS agents follow the **ReAct (Reasoning + Acting)** pattern:

```text
Thought â†’ Action â†’ Observation â†’ Thought â†’ Action â†’ ...
```

### Core Components

#### Built-in Agent Classes

SpoonOS provides two main agent classes:

- **`SpoonReactAI`**: Standard agent for blockchain operations
- **`SpoonReactMCP`**: Enhanced agent with MCP protocol support

#### Tool Integration

Agents can use various types of tools:

- **Built-in Tools**: Python classes integrated directly into the framework
- **MCP Tools**: External tools accessed via Model Context Protocol
- **Custom Tools**: User-defined tools for specific functionality

### Technical Implementation

#### Creating Custom Agents

```python
from spoon_ai.agents import SpoonReactMCP
from spoon_ai.tools import ToolManager

class SpoonMacroAnalysisAgent(SpoonReactMCP):
    name: str = "SpoonMacroAnalysisAgent"
    system_prompt: str = (
        '''You are a cryptocurrency market analyst. Your task is to provide a comprehensive
        macroeconomic analysis of a given token.

        To do this, you will perform the following steps:
        1. Use the `crypto_power_data_cex` tool to get the latest candlestick data and
           technical indicators.
        2. Use the `tavily-search` tool to find the latest news and market sentiment.
        3. Synthesize the data from both tools to form a holistic analysis.'''
    )

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.available_tools = ToolManager([])
```

#### MCP Tool Configuration

**Stdio-based Transport:**
```python
from spoon_ai.tools.mcp_tool import MCPTool

tavily_tool = MCPTool(
    name="tavily-search",
    description="Performs a web search using the Tavily API.",
    mcp_config={
        "command": "npx",
        "args": ["--yes", "tavily-mcp"],
        "env": {"TAVILY_API_KEY": os.getenv("TAVILY_API_KEY")}
    }
)
```

**HTTP Transport:**
```python
firecrawl_tool = MCPTool(
    name="firecrawl-scraper",
    description="Advanced web scraping tool",
    mcp_config={
        "url": "https://mcp.firecrawl.dev/scrape",
        "headers": {
            "Authorization": f"Bearer {os.getenv('FIRECRAWL_API_KEY')}",
            "Content-Type": "application/json"
        }
    }
)
```

1. **LLM (Language Model)** - The "brain" that provides reasoning capabilities
2. **Tools** - External capabilities the agent can use
3. **Memory** - Context and conversation history
4. **System Prompt** - Instructions that define the agent's behavior

#### Built-in Tools Integration

```python
from spoon_toolkits.crypto.crypto_powerdata.tools import CryptoPowerDataCEXTool
from spoon_toolkits.crypto.crypto_data_tools.price_data import GetTokenPriceTool

# Initialize built-in tools
crypto_tool = CryptoPowerDataCEXTool(
    exchange="binance",
    symbol="BTC/USDT",
    timeframe="1h",
    limit=100
)

price_tool = GetTokenPriceTool(
    exchange="uniswap"
)

# Add tools to agent
agent.available_tools.add_tools(crypto_tool, price_tool)
```
5. **Execution Loop** - The ReAct cycle that drives agent behavior

## Agent Types

### 1. SpoonReactAI

The standard ReAct agent for tool-enabled interactions:

```python
from spoon_ai.agents import SpoonReactAI
from spoon_ai.chat import ChatBot
from spoon_ai.tools import ToolManager

class MyAgent(SpoonReactAI):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

        # Configure LLM
        self.llm = ChatBot(model_name="gpt-5.1-chat-latest")

        # Set system prompt
        self.system_prompt = "You are a helpful AI assistant."

        # Configure execution parameters
        self.max_steps = 10

        # Set up tools (if any)
        self.available_tools = ToolManager([])
```

### 2. SpoonReactMCP

MCP-enabled agent with dynamic tool loading:

```python
from spoon_ai.agents import SpoonReactMCP
from spoon_ai.chat import ChatBot

# Agent with MCP tool support
agent = SpoonReactMCP(
    llm=ChatBot(),
    system_prompt="You are an expert research assistant.",
    max_steps=15
)

# When using the CLI (main.py / spoon-cli), agents and MCP tools are defined in the CLI's `config.json` file.
# When using the SDK directly (as in this example), configure tools and agents in Python code instead of reading config.json.
```

## Creating Custom Agents

### Basic Custom Agent

```python
from spoon_ai.agents import SpoonReactAI
from spoon_ai.tools.base import BaseTool
from spoon_ai.tools import ToolManager
from spoon_ai.chat import ChatBot

class WeatherTool(BaseTool):
    name: str = "get_weather"
    description: str = "Get current weather for a location"
    parameters: dict = {
        "type": "object",
        "properties": {
            "location": {"type": "string", "description": "City name"}
        },
        "required": ["location"]
    }

    async def execute(self, location: str) -> str:
        # Implement weather API call
        return f"Weather in {location}: Sunny, 72Â°F"

class WeatherAgent(SpoonReactAI):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

        # Configure LLM
        self.llm = ChatBot()

        # Set system prompt
        self.system_prompt = """
        You are a weather assistant. Use the get_weather tool to provide
        accurate weather information for any location the user asks about.
        """

        # Set up tools
        weather_tool = WeatherTool()
        self.available_tools = ToolManager([weather_tool])
```

### Advanced Agent with Multiple Tools

```python
from spoon_ai.agents import SpoonReactAI
from spoon_ai.tools import ToolManager
from spoon_ai.tools.mcp_tool import MCPTool
from spoon_ai.chat import ChatBot
import os

class ResearchAgent(SpoonReactAI):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

        # Configure LLM
        self.llm = ChatBot(model_name="gpt-5.1-chat-latest")

        # Set up MCP tools
        tools = []

        # Web search tool (requires TAVILY_API_KEY)
        if os.getenv("TAVILY_API_KEY"):
            search_tool = MCPTool(
                name="web_search",
                description="Search the web for current information",
                mcp_config={
                    "command": "npx",
                    "args": ["-y", "tavily-mcp"],
                    "env": {"TAVILY_API_KEY": os.getenv("TAVILY_API_KEY")},
                    "transport": "stdio"
                }
            )
            tools.append(search_tool)

        self.available_tools = ToolManager(tools)
        self.system_prompt = """
        You are a research assistant with access to web search tools.

        When asked to research a topic:
        1. Search for current information using available tools
        2. Analyze and synthesize findings
        3. Provide well-structured summaries
        4. Always cite your sources

        Be thorough and accurate in your research.
        """
```

## Agent Configuration

### System Prompts

System prompts define your agent's personality and behavior:

```python
# Task-specific prompt
system_prompt = """
You are a crypto trading assistant. Your role is to:
- Analyze market data and trends
- Provide trading insights and recommendations
- Help users understand market conditions
- Always include risk warnings in trading advice
"""

# Personality-driven prompt
system_prompt = """
You are a friendly and enthusiastic coding mentor. You:
- Explain concepts clearly with examples
- Encourage learning and experimentation
- Provide constructive feedback
- Use emojis and positive language
"""
```

### Agent Parameters

```python
from spoon_ai.agents import SpoonReactAI
from spoon_ai.chat import ChatBot

class ConfigurableAgent(SpoonReactAI):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

        # Configure LLM with specific parameters
        self.llm = ChatBot(
            model_name="gpt-5.1-chat-latest",
            temperature=0.7,        # Creativity level
            max_tokens=4096,        # Response length limit
            timeout=60              # Request timeout
        )

        # Execution settings
        self.max_steps = 20         # Maximum reasoning steps
        self.verbose = True         # Show reasoning steps

        # Agent behavior settings
        self.system_prompt = "You are a configurable AI assistant."
```

## Agent Lifecycle

### 1. Initialization

```python
from spoon_ai.agents import SpoonReactAI
from spoon_ai.chat import ChatBot

# Create agent instance
agent = SpoonReactAI(
    llm=ChatBot(model_name="gpt-5.1-chat-latest"),
    system_prompt="You are a helpful assistant.",
    max_steps=15
)
```

### 2. Execution

```python
# Single interaction
result = await agent.run("What's the weather in New York?")
print(result)

# Multiple interactions
questions = [
    "What is SpoonOS?",
    "How do I create an agent?",
    "What tools are available?"
]

for question in questions:
    response = await agent.run(question)
    print(f"Q: {question}")
    print(f"A: {response}
")
```

### 3. Configuration Management

```python
# Access agent configuration
print(f"Max steps: {agent.max_steps}")
print(f"System prompt: {agent.system_prompt}")

# Modify configuration
agent.max_steps = 20
agent.system_prompt = "You are an expert assistant."

# Check available tools
if hasattr(agent, 'available_tools'):
    tools = agent.available_tools.list_tools()
    print(f"Available tools: {tools}")
```

## Best Practices

### 1. Clear System Prompts

```python
# Good: Specific and actionable
system_prompt = """
You are a code review assistant. For each code submission:
1. Check for syntax errors and bugs
2. Suggest improvements for readability
3. Identify security vulnerabilities
4. Recommend best practices
5. Provide specific, actionable feedback
"""

# Avoid: Vague or generic
system_prompt = "You are a helpful assistant."
```

### 2. Appropriate Tool Selection

```python
# Match tools to agent purpose
class DataAnalysisAgent(ToolCallAgent):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

        # Data-focused tools
        tools = [
            PandasTool(),      # Data manipulation
            PlotlyTool(),      # Visualization
            StatsTool(),       # Statistical analysis
            DatabaseTool(),    # Data access
        ]

        self.available_tools = ToolManager(tools)
```

### 3. Error Handling

```python
class RobustAgent(ToolCallAgent):
    async def run(self, user_input: str) -> str:
        try:
            return await super().run(user_input)
        except Exception as e:
            # Log error and provide graceful fallback
            self.logger.error(f"Agent execution failed: {e}")
            return "I encountered an error. Please try rephrasing your request."
```

### 4. Performance Optimization

```python
# Use appropriate model for task complexity
simple_agent = ToolCallAgent(
    llm=ChatBot(model_name="gpt-3.5-turbo")  # Faster, cheaper
)

complex_agent = ToolCallAgent(
    llm=ChatBot(model_name="gpt-5.1-chat-latest")        # More capable
)

# Set reasonable limits
agent.max_steps = 10        # Prevent infinite loops
agent.timeout = 60.0        # Prevent hanging
```

## Testing Agents

### Unit Testing

```python
import pytest
from unittest.mock import AsyncMock

@pytest.mark.asyncio
async def test_weather_agent():
    # Mock the LLM
    mock_llm = AsyncMock()
    mock_llm.ask.return_value = "The weather in Paris is sunny."

    # Create agent with mock
    agent = WeatherAgent(llm=mock_llm)

    # Test execution
    result = await agent.run("What's the weather in Paris?")
    assert "sunny" in result.lower()
```

### Integration Testing

```python
@pytest.mark.asyncio
async def test_agent_with_real_llm():
    # Test with actual LLM (requires API key)
    agent = WeatherAgent(llm=ChatBot())

    result = await agent.run("What's the weather in London?")
    assert len(result) > 0
    assert "weather" in result.lower()
```

## Monitoring and Debugging

### Logging

```python
import logging

# Enable agent logging
logging.basicConfig(level=logging.DEBUG)

# Agent will log reasoning steps
agent = MyAgent(llm=ChatBot(), verbose=True)
```

### Performance Metrics

```python
import time

start_time = time.time()
result = await agent.run("Complex query here")
execution_time = time.time() - start_time

print(f"Agent executed in {execution_time:.2f} seconds")
print(f"Used {agent.step_count} reasoning steps")
```

## Advanced Features

### MCP Tool Integration

#### Single MCP Tool Integration

```python
from spoon_ai.agents import SpoonReactMCP
from spoon_ai.tools.mcp_tool import MCPTool
from spoon_ai.tools import ToolManager
from spoon_ai.chat import ChatBot
import os

class MCPEnabledAgent(SpoonReactMCP):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

        # Configure LLM
        self.llm = ChatBot(model_name="gpt-5.1-chat-latest")

        # Configure stdio MCP tool
        search_tool = MCPTool(
            name="tavily_search",
            description="Advanced web search capabilities",
            mcp_config={
                "command": "npx",
                "args": ["-y", "tavily-mcp"],
                "env": {"TAVILY_API_KEY": os.getenv("TAVILY_API_KEY")},
                "timeout": 30,
                "retry_attempts": 3
            }
        )

        # Configure HTTP MCP tool
        context7_tool = MCPTool(
            name="context7_docs",
            description="Access Context7 documentation and libraries",
            mcp_config={
                "url": "https://mcp.context7.com/mcp",
                "timeout": 30,
                "retry_attempts": 2,
                "headers": {
                    "User-Agent": "SpoonOS-Agent/1.0"
                }
            }
        )

        # Create tool manager
        self.available_tools = ToolManager([search_tool, context7_tool])

        self.system_prompt = """
        You are a research assistant with access to multiple MCP tools:
        - tavily_search: Web search functionality
        - context7_docs: Access Context7 documentation and library information

        When using tools:
        1. Choose the appropriate tool for the task
        2. Clearly define search objectives and keywords
        3. Analyze the reliability of search results
        4. Provide comprehensive analysis reports
        """

# Usage example
async def use_single_mcp():
    agent = MCPEnabledAgent()
    result = await agent.run("Search for the latest developments in SpoonOS framework and review related documentation")
    return result
```

#### Comprehensive MCP Tool Integration

```python
from spoon_ai.agents import SpoonReactMCP
from spoon_ai.tools.mcp_tool import MCPTool
from spoon_ai.tools import ToolManager

class ComprehensiveMCPAgent(SpoonReactMCP):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

        # Configure multiple MCP tools
        mcp_tools = []

        # Web search tool
        if os.getenv("TAVILY_API_KEY"):
            mcp_tools.append(MCPTool(
                name="web_search",
                description="Web search and information gathering",
                mcp_config={
                    "command": "npx",
                    "args": ["-y", "tavily-mcp"],
                    "env": {"TAVILY_API_KEY": os.getenv("TAVILY_API_KEY")}
                }
            ))

        # GitHub tools
        if os.getenv("GITHUB_TOKEN"):
            mcp_tools.append(MCPTool(
                name="github_management",
                description="GitHub repository and code management",
                mcp_config={
                    "command": "npx",
                    "args": ["-y", "@modelcontextprotocol/server-github"],
                    "env": {"GITHUB_TOKEN": os.getenv("GITHUB_TOKEN")},
                    "transport": "stdio"
                }
            ))

        # Filesystem tools
        mcp_tools.append(MCPTool(
            name="file_operations",
            description="File and directory operations",
            mcp_config={
                "command": "npx",
                "args": ["-y", "@modelcontextprotocol/server-filesystem"],
                "transport": "stdio"
            }
        ))

        # Database tools
        mcp_tools.append(MCPTool(
            name="database_operations",
            description="SQLite database operations",
            mcp_config={
                "command": "npx",
                "args": ["-y", "@modelcontextprotocol/server-sqlite"],
                "env": {"DATABASE_PATH": "./agent_data.db"},
                "transport": "stdio"
            }
        ))

        # HTTP MCP tool - Context7 documentation service
        mcp_tools.append(MCPTool(
            name="context7_docs",
            description="Context7 documentation and library information access",
            mcp_config={
                "url": "https://mcp.context7.com/mcp",
                "transport": "http",
                "timeout": 30,
                "retry_attempts": 2,
                "headers": {
                    "User-Agent": "SpoonOS-ComprehensiveAgent/1.0"
                }
            }
        ))

        # SSE MCP tool - Firecrawl web scraping service
        if os.getenv("FIRECRAWL_API_KEY"):
            mcp_tools.append(MCPTool(
                name="firecrawl_scraper",
                description="Advanced web scraping and content extraction service",
                mcp_config={
                    "url": f"https://mcp.firecrawl.dev/{os.getenv('FIRECRAWL_API_KEY')}/sse",
                    "transport": "sse",
                    "timeout": 60,
                    "retry_attempts": 3,
                    "reconnect_interval": 5,
                    "max_reconnect_attempts": 10,
                    "headers": {
                        "Accept": "text/event-stream",
                        "User-Agent": "SpoonOS-ComprehensiveAgent/1.0",
                        "Cache-Control": "no-cache"
                    }
                }
            ))

        # Create tool manager
        self.available_tools = ToolManager(mcp_tools)

        self.system_prompt = """
        You are a comprehensive AI assistant with the following MCP tools:

        1. web_search: Search for latest information and resources
        2. github_management: Manage GitHub repositories and code
        3. file_operations: Handle files and directories
        4. database_operations: Operate SQLite databases
        5. context7_docs: Access Context7 documentation and library information
        6. firecrawl_scraper: Advanced web scraping and content extraction service

        Workflow:
        1. Analyze user requirements and determine which tools to use
        2. Use multiple tools in logical sequence to complete tasks
        3. Integrate results from all tools to provide comprehensive solutions

        Always ensure operation safety and accuracy.
        """

# Usage example
async def use_comprehensive_mcp():
    agent = ComprehensiveMCPAgent(llm=ChatBot())

    task = """
    Please help me complete a project analysis task:
    1. Search for the latest trends in 'AI agent frameworks'
    2. Review Context7 related documentation and library information
    3. Create project directory structure locally
    4. Create database to store analysis results
    5. If possible, review related GitHub projects
    """

    result = await agent.run(task)
    return result
```

### Advanced Parameter Configuration

#### Detailed Agent Parameter Configuration

```python
class AdvancedConfigAgent(SpoonReactMCP):
    def __init__(self, **kwargs):
        # Advanced LLM configuration
        llm_config = {
            "model_name": "gpt-5.1-chat-latest",
            "temperature": 0.3,          # Creativity control
            "max_tokens": 4096,          # Maximum response length
            "top_p": 0.9,               # Nucleus sampling parameter
            "frequency_penalty": 0.1,    # Frequency penalty
            "presence_penalty": 0.1,     # Presence penalty
            "timeout": 60,              # Request timeout
            "retry_attempts": 3,         # Retry attempts
            "stream": False             # Whether to stream responses
        }

        super().__init__(
            llm=ChatBot(**llm_config),
            **kwargs
        )

        # Agent execution parameters
        self.max_iterations = 20        # Maximum reasoning steps
        self.max_execution_time = 300   # Maximum execution time (seconds)
        self.enable_memory = True       # Enable conversation memory
        self.memory_window = 10         # Memory window size
        self.enable_reflection = True   # Enable reflection mechanism
        self.reflection_threshold = 5   # Reflection trigger threshold

        # Tool execution parameters
        self.tool_timeout = 30          # Tool execution timeout
        self.tool_retry_attempts = 2    # Tool retry attempts
        self.parallel_tool_execution = False  # Parallel tool execution
        self.tool_error_handling = "graceful"  # Tool error handling strategy

        # Security and limitation parameters
        self.enable_safety_checks = True      # Enable security checks
        self.max_tool_calls_per_iteration = 3 # Maximum tool calls per iteration
        self.restricted_operations = [        # Restricted operations
            "file_delete",
            "system_command",
            "network_access"
        ]

        self.system_prompt = """
        You are an advanced AI agent with the following configuration:

        Execution parameters:
        - Maximum reasoning steps: {max_iterations}
        - Memory enabled: {enable_memory}
        - Reflection enabled: {enable_reflection}

        Security settings:
        - Security checks: Enabled
        - Operation restrictions: Configured
        - Error handling: Graceful mode

        Please complete tasks efficiently within these parameter constraints.
        """.format(
            max_iterations=self.max_iterations,
            enable_memory=self.enable_memory,
            enable_reflection=self.enable_reflection
        )

# Dynamic parameter adjustment
class DynamicConfigAgent(SpoonReactMCP):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.performance_metrics = {
            "success_rate": 0.0,
            "avg_execution_time": 0.0,
            "tool_usage_efficiency": 0.0
        }

    async def run(self, user_input: str) -> str:
        """Execution with dynamic parameter adjustment"""
        # Adjust parameters based on historical performance
        self._adjust_parameters()

        # Execute task
        start_time = time.time()
        try:
            result = await super().run(user_input)
            success = True
        except Exception as e:
            result = f"Execution failed: {str(e)}"
            success = False

        # Update performance metrics
        execution_time = time.time() - start_time
        self._update_metrics(success, execution_time)

        return result

    def _adjust_parameters(self):
        """Adjust parameters based on performance metrics"""
        if self.performance_metrics["success_rate"] < 0.8:
            # Low success rate, increase retry attempts and timeout
            self.max_iterations = min(self.max_iterations + 2, 25)
            self.tool_timeout = min(self.tool_timeout + 10, 60)

        if self.performance_metrics["avg_execution_time"] > 120:
            # Execution time too long, reduce maximum steps
            self.max_iterations = max(self.max_iterations - 1, 10)

        if self.performance_metrics["tool_usage_efficiency"] < 0.6:
            # Low tool usage efficiency, enable parallel execution
            self.parallel_tool_execution = True

    def _update_metrics(self, success: bool, execution_time: float):
        """Update performance metrics"""
        # Simplified metric update logic
        alpha = 0.1  # Learning rate

        self.performance_metrics["success_rate"] = (
            (1 - alpha) * self.performance_metrics["success_rate"] +
            alpha * (1.0 if success else 0.0)
        )

        self.performance_metrics["avg_execution_time"] = (
            (1 - alpha) * self.performance_metrics["avg_execution_time"] +
            alpha * execution_time
        )
```

### Advanced Configuration File Settings

```json
{
  "agents": {
    "advanced_mcp_agent": {
      "class": "SpoonReactMCP",
      "description": "Advanced MCP agent configuration example",
      "config": {
        "max_steps": 20,
        "temperature": 0.3,
        "max_execution_time": 300,
        "enable_memory": true,
        "memory_window": 10,
        "enable_reflection": true,
        "reflection_threshold": 5,
        "safety_checks": true,
        "parallel_tool_execution": false,
        "tool_timeout": 30,
        "tool_retry_attempts": 2,
        "max_tool_calls_per_iteration": 3,
        "system_prompt": "You are an advanced AI agent..."
      },
      "llm_config": {
        "model_name": "gpt-5.1-chat-latest",
        "temperature": 0.3,
        "max_tokens": 4096,
        "top_p": 0.9,
        "frequency_penalty": 0.1,
        "presence_penalty": 0.1,
        "timeout": 60,
        "retry_attempts": 3
      },
      "tools": [
        {
          "name": "tavily_search",
          "type": "mcp",
          "enabled": true,
          "priority": "high",
          "config": {
            "timeout": 30,
            "retry_attempts": 3,
            "cache_duration": 300,
            "rate_limit": 100
          },
          "mcp_server": {
            "command": "npx",
            "args": ["-y", "tavily-mcp"],
            "env": {"TAVILY_API_KEY": "your-tavily-key"},
            "transport": "stdio",
            "health_check_interval": 60,
            "auto_restart": true
          }
        },
        {
          "name": "github_tools",
          "type": "mcp",
          "enabled": true,
          "priority": "medium",
          "config": {
            "timeout": 45,
            "retry_attempts": 2,
            "default_branch": "main",
            "max_file_size": "10MB"
          },
          "mcp_server": {
            "command": "npx",
            "args": ["-y", "@modelcontextprotocol/server-github"],
            "env": {"GITHUB_TOKEN": "your-github-token"},
            "transport": "stdio"
          }
        },
        {
          "name": "context7_docs",
          "type": "mcp",
          "enabled": true,
          "priority": "medium",
          "config": {
            "timeout": 30,
            "retry_attempts": 2,
            "rate_limit": 50,
            "cache_duration": 600
          },
          "mcp_server": {
            "url": "https://mcp.context7.com/mcp",
            "transport": "http",
            "headers": {
              "User-Agent": "SpoonOS-Agent/1.0"
            }
          }
        },
        {
          "name": "firecrawl_scraper",
          "type": "mcp",
          "enabled": true,
          "priority": "high",
          "config": {
            "timeout": 60,
            "retry_attempts": 3,
            "reconnect_interval": 5,
            "max_reconnect_attempts": 10,
            "rate_limit": 30,
            "cache_duration": 300
          },
          "mcp_server": {
            "url": "https://mcp.firecrawl.dev/{FIRECRAWL_API_KEY}/sse",
            "transport": "sse",
            "headers": {
              "Accept": "text/event-stream",
              "User-Agent": "SpoonOS-Agent/1.0",
              "Cache-Control": "no-cache"
            }
          }
        },
        {
          "name": "crypto_powerdata_cex",
          "type": "builtin",
          "enabled": true,
          "priority": "high",
          "config": {
            "timeout": 30,
            "max_retries": 3,
            "cache_duration": 300,
            "rate_limit": 200,
            "default_currency": "USD",
            "precision": 8
          },
          "env": {
            "OKX_API_KEY": "your_okx_api_key",
            "OKX_SECRET_KEY": "your_okx_secret_key",
            "OKX_API_PASSPHRASE": "your_okx_passphrase"
          }
        }
      ],
      "monitoring": {
        "enable_metrics": true,
        "log_level": "INFO",
        "performance_tracking": true,
        "error_reporting": true,
        "health_checks": true
      },
      "security": {
        "restricted_operations": [
          "file_delete",
          "system_command",
          "network_access"
        ],
        "api_rate_limits": {
          "requests_per_minute": 100,
          "tokens_per_hour": 50000
        },
        "data_privacy": {
          "log_user_inputs": false,
          "encrypt_sensitive_data": true,
          "data_retention_days": 30
        }
      }
    }
  }
}
```

## Next Steps

### ðŸ“š **Agent Implementation Examples**

#### ðŸŽ¯ [Intent Graph Demo](../examples/intent-graph-demo.md)
**GitHub**: [View Source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/intent_graph_demo.py)

**Advanced agent patterns demonstrated:**
- Long-lived agent architecture with persistent memory
- Intelligent routing and decision-making workflows
- Advanced state management and context preservation
- Production-ready error handling and monitoring

#### ðŸ” [MCP Spoon Search Agent](../examples/mcp-spoon-search-agent.md)
**GitHub**: [View Source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/mcp/spoon_search_agent.py)

**MCP integration patterns:**
- Dynamic tool discovery and loading
- Multi-tool orchestration and coordination
- Real-time web search integration
- Advanced error handling for distributed systems

#### ðŸ“Š [Graph Crypto Analysis](../examples/graph-crypto-analysis.md)
**GitHub**: [View Source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/graph_crypto_analysis.py)

**Domain-specific agent development:**
- Cryptocurrency market analysis workflows
- Real-time data processing and technical indicators
- LLM-driven investment recommendations
- Financial data validation and error handling

### ðŸ› ï¸ **Development Resources**

- **[Tools System](./tools.md)** - Complete tool integration guide
- **[Custom Tools](../how-to-guides/add-custom-tools.md)** - Build specialized tools
- **[MCP Protocol](./mcp-protocol.md)** - Dynamic tool loading and execution
- **[Graph System](./graph-system.md)** - Advanced workflow orchestration

### ðŸ“– **Additional Documentation**

- **[API Reference](../api-reference/spoon_ai/agents/base/)** - Complete agent API documentation
- **[Performance Optimization](../troubleshooting/performance.md)** - Agent performance tuning
- **[Troubleshooting](../troubleshooting/common-issues.md)** - Common issues and solutions

Ready to build more sophisticated agents? Check out the [Tools](./tools.md) documentation! ðŸ› ï¸

---

FILE: docs/core-concepts/agents.md

# Agents

Agents are the core intelligence layer of SpoonOSâ€”**autonomous AI systems that reason, plan, and act** to accomplish complex tasks. An agent combines an LLM's reasoning capabilities with tools, memory, and structured execution patterns to go beyond simple question-answering.

## Why Agents?

A raw LLM can answer questions, but it can't:

- **Take actions** â€” Call APIs, query databases, execute transactions
- **Remember context** â€” Maintain state across conversations and sessions
- **Reason iteratively** â€” Break down complex tasks into steps, observe results, and adapt
- **Handle failures** â€” Retry, fallback, or ask for help when something goes wrong

SpoonOS agents solve these problems with two execution models:

```mermaid
graph LR
    subgraph ReAct["ReAct Agent"]
        A[Think] --> B[Act]
        B --> C[Observe]
        C --> A
    end

    subgraph Graph["Graph Agent"]
        D[Node A] --> E{Route}
        E --> F[Node B]
        E --> G[Node C]
        F --> H[Node D]
        G --> H
    end
```

| Model | Best For | How It Works |
|-------|----------|--------------|
| **ReAct** | Simple tasks, single-step tool calls, Q&A | Think â†’ Act â†’ Observe loop until done |
| **Graph** | Complex workflows, parallel tasks, conditional logic | Stateful graph with nodes, edges, and routing |

## What Can You Build?

| Use Case | Agent Type | Example |
|----------|------------|---------|
| **Chatbot with tools** | ReAct | Answer questions by searching the web or querying APIs |
| **Trading bot** | Graph | Analyze market â†’ decide action â†’ execute trade â†’ monitor |
| **Research assistant** | Graph | Gather sources â†’ summarize â†’ synthesize â†’ format report |
| **Customer support** | ReAct | Handle tickets by querying knowledge base and escalating |
| **Portfolio tracker** | Graph | Fetch prices â†’ calculate metrics â†’ generate alerts |

## SpoonOS vs Other Frameworks

| Feature | SpoonOS | LangChain | AutoGPT |
|---------|---------|-----------|---------|
| **Execution** | ReAct + Graph workflows | ReAct variants | Autonomous loop |
| **Tools** | `BaseTool` + MCP protocol | `Tool` class | Plugins |
| **Memory** | Built-in short-term + Mem0 | External modules | File-based |
| **Providers** | Unified multi-provider with fallback | Per-model adapters | Single provider |
| **Web3/Crypto** | Native CEX, DEX, on-chain toolkits | Third-party only | Limited |

---

## Quick Start

```bash
pip install spoon-ai
export OPENAI_API_KEY="your-key"
```

```python
import asyncio
from spoon_ai.agents import SpoonReactAI
from spoon_ai.chat import ChatBot

agent = SpoonReactAI(llm=ChatBot(model_name="gpt-5.1-chat-latest", llm_provider="openai"))

async def main():
    response = await agent.run("What is the capital of France?")
    print(response)

asyncio.run(main())
```

---

## Agent Types

### ReAct Agents

ReAct (Reasoning + Acting) agents follow a thought â†’ action â†’ observation loop. The agent thinks about what to do, executes a tool or generates a response, observes the result, and repeats until the task is complete.

```python
import os
from spoon_ai.agents import SpoonReactAI
from spoon_ai.chat import ChatBot
from spoon_ai.tools import ToolManager
from spoon_ai.tools.base import BaseTool
from spoon_ai.tools.mcp_tool import MCPTool
import asyncio
class PercentageTool(BaseTool):
    name: str = "calculate_percentage"
    description: str = "Calculate a percentage of a numeric value"
    parameters: dict = {
        "type": "object",
        "properties": {
            "value": {"type": "number", "description": "Base value"},
            "percent": {"type": "number", "description": "Percentage to apply (e.g., 10 for 10%)"},
        },
        "required": ["value", "percent"],
    }

    async def execute(self, value: float, percent: float) -> str:
        return str(value * percent / 100)

# MCP web search tool (requires TAVILY_API_KEY)
tavily_search = MCPTool(
    name="tavily-search",
    description="Search the web for current information",
    mcp_config={
        "command": "npx",
        "args": ["-y", "tavily-mcp"],
        "env": {"TAVILY_API_KEY": os.getenv("TAVILY_API_KEY")},
    },
)

# Agent with real tools
agent = SpoonReactAI(
    llm=ChatBot(model_name="gpt-5.1-chat-latest", llm_provider="openai"),
    tools=ToolManager([tavily_search, PercentageTool()]),
    max_iterations=10  # Limit reasoning loops
)

async def main():
    # Pre-load MCP tool parameters so LLM can see the correct schema
    await tavily_search.ensure_parameters_loaded()
    print("MCP tool parameters loaded")
    response = await agent.run("Search Bitcoin price and calculate 10% of it")
    print(response)

asyncio.run(main())
```

**Best for:** Single-step tasks, API calls, Q&A, simple automation.

### Graph Agents

Graph agents execute structured workflows defined as state graphs, supporting conditional branching, parallel execution, and complex multi-step pipelines.

**Basic Usage:**

```python
from spoon_ai.graph import StateGraph, START, END, GraphAgent

# Build workflow
graph = StateGraph(MyState)
graph.add_node("analyze", analyze_fn)  # Node function
graph.add_node("execute", execute_fn)  # Node function
graph.set_entry_point("analyze")
graph.add_conditional_edges("analyze", router_fn, {
    "execute": "execute",
    "skip": "skip_execution"
})

# Create agent
agent = GraphAgent(
    name="market_agent",
    graph=graph
)

result = await agent.run("Analyze market and execute trades")
```

**Node Functions:**
- `analyze_fn(state)`: Receives current state, returns updated state dict
- `execute_fn(state)`: Receives state with analysis results, returns execution results
- `router_fn(state)`: Returns next node name as string for conditional routing

> ðŸ“– **See complete example:** [`examples/intent_graph_demo.py`](https://github.com/XSpoonAi/spoon-core/blob/main/examples/intent_graph_demo.py) for full implementation including state schema, node functions, and error handling.
```

**Best for:** Multi-step workflows, conditional logic, parallel tasks, human-in-the-loop.

---

## Agent Architecture

### Core Components

1. **LLM Provider** - The language model powering the agent
2. **Tool Manager** - Manages available tools and execution
3. **Memory System** - Stores conversation history and context
4. **Prompt System** - Handles system prompts and instructions

### Agent Lifecycle

```mermaid
graph TD
    A[Initialize Agent] --> B[Load Tools]
    B --> C[Receive Input]
    C --> D[Reason About Task]
    D --> E[Select Action]
    E --> F[Execute Tool]
    F --> G[Observe Result]
    G --> H{Task Complete?}
    H -->|No| D
    H -->|Yes| I[Return Response]
```

## Creating Custom Agents

### Basic Agent Setup

```python
from spoon_ai.agents.base import BaseAgent
from spoon_ai.tools import ToolManager

class CustomAgent(BaseAgent):
    def __init__(self, llm, tools=None):
        super().__init__(llm)
        self.tool_manager = ToolManager(tools or [])

    async def run(self, message: str) -> str:
        # Custom agent logic here
        return await self.process_message(message)
```

### Agent Configuration

```python
import os
import asyncio
from dotenv import load_dotenv
from spoon_ai.agents.spoon_react_mcp import SpoonReactMCP
from spoon_ai.tools import ToolManager
from spoon_ai.tools.mcp_tool import MCPTool
from spoon_ai.chat import ChatBot
from spoon_toolkits.crypto.crypto_powerdata.tools import CryptoPowerDataCEXTool

# Install extra dependency: pip install spoon-toolkits
load_dotenv()

class SpoonMacroAnalysisAgent(SpoonReactMCP):
    name: str = "SpoonMacroAnalysisAgent"
    system_prompt: str = (
        "You are a crypto market analyst. Use tavily-search for news and "
        "crypto_power_data_cex for market data, then deliver a concise macro view."
    )

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.available_tools = ToolManager([])

    async def initialize(self):
        tavily_key = os.getenv("TAVILY_API_KEY")
        if not tavily_key:
            raise Val ueError("Set TAVILY_API_KEYbefore running the agent.")

        tavily_tool = MCPTool(
            name="tavily-search",
            description="Web search via Tavily",
            mcp_config={
                "command": "npx",
                "args": ["--yes", "tavily-mcp"],
                "env": {"TAVILY_API_KEY": tavily_key},
            },
        )

        # Pre-load MCP tool parameters
        await tavily_tool.ensure_parameters_loaded()

        crypto_tool = CryptoPowerDataCEXTool()
        self.available_tools = ToolManager([tavily_tool, crypto_tool])


async def main():
    print("--- SpoonOS Macro Analysis Agent Demo ---")
    agent = SpoonMacroAnalysisAgent(
        llm=ChatBot(llm_provider="gemini", model_name="gemini-2.5-flash")
    )
    await agent.initialize()

    query = (
        "Perform a macro analysis of Bitcoin (BTC). "
        "Search for recent news and get current market data from Binance using BTC/USDT pair."
    )
    print(f"\nRunning query: {query}")
    response = await agent.run(query)
    print(f"\n--- Analysis Complete ---\n{response}")


if __name__ == "__main__":
    asyncio.run(main())
```

## Best Practices

### Tool Selection
- Choose tools that match your use case
- Avoid tool overload - too many tools can confuse the agent
- Test tool combinations thoroughly

### Prompt Engineering
- Provide clear, specific instructions
- Include examples of desired behavior
- Set appropriate constraints and guidelines

### Error Handling

- Leverage framework's automatic retry mechanisms
- Use built-in fallback strategies
- Rely on framework's structured error handling

### Framework Error Handling

SpoonOS agents benefit from built-in error resilience:

```python
# Framework handles errors automatically
# Requires: pip install spoon-toolkits
from spoon_ai.tools import ToolManager
from spoon_toolkits.crypto.crypto_powerdata.tools import CryptoPowerDataCEXTool

agent = SpoonReactAI(
    llm=ChatBot(model_name="gpt-5.1-chat-latest", llm_provider="openai"),
    tools=ToolManager([CryptoPowerDataCEXTool()])  # real tool with retries/failures
)

# Automatic handling includes:
# - LLM provider failures with fallback
# - Tool execution errors with retry
# - Network issues with graceful degradation
response = await agent.run("Get Bitcoin price and analyze trends")
```

## Performance Considerations

### Memory Usage
- ReAct agents: Lower memory footprint
- Graph agents: Higher memory for complex workflows

### Execution Speed
- Simple tasks: ReAct agents are faster
- Complex workflows: Graph agents are more efficient

### Scalability
- ReAct: Better for high-frequency, simple tasks
- Graph: Better for complex, long-running processes

## Next Steps

### ðŸ“š **Agent Implementation Examples**

#### ðŸŽ¯ [Intent Graph Demo](../examples/intent-graph-demo.md)
**GitHub**: [View Source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/intent_graph_demo.py)

**What it demonstrates:**
- Complete Graph agent implementation with intelligent routing
- Long-lived agent architecture with persistent memory
- Advanced state management and context preservation
- Production-ready error handling and recovery

**Key features:**
- Dynamic query routing based on user intent (general_qa â†’ short_term_trend â†’ macro_trend â†’ deep_research)
- True parallel execution across multiple data sources
- Memory persistence and conversation context
- Real-time performance monitoring and metrics

**Best for learning:**
- Graph agent architecture patterns
- Long-running process management
- Advanced memory and state handling
- Production deployment considerations

#### ðŸ” [MCP Spoon Search Agent](../examples/mcp-spoon-search-agent.md)
**GitHub**: [View Source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/mcp/spoon_search_agent.py)

**What it demonstrates:**
- MCP-enabled agent with dynamic tool discovery
- Web search integration with cryptocurrency analysis
- Multi-tool orchestration and data synthesis
- Real-world agent deployment patterns

**Key features:**
- Tavily MCP server integration for web search
- Crypto PowerData tools for market analysis
- Unified analysis combining multiple data sources
- Dynamic tool loading and validation

**Best for learning:**
- MCP protocol implementation
- Multi-tool agent architecture
- Real-time data integration patterns
- Error handling in distributed systems

#### ðŸ“Š [Graph Crypto Analysis](../examples/graph-crypto-analysis.md)
**GitHub**: [View Source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/graph_crypto_analysis.py)

**What it demonstrates:**
- Specialized cryptocurrency analysis agent
- LLM-driven decision making throughout the workflow
- Real-time market data processing and analysis
- Investment recommendation generation

**Key features:**
- Real Binance API integration (no simulated data)
- Technical indicator calculation (RSI, MACD, EMA, Bollinger Bands)
- Multi-timeframe analysis and correlation
- Risk assessment and market sentiment analysis

**Best for learning:**
- Domain-specific agent development
- Financial data processing patterns
- LLM-driven workflow automation
- Real API integration in agents

### ðŸ› ï¸ **Development Guides**

- **[Tools System](./tools.md)** - Complete guide to available tools and integrations
- **[LLM Providers](./llm-providers.md)** - Configure and optimize language models
- **[Build Your First Agent](../how-to-guides/build-first-agent.md)** - Step-by-step agent development tutorial

### ðŸ“– **Advanced Topics**

- **[Graph System](../graph-system/index.md)** - Advanced workflow orchestration
- **[MCP Protocol](../core-concepts/mcp-protocol.md)** - Dynamic tool discovery and execution
- **[API Reference](../api-reference/spoon_ai/agents/base/)** - Complete agent API documentation

---

FILE: docs/core-concepts/graph-system.md

# Graph System

The SpoonOS Graph System is a library for building **stateful, multi-step AI agent workflows**. It models applications as directed graphs where **nodes** represent actions (calling an LLM, executing a tool, processing data) and **edges** define how control flows between themâ€”including conditional branching, parallel fan-out, and cycles for iterative reasoning.

## Why Graph System?

Traditional LLM applications are often simple chains: prompt â†’ response â†’ done. But real-world AI agents need more:

- **State persistence** â€” Remember context across multiple steps and interactions
- **Conditional logic** â€” Take different paths based on LLM outputs or external data
- **Parallel execution** â€” Run multiple tasks simultaneously and combine results
- **Human-in-the-loop** â€” Pause for user input, approval, or correction
- **Error recovery** â€” Handle failures gracefully without losing progress

The Graph System makes these patterns first-class citizens, not afterthoughts.

## Key Concepts

```mermaid
graph LR
    A[User Input] --> B[Node: Analyze]
    B -->|route| C{Router}
    C -->|intent=search| D[Node: Search]
    C -->|intent=calculate| E[Node: Calculate]
    C -->|intent=chat| F[Node: Respond]
    D --> G[Node: Summarize]
    E --> G
    F --> H[END]
    G --> H
```

| Concept | Description |
|---------|-------------|
| **State** | A typed dictionary (`TypedDict`) shared across all nodes. Each node reads state, performs work, and returns updates to merge back. |
| **Node** | An async function that receives state and returns a partial update. Nodes are the "actions" in your workflow. |
| **Edge** | A connection between nodes. Can be static (always go Aâ†’B), conditional (go Aâ†’B or Aâ†’C based on state), or LLM-driven. |
| **Checkpoint** | An automatic snapshot of state before each node. Enables recovery, debugging, and human-in-the-loop interrupts. |

## What Can You Build?

| Use Case | How Graph System Helps |
|----------|----------------------|
| **Autonomous Agents** | Multi-step reasoning with tool calls, observation loops, and adaptive planning |
| **RAG Pipelines** | Retrieve â†’ Grade â†’ Regenerate cycles with conditional routing based on relevance |
| **Multi-Agent Systems** | Multiple specialized agents collaborating via shared state and handoffs |
| **Approval Workflows** | Pause execution for human review, then resume from checkpoint |
| **Parallel Analysis** | Fan-out to multiple data sources, join results with configurable strategies |

## Graph System vs LangGraph

SpoonOS Graph System is inspired by [LangGraph](https://github.com/langchain-ai/langgraph) and shares similar concepts. Key differences:

| Feature | SpoonOS Graph | LangGraph |
|---------|---------------|-----------|
| **Parallel Groups** | Native `add_parallel_group()` with quorum joins, timeouts, circuit breakers | Manual asyncio or branching |
| **Routing Stack** | Priority-based: explicit â†’ rules â†’ intelligent â†’ LLM â†’ fallback | Conditional edges only |
| **Declarative Definition** | `GraphTemplate` / `NodeSpec` / `EdgeSpec` for serializable, composable graphs | Imperative builder only |
| **Resource Control** | Built-in rate limiting, max concurrency, circuit breakers | External implementation |
| **Web3/Crypto** | Native integration with SpoonOS toolkits (CEX, DEX, on-chain) | Via third-party tools |

Choose SpoonOS Graph when you need production-grade parallel execution, multi-layer routing, or crypto/Web3 integrations.

---

## Quick Start

```bash
pip install spoon-ai
```

```python
import asyncio
from typing import TypedDict
from spoon_ai.graph import StateGraph  # note: no START/END sentinels needed

class MyState(TypedDict):
    query: str
    result: str

async def process(state: MyState) -> dict:
    return {"result": f"Processed: {state['query']}"}

graph = StateGraph(MyState)
graph.add_node("process", process)

# â­ï¸ Tell SpoonOS where to start
graph.set_entry_point("process")

app = graph.compile()

async def main():
    result = await app.invoke({"query": "Hello", "result": ""})
    print(result["result"])  # should print: Processed: Hello

if __name__ == "__main__":
    asyncio.run(main())
```

---

## Architecture

```mermaid
flowchart TB
    subgraph Execution["CompiledGraph Execution"]
        direction TB
        S((START)) --> N1[Node: analyze_intent]
        N1 -->|"intent=price"| N2[Node: fetch_price]
        N1 -->|"intent=trade"| N3[Node: execute_trade]
        N1 -->|"intent=general"| N4[Node: general_response]
        N2 --> N5[Node: summarize]
        N3 --> N5
        N4 --> N5
        N5 --> E((END))
    end

    subgraph State["Shared State (TypedDict)"]
        ST["user_query: str<br/>intent: str<br/>result: str<br/>..."]
    end

    Execution -.->|"reads/writes"| State
```

The graph system consists of three main components:

| Component | Responsibility |
|-----------|---------------|
| **`StateGraph`** | Builder class for defining workflow topologyâ€”nodes, edges, routing rules, parallel groups |
| **`CompiledGraph`** | Executable runtime that manages state transitions, checkpointing, and metrics collection |
| **`GraphAgent`** | Optional wrapper that integrates graph execution with SpoonOS agent lifecycle and memory |

```mermaid
flowchart LR
    subgraph Build["Build Phase"]
        SG[StateGraph]
        SG -->|"add_node()"| N[Nodes]
        SG -->|"add_edge()"| E[Edges]
        SG -->|"add_parallel_group()"| P[Parallel Groups]
    end

    subgraph Compile["Compile Phase"]
        SG -->|"compile()"| CG[CompiledGraph]
    end

    subgraph Execute["Execute Phase"]
        CG -->|"invoke(state)"| R[Final State]
        CG -->|"stream(state)"| S[State Updates]
    end
```

---

## Core Concepts

### Nodes

A **node** is an async function that receives the current workflow state and returns a dictionary of updates to merge back into state.

```python
from typing import TypedDict, Dict, Any

class AnalysisState(TypedDict):
    user_query: str
    symbol: str
    price_data: Dict[str, Any]
    analysis: str

async def fetch_price_node(state: AnalysisState) -> dict:
    """
    Node that fetches price data for a trading symbol.

    Receives: Full state dictionary
    Returns: Dictionary of fields to update (merged into state)
    """
    symbol = state.get("symbol", "BTC")

    # Use the CryptoPowerData tool for CEX data
    from spoon_toolkits.crypto.crypto_powerdata.tools import get_cex_data_with_indicators
    result = get_cex_data_with_indicators(
        exchange="binance",
        symbol=f"{symbol}/USDT",
        timeframe="1h",
        limit=24,
        indicators_config='{"sma": [{"timeperiod": 20}], "rsi": [{"timeperiod": 14}]}'
    )

    # Extract data from result
    data = result.get("data", [])
    if not data:
        return {"price_data": {"symbol": symbol, "error": "No data available"}}

    return {
        "price_data": {
            "symbol": symbol,
            "current_price": data[-1]["close"] if data else 0,
            "high_24h": max(c["high"] for c in data) if data else 0,
            "low_24h": min(c["low"] for c in data) if data else 0,
            "volume_24h": sum(c["volume"] for c in data) if data else 0,
        }
    }
```

**Node contract:**

- **Input**: Receives immutable view of current state
- **Output**: Returns `dict` of fields to update (partial update, not full replacement)
- **Async**: Should be `async def` (sync functions are auto-wrapped but less efficient)
- **Idempotent**: Should produce same result given same state (for retry safety)

### Edges

**Edges** define transitions between nodes. Three types are supported:

```python
from spoon_ai.graph import StateGraph, END

graph = StateGraph(AnalysisState)

# 1. Static edge: Always transitions to target
graph.add_edge("fetch_price", "analyze")
graph.add_edge("analyze", END)

# 2. Conditional edge: Routes based on state inspection
def route_by_intent(state: AnalysisState) -> str:
    """Return key from path_map based on detected intent."""
    intent = state.get("intent", "unknown")
    if intent == "price_query":
        return "price"
    elif intent == "technical_analysis":
        return "technical"
    return "general"

graph.add_conditional_edges(
    source="classify_intent",
    condition=route_by_intent,
    path_map={
        "price": "fetch_price",
        "technical": "compute_indicators",
        "general": "general_response",
    }
)

# 3. Routing rule: Pattern-based with priorities
graph.add_routing_rule(
    source_node="entry",
    condition=lambda state, query: "bitcoin" in query.lower(),
    target_node="btc_specialist",
    priority=10  # Higher priority = checked first
)
```

### State

**State** is a shared `TypedDict` that flows through the graph. Each node reads from state and returns updates to merge.

```python
from typing import TypedDict, List, Dict, Any, Optional, Annotated

class CryptoAnalysisState(TypedDict):
    # Input fields
    user_query: str
    user_id: str

    # Intermediate fields
    intent: str
    symbol: str
    timeframes: List[str]

    # Data fields
    price_data: Dict[str, Any]
    technical_indicators: Dict[str, float]
    news_sentiment: Dict[str, Any]

    # Output fields
    analysis: str
    recommendations: List[str]
    confidence: float

    # System fields
    messages: Annotated[List[dict], "Conversation history - appended via reducer"]
    execution_log: List[str]
```

**State merge behavior:**

| Field Type | Merge Strategy |
|------------|---------------|
| `dict` | Deep merge (nested dicts merged recursively) |
| `list` | Append (capped at 100 items to prevent unbounded growth) |
| `messages` field | Uses `add_messages` reducer (append with deduplication) |
| Other types | Replace |

### Checkpointing

The graph system automatically checkpoints state before each node execution, enabling:

- **Recovery**: Resume from last successful node after failure
- **Debugging**: Inspect state at any point in execution history
- **Human-in-the-loop**: Pause execution, collect input, resume

```python
from spoon_ai.graph import InMemoryCheckpointer, StateGraph

# Configure checkpointer
checkpointer = InMemoryCheckpointer(max_checkpoints_per_thread=100)
graph = StateGraph(AnalysisState, checkpointer=checkpointer)

# After compilation, access state history
compiled = graph.compile()
result = await compiled.invoke(
    {"user_query": "Analyze BTC"},
    config={"configurable": {"thread_id": "session_123"}}
)

# Retrieve checkpoint history
config = {"configurable": {"thread_id": "session_123"}}
history = list(graph.get_state_history(config))

for checkpoint in history:
    print(f"Node: {checkpoint.metadata.get('node')}")
    print(f"Iteration: {checkpoint.metadata.get('iteration')}")
    print(f"State keys: {list(checkpoint.values.keys())}")
```

---

## Building Graphs

### Imperative API

For simple workflows, use the imperative builder methods directly:

```python
from spoon_ai.graph import StateGraph, END

graph = StateGraph(AnalysisState)

# Add nodes
graph.add_node("classify", classify_intent_node)
graph.add_node("fetch_price", fetch_price_node)
graph.add_node("analyze", analyze_node)
graph.add_node("respond", generate_response_node)

# Add edges
graph.add_edge("classify", "fetch_price")
graph.add_edge("fetch_price", "analyze")
graph.add_edge("analyze", "respond")
graph.add_edge("respond", END)

# Set entry point
graph.set_entry_point("classify")

# Compile and execute
compiled = graph.compile()
result = await compiled.invoke({"user_query": "What is BTC price?"})
```

### Declarative API

For larger workflows, use `GraphTemplate` for better maintainability:

```python
from spoon_ai.graph import StateGraph, END
from spoon_ai.graph.builder import (
    DeclarativeGraphBuilder,
    GraphTemplate,
    NodeSpec,
    EdgeSpec,
    ParallelGroupSpec,
)
from spoon_ai.graph.config import GraphConfig, ParallelGroupConfig

# Define node specifications
nodes = [
    NodeSpec("classify", classify_intent_node),
    NodeSpec("fetch_price", fetch_price_node, parallel_group="data_fetch"),
    NodeSpec("fetch_news", fetch_news_node, parallel_group="data_fetch"),
    NodeSpec("fetch_sentiment", fetch_sentiment_node, parallel_group="data_fetch"),
    NodeSpec("analyze", analyze_node),
    NodeSpec("respond", generate_response_node),
]

# Define edge specifications
edges = [
    EdgeSpec("classify", "fetch_price"),  # Entry to parallel group
    EdgeSpec("fetch_price", "analyze"),   # All parallel nodes -> analyze
    EdgeSpec("fetch_news", "analyze"),
    EdgeSpec("fetch_sentiment", "analyze"),
    EdgeSpec("analyze", "respond"),
    EdgeSpec("respond", END),
]

# Define parallel groups
parallel_groups = [
    ParallelGroupSpec(
        name="data_fetch",
        nodes=["fetch_price", "fetch_news", "fetch_sentiment"],
        config=ParallelGroupConfig(
            join_strategy="all",
            timeout=30.0,
            error_strategy="collect_errors",
        )
    )
]

# Create template
template = GraphTemplate(
    entry_point="classify",
    nodes=nodes,
    edges=edges,
    parallel_groups=parallel_groups,
    config=GraphConfig(max_iterations=50),
)

# Build graph
builder = DeclarativeGraphBuilder(AnalysisState)
graph = builder.build(template)
```

---

## Routing Strategies

The graph system evaluates routing in priority order:

```mermaid
flowchart TD
    A[Current Node Complete] --> B{Explicit Edge?}
    B -->|Yes| C[Follow Edge]
    B -->|No| D{Routing Rule Match?}
    D -->|Yes| E[Apply Rule]
    D -->|No| F{Intelligent Router?}
    F -->|Yes| G[Call Router Function]
    F -->|No| H{LLM Router Enabled?}
    H -->|Yes| I[LLM Decides Next Node]
    H -->|No| J{Default Target Set?}
    J -->|Yes| K[Use Default]
    J -->|No| L[END]
```

### Conditional Edges

Route based on state inspection:

```python
def route_by_confidence(state: AnalysisState) -> str:
    """Route based on analysis confidence level."""
    confidence = state.get("confidence", 0.0)
    if confidence >= 0.8:
        return "high_confidence"
    elif confidence >= 0.5:
        return "medium_confidence"
    return "low_confidence"

graph.add_conditional_edges(
    "analyze",
    route_by_confidence,
    {
        "high_confidence": "generate_recommendation",
        "medium_confidence": "request_clarification",
        "low_confidence": "escalate_to_human",
    }
)
```

### LLM-Powered Routing

Enable natural language routing when patterns are complex:

```python
from spoon_ai.graph.config import GraphConfig, RouterConfig

config = GraphConfig(
    router=RouterConfig(
        allow_llm=True,
        llm_timeout=8.0,
        default_target="fallback_handler",
        allowed_targets=["price_handler", "trade_handler", "analysis_handler"],
    )
)

graph = StateGraph(AnalysisState)
graph.config = config

# Or enable after graph creation
graph.enable_llm_routing(config={
    "model": "gpt-4",
    "temperature": 0.1,
    "max_tokens": 50,
})
```

---

## Parallel Execution

Execute multiple nodes concurrently with configurable join and error strategies:

```python
from spoon_ai.graph.config import ParallelGroupConfig, ParallelRetryPolicy

# Define parallel data collection
graph.add_parallel_group(
    "market_data_collection",
    nodes=["fetch_binance", "fetch_coinbase", "fetch_kraken"],
    config=ParallelGroupConfig(
        # Join strategy
        join_strategy="quorum",  # "all", "any", "quorum"
        quorum=0.66,             # 66% must complete (2 of 3)

        # Timing
        timeout=15.0,            # Max wait time in seconds

        # Error handling
        error_strategy="collect_errors",  # "fail_fast", "collect_errors", "ignore_errors"

        # Retry policy for individual nodes
        retry_policy=ParallelRetryPolicy(
            max_retries=2,
            backoff_initial=0.5,
            backoff_multiplier=2.0,
            backoff_max=5.0,
        ),

        # Resource controls
        max_in_flight=10,                # Max concurrent tasks
        circuit_breaker_threshold=5,     # Disable group after N failures
        circuit_breaker_cooldown=30.0,   # Re-enable after cooldown
    )
)
```

**Join Strategies:**

| Strategy | Behavior | Use Case |
|----------|----------|----------|
| `"all"` | Wait for all nodes | Need complete data from all sources |
| `"any"` | Return on first success | Redundant sources, want fastest |
| `"quorum"` | Wait for majority | Fault-tolerant consensus |

**Error Strategies:**

| Strategy | Behavior | Use Case |
|----------|----------|----------|
| `"fail_fast"` | Cancel all, raise exception | Critical path, must succeed |
| `"collect_errors"` | Continue, store errors in `__errors__` | Best-effort, report issues |
| `"ignore_errors"` | Continue, discard failures | Non-critical enrichment |

---

## Human-in-the-Loop

Interrupt execution to collect user input:

```python
from spoon_ai.graph import interrupt, Command

async def confirm_trade_node(state: AnalysisState) -> dict:
    """Node that requires user confirmation before proceeding."""
    trade_details = state.get("trade_details", {})

    if not state.get("user_confirmed"):
        # Interrupt execution
        interrupt({
            "type": "confirmation_required",
            "question": f"Execute {trade_details['action']} {trade_details['amount']} {trade_details['symbol']}?",
            "trade_details": trade_details,
        })

    # This code runs after resume with confirmation
    return {"trade_executed": True, "execution_time": "2024-01-15T10:30:00Z"}

# Handling the interrupt
compiled = graph.compile()

# Initial execution - will interrupt
result = await compiled.invoke(
    {"user_query": "Buy 0.1 BTC"},
    config={"configurable": {"thread_id": "trade_session"}}
)

if "__interrupt__" in result:
    interrupt_info = result["__interrupt__"][0]
    print(f"Question: {interrupt_info['value']['question']}")

    # Get user confirmation (from UI, API, etc.)
    user_confirmed = await get_user_confirmation()

    # Resume execution with confirmation
    result = await compiled.invoke(
        Command(resume={"user_confirmed": user_confirmed}),
        config={"configurable": {"thread_id": "trade_session"}}
    )
```

---

## GraphAgent Integration

`GraphAgent` wraps graph execution with SpoonOS agent lifecycle and persistent memory:

```python
from spoon_ai.graph import StateGraph, GraphAgent, Memory

# Build graph
graph = build_analysis_graph()

# Create agent with memory
agent = GraphAgent(
    name="crypto_analyzer",
    graph=graph,
    memory_path="./agent_memory",
    session_id="user_123_session",
    preserve_state=True,  # Preserve state between runs
)

# Execute
result = await agent.run("Analyze BTC price trends")
print(result)

# Access execution metadata
metadata = agent.get_execution_metadata()
print(f"Successful: {metadata.get('execution_successful')}")

# Memory operations
agent.set_memory_metadata("last_analysis_time", "2024-01-15T10:30:00Z")
stats = agent.get_memory_statistics()
print(f"Total messages: {stats['total_messages']}")

# Search memory
matches = agent.search_memory("bitcoin", limit=5)

# Switch sessions
agent.load_session("user_456_session")
```

---

## Monitoring and Debugging

### Execution Metrics

```python
# Enable monitoring
graph.enable_monitoring([
    "execution_time",
    "success_rate",
    "routing_performance",
])

compiled = graph.compile()
result = await compiled.invoke(initial_state)

# Retrieve metrics
metrics = compiled.get_execution_metrics()
print(f"""
Execution Summary:
  Total executions: {metrics['total_executions']}
  Success rate: {metrics['success_rate']:.1%}
  Avg execution time: {metrics['avg_execution_time']:.3f}s

Per-Node Statistics:
""")
for node, stats in metrics['node_stats'].items():
    print(f"  {node}:")
    print(f"    Calls: {stats['count']}")
    print(f"    Avg time: {stats['avg_time']:.3f}s")
    print(f"    Error rate: {stats['error_rate']:.1%}")
```

### Execution History

```python
# Get detailed execution history
history = compiled.get_execution_history()

for step in history:
    print(f"""
Step: {step['node']}
  Iteration: {step['iteration']}
  Success: {step['success']}
  Execution time: {step['execution_time']:.3f}s
  Timestamp: {step['timestamp']}
""")
```

---

## Configuration Reference

### GraphConfig

```python
from spoon_ai.graph.config import GraphConfig, RouterConfig

config = GraphConfig(
    # Execution limits
    max_iterations=100,              # Maximum node transitions per invoke()

    # Router configuration
    router=RouterConfig(
        allow_llm=False,             # Enable LLM-based routing
        allowed_targets=None,        # Restrict valid routing targets (None = all)
        default_target=None,         # Fallback target when no route matches
        llm_timeout=8.0,             # Timeout for LLM router calls
        enable_fallback_to_default=True,  # Use default_target on routing failure
    ),

    # Validation
    state_validators=[],             # List of (state) -> None functions

    # Pre-configured parallel groups
    parallel_groups={},              # name -> ParallelGroupConfig
)
```

### ParallelGroupConfig

```python
from spoon_ai.graph.config import ParallelGroupConfig, ParallelRetryPolicy

config = ParallelGroupConfig(
    # Join behavior
    join_strategy="all",             # "all", "any", "quorum"
    quorum=None,                     # For quorum: 0.0-1.0 (ratio) or int (count)
    join_condition=None,             # Optional: async (state, completed_nodes) -> bool

    # Timing
    timeout=None,                    # Max wait time in seconds (None = unlimited)

    # Error handling
    error_strategy="fail_fast",      # "fail_fast", "collect_errors", "ignore_errors"
    retry_policy=ParallelRetryPolicy(
        max_retries=0,               # Retries per node
        backoff_initial=0.5,         # Initial backoff delay
        backoff_multiplier=2.0,      # Backoff multiplier
        backoff_max=10.0,            # Maximum backoff delay
    ),

    # Resource controls
    max_in_flight=None,              # Max concurrent tasks (None = unlimited)
    rate_limit_per_second=None,      # Rate limit (None = unlimited)

    # Circuit breaker
    circuit_breaker_threshold=None,  # Disable after N failures
    circuit_breaker_cooldown=30.0,   # Re-enable after cooldown seconds
)
```

---

## Best Practices

### Node Design

1. **Single responsibility**: Each node should do one thing. Split complex logic across multiple nodes.

2. **Idempotency**: Nodes should produce the same result given the same state (enables safe retries).

3. **Minimal state updates**: Return only changed fields. The system handles merging.

```python
# Good: Returns only updated fields
async def good_node(state):
    return {"result": "computed value"}

# Avoid: Returns entire state copy
async def avoid_node(state):
    new_state = state.copy()
    new_state["result"] = "computed value"
    return new_state
```

### State Management

1. **Type your state**: Use `TypedDict` for IDE support and documentation.

2. **Bound list growth**: Use reducers or explicit trimming to prevent unbounded lists.

3. **Avoid large objects**: State is checkpointed frequently. Keep it JSON-serializable and reasonably sized.

### Error Handling

1. **Use appropriate error strategies**: `fail_fast` for critical paths, `collect_errors` for best-effort.

2. **Add validation**: Use `state_validators` to catch invalid state early.

3. **Log in nodes**: Include context in error messages for debugging.

```python
async def robust_node(state):
    try:
        result = await external_api_call(state["symbol"])
        return {"data": result}
    except ExternalAPIError as e:
        logger.error(f"API call failed for {state['symbol']}: {e}")
        return {"error": str(e), "data": None}
```

---

## Examples

- **[Intent Graph Demo](../examples/intent-graph-demo.md)** â€” Intelligent routing with parallel execution
  ([source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/intent_graph_demo.py))

- **[Graph Crypto Analysis](../examples/graph-crypto-analysis.md)** â€” Real-time market analysis pipeline
  ([source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/graph_crypto_analysis.py))

## Related Documentation

- **[Agents](./agents.md)** â€” When to use `GraphAgent` vs `ReActAgent`
- **[Tools Integration](./tools.md)** â€” Adding tools to graph nodes
- **[MCP Protocol](./mcp-protocol.md)** â€” Dynamic tool discovery in workflows

---

FILE: docs/core-concepts/llm-providers.md

# LLM Providers

SpoonOS provides a **unified interface** to multiple LLM providers. Write your code once, then switch between OpenAI, Anthropic, Google, DeepSeek, or OpenRouter by changing a single parameterâ€”no code rewrites, no API differences to handle.

## Why Multi-Provider?

Relying on a single LLM provider is risky:

- **Outages** â€” OpenAI goes down, your app goes down
- **Rate limits** â€” Hit the ceiling, requests fail
- **Cost** â€” Different models have different pricing
- **Capabilities** â€” Some models excel at code, others at analysis

SpoonOS solves this with:

```mermaid
graph LR
    A[Your Agent] --> B[ChatBot]
    B --> C{Provider Router}
    C -->|primary| D[OpenAI GPT-4]
    C -->|fallback 1| E[Anthropic Claude]
    C -->|fallback 2| F[Google Gemini]
    D -->|rate limited| C
    E -->|success| A
```

## Provider Comparison

| Provider | Best For | Context | Strengths |
|----------|----------|---------|-----------|
| **OpenAI** | General purpose, code | 128K | Fastest iteration, best tool calling |
| **Anthropic** | Long documents, analysis | 200K | Prompt caching, safety features |
| **Google** | Multimodal, cost-sensitive | 1M | Longest context, fast inference |
| **DeepSeek** | Complex reasoning, code | 64K | Best cost/performance for code |
| **OpenRouter** | Experimentation | Varies | 100+ models, automatic routing |

## Key Features

| Feature | What It Does |
|---------|--------------|
| **Unified API** | Same `ChatBot` class for all providers |
| **Auto-fallback** | Chain providers: GPT-4 â†’ Claude â†’ Gemini |
| **Streaming** | Real-time responses across all providers |
| **Tool calling** | Consistent function calling interface |
| **Token tracking** | Automatic counting and cost monitoring |

---

## Quick Start

```bash
pip install spoon-ai
export OPENAI_API_KEY="your-key"
```

```python
import asyncio
from spoon_ai.chat import ChatBot

# Same interface for all providersâ€”just change model_name and llm_provider
llm = ChatBot(model_name="gpt-5.1-chat-latest", llm_provider="openai")

async def main():
    response = await llm.ask([{"role": "user", "content": "Explain quantum computing in one sentence"}])
    print(response)

asyncio.run(main())
```

---

## Supported Providers

### OpenAI

- **Models**: GPT-5.1, GPT-4o, o1, o3, etc. ([See latest models](https://platform.openai.com/docs/models))
- **Features**: Function calling, streaming, embeddings, reasoning models
- **Best for**: General-purpose tasks, reasoning, code generation

```python
from spoon_ai.chat import ChatBot

# OpenAI configuration with default model
llm = ChatBot(
    model_name="gpt-5.1-chat-latest",  # Check docs for latest model names
    llm_provider="openai",
    temperature=0.7
)
```

### Anthropic (Claude)

- **Models**: Claude 4.5 Opus, Claude 4.5 Sonnet, etc. ([See latest models](https://docs.anthropic.com/en/docs/about-claude/models))
- **Features**: Large context windows, prompt caching, safety features
- **Best for**: Long documents, analysis, safety-critical applications

```python
# Anthropic configuration with default model
llm = ChatBot(
    model_name="claude-sonnet-4-20250514",  # Check docs for latest model names
    llm_provider="anthropic",
    temperature=0.1
)
```

### Google (Gemini)

- **Models**: Gemini 3 Pro, Gemini 2.5 Flash, etc. ([See latest models](https://ai.google.dev/gemini-api/docs/models))
- **Features**: Multimodal capabilities, fast inference, large context
- **Best for**: Multimodal tasks, cost-effective solutions, long context

```python
# Google configuration with default model
llm = ChatBot(
    model_name="gemini-3-pro",  # Check docs for latest model names
    llm_provider="gemini",
    temperature=0.1
)
```

### DeepSeek

- **Models**: DeepSeek-V3, DeepSeek-Reasoner, etc. ([See latest models](https://platform.deepseek.com/api-docs/))
- **Features**: Advanced reasoning, code-specialized models, cost-effective
- **Best for**: Complex reasoning, code generation, technical tasks

```python
# DeepSeek configuration with default model
llm = ChatBot(
    model_name="deepseek-reasoner",  # Check docs for latest model names
    llm_provider="deepseek",
    temperature=0.2
)
```

### OpenRouter

- **Models**: Access to multiple providers through one API
- **Features**: Model routing, cost optimization
- **Best for**: Experimentation, cost optimization

```python
# OpenRouter configuration
llm = ChatBot(
    model_name="anthropic/claude-3-opus",
    llm_provider="openrouter",
    temperature=0.7
)
```

## Unified LLM Manager

The LLM Manager provides provider-agnostic access with automatic fallback:

```python
from spoon_ai.llm.manager import LLMManager
from spoon_ai.schema import Message
import asyncio

# Initialize LLM Manager
llm_manager = LLMManager()

# Clear default_provider so fallback_chain takes precedence
llm_manager.default_provider = None

# Set fallback chain (primary provider first, then fallbacks)
llm_manager.set_fallback_chain(["gemini", "openai"])

async def main():
    # Create messages
    messages = [Message(role="user", content="Explain quantum computing in one sentence")]
    response = await llm_manager.chat(messages)
    print(f"Response: {response.content}")
    print(f"Provider used: {response.provider}")
    
if __name__ == "__main__":
    asyncio.run(main())
```

## Configuration

### Environment Variables

```bash
# Provider API Keys
OPENAI_API_KEY=sk-your_openai_key_here
ANTHROPIC_API_KEY=sk-ant-your_anthropic_key_here
GEMINI_API_KEY=your_gemini_key_here
DEEPSEEK_API_KEY=your_deepseek_key_here
OPENROUTER_API_KEY=sk-or-your_openrouter_key_here

# Default Settings
DEFAULT_LLM_PROVIDER=openai
DEFAULT_MODEL=gpt-5.1-chat-latest
DEFAULT_TEMPERATURE=0.3
```

### Runtime Configuration

```json
{
  "llm": {
    "provider": "openai",
    "model": "gpt-5.1-chat-latest",
    "temperature": 0.3,
    "max_tokens": 32768,
    "fallback_providers": ["anthropic", "deepseek", "gemini"]
  }
}
```

## Advanced Features

### Response Caching

```python
from spoon_ai.llm.cache import LLMResponseCache, CachedLLMManager
from spoon_ai.llm.manager import LLMManager
from spoon_ai.schema import Message
import asyncio


# Enable response caching to avoid redundant API calls
cache = LLMResponseCache()
llm_manager = LLMManager()
cached_manager = CachedLLMManager(llm_manager, cache=cache)

async def main():
    messages = [Message(role="user", content="Explain quantum computing in one sentence")]
    response1 = await cached_manager.chat(messages)
    print(response1)
    
if __name__ == "__main__":
    asyncio.run(main())

### Streaming Responses

```python
# Stream responses for real-time interaction
import asyncio
from spoon_ai.chat import ChatBot

async def main():
    # Create a ChatBot instance
    llm = ChatBot(
        model_name="gpt-5.1-chat-latest",
        llm_provider="openai",
        temperature=0.7
    )
    
    # Prepare messages
    messages = [{"role": "user", "content": "Write a long story about AI"}]

    # Stream the response chunk by chunk
    async for chunk in llm.astream(messages):
        # chunk.delta contains the text content of this chunk
        print(chunk.delta, end="", flush=True)

if __name__ == "__main__":
    asyncio.run(main())
```

### Function Calling

```python
# Define functions for the model to call
tools = [
    {
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "Get current weather for a location",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {
                        "type": "string",
                        "description": "The city and state, e.g. San Francisco, CA"
                    }
                },
                "required": ["location"]
            }
        }
    }
]

response = await llm.ask_tool(
        messages=messages,
        tools=tools
    )
```

## Model Selection Guide

### Task-Based Recommendations

> Choose the right model for your use case. Check official documentation for the latest model capabilities.

#### Code Generation
- **Recommended**: DeepSeek (cost-effective), OpenAI GPT models (fast iteration)
- **Alternative**: Anthropic Claude (strong reasoning)

#### Analysis & Reasoning
- **Recommended**: OpenAI o-series models, DeepSeek Reasoner, Claude
- **Alternative**: Gemini Pro

#### Cost-Sensitive Tasks
- **Recommended**: DeepSeek models, Gemini models
- **Alternative**: OpenRouter for provider comparison

#### Long Context Tasks
- **Recommended**: Gemini (largest context), Claude (large context)
- **Alternative**: Check each provider's latest context window limits

### Performance Comparison

> **Note**: Model capabilities and pricing change frequently. Always check the official documentation for the latest information:
> - [OpenAI Models](https://platform.openai.com/docs/models)
> - [Anthropic Models](https://docs.anthropic.com/en/docs/about-claude/models)
> - [Google Gemini Models](https://ai.google.dev/gemini-api/docs/models)
> - [DeepSeek Models](https://platform.deepseek.com/api-docs/)

| Provider | Model Example | Context Window | Best For |
|----------|---------------|----------------|----------|
| **OpenAI** | gpt-5.1-chat-latest | Check docs | General purpose, tool calling |
| **Anthropic** | claude-sonnet-4-20250514 | Check docs | Analysis, long documents |
| **Google** | gemini-2.5-pro | Check docs | Multimodal, cost-effective |
| **DeepSeek** | deepseek-reasoner | Check docs | Reasoning, code generation |
| **OpenRouter** | Various | Varies | Access multiple providers |

## Error Handling & Fallbacks

### Automatic Fallback

The framework provides built-in error handling with automatic fallback between providers:

```python
"""
LLMManager with fallback chain demo - demonstrates automatic provider fallback.
"""
from spoon_ai.llm.manager import LLMManager
from spoon_ai.schema import Message
import asyncio

# Initialize LLM Manager
llm_manager = LLMManager()
# Clear default_provider so fallback_chain takes precedence
llm_manager.default_provider = None
# The manager will try providers in order: gemini -> openai -> anthropic
llm_manager.set_fallback_chain(["gemini", "openai", "anthropic"])

async def main():
    # Create messages
    messages = [Message(role="user", content="Hello world")]
    response = await llm_manager.chat(messages)
    print(response.content)
 
if __name__ == "__main__":
    asyncio.run(main())
```

### Error Types & Recovery

The framework uses structured error types for clean error handling:

```python
from spoon_ai.llm.errors import RateLimitError, AuthenticationError, ModelNotFoundError

# Simple error handling with specific error types
response = await llm.ask([{"role": "user", "content": "Hello world"}])

# Framework handles common errors automatically:
# - Rate limits: automatic retry with backoff
# - Network issues: automatic retry with fallback
# - Authentication: clear error messages
# - Model availability: fallback to alternative models
```

### Graceful Degradation

```python
# Framework provides graceful degradation patterns
llm_manager = LLMManager()
llm_manager.default_provider = "openai"
llm_manager.set_fallback_chain(["openai", "deepseek", "gemini"]) # Cost-effective fallbacks

# If primary fails, automatically uses fallback
# No manual error handling required
messages = [Message(role="user", content="Complex reasoning task: Explain quantum computing and its applications")]
await llm_manager.chat(messages)
```

## Monitoring & Metrics

### Usage Tracking

```python
from spoon_ai.llm.monitoring import MetricsCollector, get_metrics_collector

# Get the global metrics collector
collector = get_metrics_collector()

# Metrics are automatically tracked during LLM calls
response = await llm.ask([{"role": "user", "content": "Hello"}])

# Get collected stats per provider
 stats = collector.get_provider_stats("openai")
print(f" Total requests: {stats.total_requests}")
print(f" Successful requests: {stats.successful_requests}")
print(f" Failed requests: {stats.failed_requests}")
print(f" Success rate: {stats.success_rate:.2f}%")
print(f" Average duration: {stats.average_duration:.3f}s")
print(f" Total tokens: {stats.total_tokens}")
print(f" Total cost: ${stats.total_cost:.6f}")
```

### Performance Monitoring

```python
# The MetricsCollector automatically tracks:
# - Request counts and success/failure rates
# - Token usage (input/output)
# - Latency statistics (average, min, max)
# - Error tracking per provider

# Access provider-specific stats
for provider in ["openai", "anthropic", "gemini"]:
    stats = collector.get_provider_stats(provider)
    if stats and stats.total_requests > 0:
        print(f"{provider}: {stats.total_requests} requests, {stats.failed_requests} errors")

# Access provider-specific stats
all_stats = collector.get_all_stats()
if all_stats:
    print(f"\nðŸ“ˆ All Providers Summary:")
    for provider_name, provider_stats in all_stats.items():
        print(f"{provider_name}: {provider_stats.total_requests} requests, "
                f"{provider_stats.success_rate:.1f}% success rate")
```

## Best Practices

### Provider Selection

- **Test multiple providers** for your specific use case
- **Consider cost vs. quality** trade-offs
- **Use fallbacks** for production reliability

### Configuration Management

- **Store API keys securely** in environment variables
- **Use configuration files** for easy switching
- **Monitor usage and costs** regularly

### Performance Optimization

- **Cache responses** when appropriate
- **Use streaming** for long responses
- **Batch requests** when possible

### Error Handling Philosophy

The SpoonOS framework follows a "fail-fast, recover-gracefully" approach:

- **Automatic Recovery**: Common errors (rate limits, network issues) are handled automatically
- **Structured Errors**: Use specific error types instead of generic exceptions
- **Fallback Chains**: Configure multiple providers for automatic failover
- **Minimal Try-Catch**: Let the framework handle errors; only catch when you need custom logic

```python
# Preferred: Let framework handle errors
messages = [Message(role="user", content="Hello world")]
response = await llm_manager.chat("Hello world")

# Only use explicit error handling for custom business logic
if response.provider != "openai":
    logger.info(f"Fell back to {response.provider}")
```

## Next Steps

- [Agents](./agents.md) - Learn how agents use LLMs
- [MCP Protocol](./mcp-protocol.md) - Dynamic tool integration
- [Configuration Guide](../getting-started/configuration.md) - Detailed setup instructions

---

FILE: docs/core-concepts/long-term memory.md

# Long-Term Memory

Long-term memory lets your agent **remember across sessions**. Unlike short-term memory (which resets each conversation), long-term memory persists indefinitelyâ€”enabling personalized experiences, learning from past interactions, and building knowledge over time.

## Why Long-Term Memory?

Without long-term memory, every conversation starts from zero:

```text
Session 1: User: "I prefer dark mode"    Agent: "Got it!"
Session 2: User: "Change my settings"    Agent: "What settings?" â† forgot everything
```

With long-term memory:

```text
Session 1: User: "I prefer dark mode"    Agent: "Got it!" â†’ saves to memory
Session 2: User: "Change my settings"    Agent: "I'll enable dark mode for you" â† remembers
```

## How It Works

SpoonOS integrates with [Mem0](https://mem0.ai), a managed memory service that handles storage, indexing, and semantic search:

```mermaid
graph LR
    A[Agent] -->|"store"| B[SpoonMem0]
    B -->|"embed & index"| C[Mem0 Cloud]
    A -->|"search"| B
    B -->|"semantic query"| C
    C -->|"relevant memories"| B
    B -->|"results"| A
```

| Feature | How It Helps |
|---------|--------------|
| **Semantic search** | Find memories by meaning: "user preferences" finds "I like dark mode" |
| **Auto-scoping** | Memories are isolated per user/agent automatically |
| **Graceful fallback** | If Mem0 is down, operations return empty (no crashes) |

## What Can You Store?

| Memory Type | Example |
|-------------|---------|
| **Preferences** | "User prefers concise responses" |
| **Facts** | "User's portfolio includes BTC and ETH" |
| **Context** | "User is a day trader focused on meme coins" |
| **History** | "User asked about Solana DeFi protocols last week" |

---

## Quick Start

```bash
pip install spoon-ai mem0ai
export MEM0_API_KEY="your-mem0-key"
```

```python
from spoon_ai.memory.mem0_client import SpoonMem0

mem0 = SpoonMem0({"user_id": "user_123"})

# Store and search
mem0.add_text("User prefers dark mode")
results = mem0.search("UI preferences")
print(results)
```

---

**Core class:** `spoon_ai.memory.mem0_client.SpoonMem0`

### Initialization

```python
from spoon_ai.memory.mem0_client import SpoonMem0

mem0 = SpoonMem0({
    "api_key": "YOUR_MEM0_API_KEY",   # or MEM0_API_KEY env var
    "user_id": "user_123",            # scope all operations to this user
    "collection": "my_namespace",     # optional namespace isolation
    "metadata": {"project": "demo"},  # auto-attached to writes
    "filters": {"project": "demo"},   # auto-applied to queries
    "async_mode": False,              # sync writes (default)
})

if not mem0.is_ready():
    print("Mem0 service unavailable")
```

### Add Memory

Store conversation history or individual text:

```python
# Add conversation messages
mem0.add_memory([
    {"role": "user", "content": "I love Solana meme coins"},
    {"role": "assistant", "content": "Got it, focusing on Solana"},
], user_id="user_123")

# Add single text (shorthand)
mem0.add_text("User prefers low gas fees")
```

Async variant: `await mem0.aadd_memory(messages, user_id=...)`

### Search memory
```python
results = mem0.search_memory(
    "Solana meme coins high risk",
    user_id="user_123",
    limit=5,
)
for r in results:  # results is a list of strings extracted from Mem0 responses
    print("-", r)
```

Async variant: `await mem0.asearch_memory(query, user_id=...)`

### Get all memory
```python
all_memories = mem0.get_all_memory(user_id="user_123", limit=20)  # returns [] if client is not ready or call fails
```

## Demo: Intelligent Web3 Portfolio Assistant
Path: `examples/mem0_agent_demo.py`

Key idea: The agent (ChatBot) is configured with Mem0 so it can recall user preferences after restart.

```python
from spoon_ai.chat import ChatBot

USER_ID = "crypto_whale_001"
SYSTEM_PROMPT = "...portfolio assistant..."

mem0_config = {
    "user_id": USER_ID,
    "metadata": {"project": "web3-portfolio-assistant"},
    "async_mode": False,  # sync writes so next query sees the data
}

# Create an LLM with long-term memory enabled
llm = ChatBot(
    llm_provider="openrouter",
    model_name="openai/gpt-5.1",
    enable_long_term_memory=True,
    mem0_config=mem0_config,
)
```

Flow:
1) **Session 1** â€“ capture preferences: user says they are a high-risk Solana meme trader; model replies; Mem0 stores the interaction.
2) **Session 2** â€“ reload a fresh ChatBot with the same `mem0_config`; the agent recalls past preferences (via Mem0 search) before answering.
3) **Session 3** â€“ user pivots to safe Arbitrum yield; new info is stored; subsequent queries reflect updated preferences.

Run the demo:
```bash
python examples/mem0_agent_demo.py
```

## Notes & Best Practices
- Always set `MEM0_API_KEY` or pass `api_key` in `mem0_config`.
- Use a stable `user_id` (or `agent_id`) so memories stay scoped; include `collection`/`filters` if you want stricter isolation. The wrapper injects `user_id` into filters and metadata if missing.
- Keep `async_mode=False` during demos/tests to avoid read-after-write delays; the wrapper always uses `mem0_config.get("async_mode", False)` for adds (no per-call override).
- Handle absence gracefully: `SpoonMem0.is_ready()` lets you disable LTM if Mem0 isnâ€™t installed or configured; helpers will otherwise return empty results when the client is unavailable.

---

FILE: docs/core-concepts/mcp-protocol.md

# MCP Protocol

The **Model Context Protocol (MCP)** is an open standard for connecting AI agents to external tools and data sources. Instead of hardcoding tool integrations, agents discover tools dynamically at runtimeâ€”enabling modular, federated ecosystems where tools can be shared across different AI applications.

## Why MCP?

Traditional tool integration is brittle:

```text
âŒ Old way: Agent â†” Hardcoded Tool A â†” Hardcoded Tool B â†” Hardcoded Tool C
âœ… MCP way: Agent â†” MCP Client â†” Any MCP Server (tools discovered at runtime)
```

With MCP, your agent can:

- **Discover tools dynamically** â€” No code changes when tools are added or updated
- **Connect to any MCP server** â€” Use tools from Cursor, Claude Desktop, or custom servers
- **Share tools across apps** â€” One MCP server can serve multiple agents
- **Hot-reload** â€” Update tool definitions without redeploying

## How It Works

```mermaid
sequenceDiagram
    participant Agent
    participant MCP Client
    participant MCP Server
    participant External API

    Agent->>MCP Client: Connect to server
    MCP Client->>MCP Server: list_tools()
    MCP Server-->>MCP Client: [tool schemas]
    Agent->>MCP Client: call_tool("search", {query: "..."})
    MCP Client->>MCP Server: Execute tool
    MCP Server->>External API: API call
    External API-->>MCP Server: Response
    MCP Server-->>MCP Client: Result
    MCP Client-->>Agent: Tool output
```

| Concept | Description |
|---------|-------------|
| **MCP Server** | Exposes tools via a standard protocol. Can run as subprocess (stdio), HTTP/SSE, or WebSocket. |
| **MCP Client** | Connects to servers, discovers tools, and executes them on behalf of agents. |
| **Tool Schema** | JSON-schema definition of tool name, description, and parametersâ€”fetched at runtime. |
| **Resources** | Optional: MCP also supports resource URIs for documents, databases, and other data. |

## MCP vs Other Approaches

| Aspect | MCP | OpenAI Plugins | Hardcoded Tools |
|--------|-----|----------------|-----------------|
| **Discovery** | Runtime `list_tools()` | Manifest file at URL | Compile-time |
| **Transport** | stdio, SSE, WebSocket | HTTPS only | In-process |
| **Ecosystem** | Cursor, Claude, SpoonOS, etc. | ChatGPT only | Single app |
| **Updates** | Hot-reload, no redeploy | Redeploy plugin | Redeploy app |

---

## Quick Start

```bash
pip install spoon-ai
```

### Using MCP Tools with an Agent

The recommended way to use MCP tools is through `SpoonReactMCP`:

```python
"""
DeepWiki MCP Agent Demo - demonstrates how to use MCP tools with SpoonReactMCP agent.
"""
import asyncio
from spoon_ai.agents.spoon_react_mcp import SpoonReactMCP
from spoon_ai.tools.mcp_tool import MCPTool
from spoon_ai.tools.tool_manager import ToolManager
from spoon_ai.chat import ChatBot

class DeepWikiAgent(SpoonReactMCP):
    """Agent that can analyze GitHub repositories via DeepWiki MCP"""
    name: str = "DeepWikiAgent"
    system_prompt: str = """You are a helpful assistant that can analyze GitHub repositories using DeepWiki.
When asked about a repository, use the read_wiki_structure tool with the repoName parameter.
For example, if asked about "XSpoonAi/spoon-core", call read_wiki_structure with repoName="XSpoonAi/spoon-core".
Always use the tool to get information about repositories before answering questions."""

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.available_tools = ToolManager([])

    async def initialize(self):
        # Create MCP tool for DeepWiki (no API key needed!)
        # Important: Use the actual tool name from the server: "read_wiki_structure"
        deepwiki_tool = MCPTool(
            name="read_wiki_structure",  # Use actual tool name, not "deepwiki"
            description="Analyze GitHub repositories and get their structure and documentation",
            mcp_config={
                "url": "https://mcp.deepwiki.com/sse",
                "transport": "sse",
                "timeout": 30,
            }
        )
        # Pre-load parameters so LLM can see the correct schema
        await deepwiki_tool.ensure_parameters_loaded()
        self.available_tools = ToolManager([deepwiki_tool])

async def main():
    # Create and initialize agent
    agent = DeepWikiAgent(
        llm=ChatBot(
            llm_provider="openai", 
            model_name="gpt-5.1-chat-latest"  
        )
    )
    
    print("Initializing agent and loading MCP tool...")
    await agent.initialize()
    # Query the agent with a clear request
    query = "What is XSpoonAi/spoon-core about? Please analyze the repository and summarize its purpose."
    response = await agent.run(query)
    print("\n Response:")
    print(response)

if __name__ == "__main__":
    asyncio.run(main())
```

### Direct MCP Tool Usage

For direct MCP tool calls without an agent:

```python
import asyncio
from spoon_ai.tools.mcp_tool import MCPTool

# Connect to an SSE/HTTP MCP server
mcp_tool = MCPTool(
    name="read_wiki_structure",
    description="DeepWiki MCP tool for repository analysis",
    mcp_config={
        "url": "https://mcp.deepwiki.com/sse",
        "transport": "sse",
        "timeout": 30,
        "headers": {"User-Agent": "SpoonOS-MCP/1.0"}
    }
)

async def main():
    # Tool parameters are loaded lazily when first used
    await mcp_tool.ensure_parameters_loaded()

    # Call the tool
    result = await mcp_tool.execute(repo="XSpoonAi/spoon-core")
    print(result)

asyncio.run(main())
```

---

## Architecture

```mermaid
graph TD
    A[Agent] --> B[MCP Client]
    B --> C[MCP Server]
    C --> D[Tool 1]
    C --> E[Tool 2]
    C --> F[Tool N]

    B --> G[Tool Discovery]
    B --> H[Tool Execution]
    B --> I[Resource Access]
```

### MCP Components

1. **MCP Server** - Hosts tools and resources
2. **MCP Client** - Connects agents to servers
3. **Tools** - Executable functions with defined schemas
4. **Resources** - Data sources and content

## Connecting to MCP Servers (client-only)

The cookbook focuses on MCP **clients**. Use `MCPTool` to connect to any MCP server (stdio/HTTP/SSE/WS). Hosting servers is out of scope hereâ€”follow your chosen serverâ€™s docs.

### MCP Client Configuration

SpoonOS supports multiple MCP transport types:

#### SSE/HTTP Transport (Remote Servers)

```python
from spoon_ai.tools.mcp_tool import MCPTool

# SSE transport (Server-Sent Events)
sse_tool = MCPTool(
    name="read_wiki_structure",
    description="DeepWiki MCP tool for repository analysis",
    mcp_config={
        "url": "https://mcp.deepwiki.com/sse",
        "transport": "sse",
        "timeout": 30,
        "headers": {"User-Agent": "SpoonOS-MCP/1.0"}
    }
)

# HTTP transport (Streamable HTTP)
http_tool = MCPTool(
    name="read_wiki_structure",
    description="DeepWiki HTTP MCP tool",
    mcp_config={
        "url": "https://mcp.deepwiki.com/mcp",
        "transport": "http",
        "timeout": 30,
        "headers": {"Accept": "application/json"}
    }
)
```

#### Stdio Transport (CLI Tools via npx/uvx)

```python
import os
from spoon_ai.tools.mcp_tool import MCPTool

# NPX transport (Node.js MCP servers)
tavily_tool = MCPTool(
    name="tavily-search",
    description="Web search via Tavily",
    mcp_config={
        "command": "npx",
        "args": ["--yes", "tavily-mcp"],
        "env": {"TAVILY_API_KEY": os.getenv("TAVILY_API_KEY")}
    }
)

# UVX transport (Python MCP servers)
python_tool = MCPTool(
    name="python-mcp",
    description="Python MCP server",
    mcp_config={
        "command": "uvx",
        "args": ["my-python-mcp-server"],
        "env": {}
    }
)

# python transport (Python MCP servers)
python_tool = MCPTool(
    name="python-mcp",
    description="Python MCP server",
    mcp_config={
        "command": "python",
        "args": ["my-python-mcp-server"],
        "env": {}
    }
)
```

#### WebSocket Transport

```python
from spoon_ai.tools.mcp_tool import MCPTool

ws_tool = MCPTool(
    name="ws-mcp",
    description="WebSocket MCP server",
    mcp_config={
        "url": "ws://localhost:8765",  # or wss:// for secure
    }
)
```

## Tool Discovery

### Automatic Discovery

```python
mcp_tool = MCPTool(
    name="discover_tools",  # Temporary name, will be replaced when we discover actual tools
    description="Tool to discover available MCP tools",
    mcp_config={
        "url": "https://mcp.deepwiki.com/sse",
        "transport": "sse",
        "timeout": 30,
    }
)

tools = await mcp_tool.list_available_tools()
for tool in tools:
    tool_name = tool.get('name')
    tool_desc = tool.get('description')
          
```

### Tool Registration

Use `MCPTool` to connect to any MCP server (stdio/HTTP/SSE/WS). No `spoon_cli` imports are needed in cookbook examples.

## Tool Execution

### Direct Execution

```python
# Execute tool 
result = await mcp_tool.call_mcp_tool("get_weather", location="New York"))
result2 = await mcp_tool.execute("get_weather", location="New York"))

print(result)
```

### Agent-Driven Execution

```python
import asyncio
import os
from spoon_ai.agents.spoon_react_mcp import SpoonReactMCP
from spoon_ai.tools.mcp_tool import MCPTool
from spoon_ai.tools.tool_manager import ToolManager
from spoon_ai.chat import ChatBot

class MyMCPAgent(SpoonReactMCP):
    """Custom agent with MCP tools"""
    name: str = "MyMCPAgent"
    system_prompt: str = "You are a helpful assistant with web search capabilities."

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.available_tools = ToolManager([])

    async def initialize(self):
        """Initialize MCP tools"""
        tavily_tool = MCPTool(
            name="tavily-search",
            description="Web search via Tavily",
            mcp_config={
                "command": "npx",
                "args": ["--yes", "tavily-mcp"],
                "env": {"TAVILY_API_KEY": os.getenv("TAVILY_API_KEY")}
            }
        )
        self.available_tools = ToolManager([tavily_tool])

async def main():
    agent = MyMCPAgent(llm=ChatBot(llm_provider="openai", model_name="gpt-5.1-chat-latest"))
    await agent.initialize()

    response = await agent.run("Search for the latest cryptocurrency news")
    print(response)

asyncio.run(main())
```

## Security Considerations

### Authentication

```python
# Use headers for authentication
external_api = MCPTool(
    name="external_api_tool",  # Tool name from the server
    description="External API with Bearer token",
    mcp_config={
        "url": "https://api.example.com/mcp",
        "transport": "http",
        "timeout": 30,
        # Bearer token via headers (not auth config)
        "headers": {
            "Authorization": f"Bearer {os.getenv('MCP_API_TOKEN', 'your_token_here')}",
        }
    }
)
```

### Tool Permissions

```python
# Define tool permissions
class RestrictedTool(BaseTool):
    required_permissions = ["read_data", "write_files"]

    async def execute(self, **kwargs):
        # Check permissions before execution
        if not self.check_permissions():
            raise PermissionError("Insufficient permissions")

        return await self.perform_action(**kwargs)
```

### Input Validation

```python
# Validate tool inputs
class SecureTool(BaseTool):
    async def execute(self, user_input: str) -> str:
        # Sanitize input
        clean_input = self.sanitize_input(user_input)

        # Validate against schema
        if not self.validate_input(clean_input):
            raise ValueError("Invalid input")

        return await self.process(clean_input)
```

## Performance Optimization

### Connection Pooling

> **Note:** `MCPConnectionPool` is not provided by `spoon_ai`. The `MCPClientMixin` already reuses sessions per task; wrap it or your FastMCP client in your own pooling logic if you need cross-server pooling.

```python
If you really need direct control (usually not necessary):

```python
from spoon_ai.agents.mcp_client_mixin import MCPClientMixin
from fastmcp.client.transports import SSETransport

# Create transport object (not string!)
transport = SSETransport(url="https://mcp.example.com/sse")
client = MCPClientMixin(transport)

# MCPClientMixin already pools sessions per task
async with client.get_session() as session:
    tools = await session.list_tools()
    result = await session.call_tool("tool_name", arguments={"param": "value"})

# Sessions are automatically reused within the same task
async with client.get_session() as session:  # Reuses existing session
    tools2 = await session.list_tools()
```

### Caching

> **Note:** `MCPCache` is not included in `spoon_ai`. Use a simple in-memory cache or a library like `functools.lru_cache` for discovery results.

```python
# Minimal in-memory cache for tool discovery
tool_cache: dict[str, list] = {}

async def get_tools_cached():
    if "tools" not in tool_cache:
        tool_cache["tools"] = await mcp_tool.list_available_tools()
    return tool_cache["tools"]
```

### Async Operations

```python
# Execute multiple tools concurrently
import asyncio

async def parallel_execution():
    tasks = [
        mcp_tools.execute_tool("tool1", {"param": "value1"}),
        mcp_tools.execute_tool("tool2", {"param": "value2"}),
        mcp_tools.execute_tool("tool3", {"param": "value3"})
    ]

    results = await asyncio.gather(*tasks)
    return results
```

## Common Use Cases

### API Integration

```python
# Integrate external APIs through MCP
class APITool(BaseTool):
    name = "api_call"

    async def execute(self, endpoint: str, method: str = "GET") -> dict:
        async with aiohttp.ClientSession() as session:
            async with session.request(method, endpoint) as response:
                return await response.json()
```

### Database Access

```python
# Database operations through MCP
class DatabaseTool(BaseTool):
    name = "query_database"

    async def execute(self, query: str) -> list:
        # Execute database query
        return await self.db.execute(query)
```

### File Operations

```python
# File system operations
class FileTool(BaseTool):
    name = "read_file"

    async def execute(self, filepath: str) -> str:
        with open(filepath, 'r') as f:
            return f.read()
```

## Best Practices

### Tool Design

- **Clear naming** - Use descriptive tool names
- **Comprehensive schemas** - Define complete parameter schemas
- **Error handling** - Leverage framework's automatic error handling
- **Documentation** - Provide clear descriptions and examples

### Performance

- **Connection reuse** - Reuse MCP connections when possible
- **Caching** - Cache discovery results and frequently used data
- **Timeouts** - Set appropriate timeouts for tool execution

### Security

- **Input validation** - Always validate tool inputs
- **Authentication** - Implement proper authentication mechanisms
- **Permissions** - Use least-privilege access principles

### Error Handling Philosophy

The SpoonOS framework follows a "fail-fast, recover-gracefully" approach for MCP operations:

- **Automatic Recovery**: Connection failures, timeouts, and server errors are handled automatically
- **Graceful Degradation**: When tools are unavailable, the system provides meaningful fallbacks
- **Minimal Manual Handling**: Let the framework handle errors; only intervene for custom business logic

```python
# Preferred: Let framework handle MCP errors
result = await mcp_tools.execute_tool("weather_tool", {"location": "NYC"})

# Framework automatically handles:
# - Server connection issues
# - Tool discovery failures
# - Execution timeouts
# - Parameter validation errors
```

## Troubleshooting

### Common Issues

#### Connection Errors

The framework automatically handles connection failures with built-in retry mechanisms:

```python
# Framework handles connection failures automatically
await mcp_client.connect()  # Automatic retry with exponential backoff
```

#### Tool Discovery Failures

```python
# Framework provides graceful handling of discovery issues
tools = await mcp_tools.discover_tools()
# Automatic fallback to cached tools if server unavailable
```

#### Execution Timeouts

```python
# Framework manages timeouts automatically
result = await mcp_tools.execute_tool("slow_tool", {})
# Automatic timeout handling with configurable limits
```

## Next Steps

### ðŸ“š **MCP Implementation Examples**

#### ðŸ” [MCP Spoon Search Agent](../examples/mcp-spoon-search-agent.md)
**GitHub**: [View Source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/mcp/spoon_search_agent.py)

**What it demonstrates:**
- Complete MCP server integration and tool discovery
- Real-world MCP implementation with Tavily web search
- Dynamic tool loading and orchestration
- Production-ready MCP error handling and recovery

**Key features:**
- Tavily MCP server integration via `npx tavily-mcp`
- Automatic tool discovery and validation
- Seamless integration with existing SpoonOS architecture
- Advanced error handling for MCP server failures

**Learning outcomes:**
- How to initialize and manage MCP servers
- Dynamic tool discovery patterns
- MCP server error handling and recovery
- Integration of MCP tools with LLM workflows

### ðŸ› ï¸ **Development Resources**

- **[Tools System](./tools.md)** - Learn about the complete tool ecosystem
- **[Custom Tool Development](../how-to-guides/add-custom-tools.md)** - Build MCP-compatible tools
- **[MCP Tool Reference](../api-reference/spoon_ai/tools/)** - MCP-specific tool documentation

### ðŸ“– **Additional Resources**

- **[Graph System](../graph-system/index.md)** - Advanced workflow orchestration
- **[Agent Architecture](../core-concepts/agents.md)** - Agent-MCP integration patterns
- **[API Reference](../api-reference/index)** - Complete SpoonOS API documentation
**GitHub**: [View Source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/mcp/spoon_search_agent.py)

**What it demonstrates:**
- Complete MCP server integration and tool discovery
- Real-world MCP implementation with Tavily web search
- Dynamic tool loading and orchestration
- Production-ready MCP error handling and recovery

**Key features:**
- Tavily MCP server integration via `npx tavily-mcp`
- Automatic tool discovery and validation
- Seamless integration with existing SpoonOS architecture
- Advanced error handling for MCP server failures

**Learning outcomes:**
- How to initialize and manage MCP servers
- Dynamic tool discovery patterns
- MCP server error handling and recovery
- Integration of MCP tools with LLM workflows

### ðŸ› ï¸ **Development Resources**

- **[Tools System](./tools.md)** - Learn about the complete tool ecosystem
- **[Custom Tool Development](../how-to-guides/add-custom-tools.md)** - Build MCP-compatible tools
- **[MCP Tool Reference](../api-reference/spoon_ai/tools/)** - MCP-specific tool documentation

### ðŸ“– **Additional Resources**

- **[Graph System](../graph-system/index.md)** - Advanced workflow orchestration
- **[Agent Architecture](../core-concepts/agents.md)** - Agent-MCP integration patterns
- **[API Reference](../api-reference/index)** - Complete SpoonOS API documentation

---

FILE: docs/core-concepts/Short-term memory.md

# Short-Term Memory

Short-term memory keeps track of the **current conversation**. It's what lets your agent remember "My name is Alice" three messages ago, and what prevents context windows from overflowing in long conversations.

## The Problem

LLMs have limited context windows. As conversations grow:

```text
Turn 1:  User: "My name is Alice"          â† 10 tokens
Turn 50: User: "What's my name again?"     â† 50,000 tokens total
         Agent: "I don't know" â† context overflow, lost earlier messages
```

Naive solutions have tradeoffs:

| Approach | Problem |
|----------|---------|
| **Keep everything** | Exceeds context window, costs explode |
| **Drop old messages** | Loses important context ("My name is Alice") |
| **Fixed sliding window** | Arbitrary cutoff, may drop critical info |

## SpoonOS Solution

`ShortTermMemoryManager` intelligently manages context:

```mermaid
graph LR
    A[New Message] --> B{Token Budget?}
    B -->|Under limit| C[Add to history]
    B -->|Over limit| D[Trim Strategy]
    D --> E[Summarize old messages]
    E --> F[Keep summary + recent]
    F --> C
```

| Feature | What It Does |
|---------|--------------|
| **Token-aware** | Tracks actual token count, not message count |
| **Smart trimming** | Multiple strategies: oldest first, from start, from end |
| **Summarization** | Condenses old messages into a summary when needed |
| **Built-in** | `ChatBot` handles this automaticallyâ€”no extra code |

## When To Use

- **Chatbots** with multi-turn conversations
- **Agents** that need to remember earlier context
- **Long sessions** that would exceed context limits
- **Cost optimization** to reduce token usage

---

## Quick Start

```bash
pip install spoon-ai
export OPENAI_API_KEY="your-key"
```

```python
import asyncio
from spoon_ai.chat import ChatBot

# ChatBot includes built-in short-term memory with auto-trimming
llm = ChatBot(model_name="gpt-5.1-chat-latest", llm_provider="openai")

async def main():
    await llm.ask([{"role": "user", "content": "My name is Alice"}])
    await llm.ask([{"role": "user", "content": "What's the capital of France?"}])
    response = await llm.ask([{"role": "user", "content": "What's my name?"}])  # Remembers "Alice"
    print(response)

asyncio.run(main())
```

---

## ShortTermMemoryManager

For fine-grained control beyond `ChatBot`'s automatic handling, use `ShortTermMemoryManager` directly:

```python
import asyncio
from spoon_ai.memory.short_term_manager import ShortTermMemoryManager, TrimStrategy
from spoon_ai.schema import Message

manager = ShortTermMemoryManager()
history = [
    Message(id="u1", role="user", content="Hello!"),
    Message(id="a1", role="assistant", content="Hi there â€” how can I help?"),
    Message(id="u2", role="user", content="What's DeFi?"),
]


#  Trim the message list by token budget
trimmed = asyncio.run(
    manager.trim_messages(
        messages=history,
        max_tokens=48,
        strategy=TrimStrategy.FROM_END,
        keep_system=True,
    )
)

#  Summarize history before model call
llm_ready, removals, summary = asyncio.run(
    manager.summarize_messages(
        messages=history,
        max_tokens_before_summary=48,
        messages_to_keep=2,
        summary_model="anthropic/claude-3.5-sonnet",
        llm_manager=chatbot.llm_manager,
        llm_provider=chatbot.llm_provider,
        existing_summary=chatbot.latest_summary() or "",
    )
)
```

 `llm_ready` â€” condensed history you can pass to the LLM
 `removals` â€” list of RemoveMessage directives:apply removals to your persisted history using spoon_ai.graph.reducers.add_messages.

Note: both `summarize_messages()` and `ChatBot.ask()` invoke your configured LLM. Ensure `chatbot.llm_manager`/`chatbot.llm_provider` (and any required API keys or env vars) are set so these examples can run endâ€‘toâ€‘end.

```python
from spoon_ai.chat import ChatBot
from spoon_ai.graph.reducers import add_messages

chatbot = ChatBot(enable_short_term_memory=True)
history = [
    Message(id="u1", role="user", content="Hello!"),
    Message(id="a1", role="assistant", content="Hi there â€” how can I help?"),
    Message(id="u2", role="user", content="What's DeFi?"),
]
# Remove the latest assistant message
assistant_ids = [msg.id for msg in history if msg.role == "assistant"]
remove_last = chatbot.remove_message(assistant_ids[-1])

# Or clear the entire history
remove_all = chatbot.remove_all_messages()

# Apply directives to persisted history
updated_history = add_messages(history, [remove_last])
cleared_history = add_messages(history, [remove_all])
```

`add_messages()` merges the removal directives into the existing history,
deleting targeted entries (or the entire transcript).
This mirrors how the short-term memory manager emits `RemoveMessage` items
when summarization trims older turns.

---

## Inspecting Thread State and Checkpoints

every time the graph runs, you can retrieve the latest snapshot (messages plus metadata), iterate the full checkpoint history, or read a `CheckpointTuple` for an external consumer. This makes it easy to debug memory behaviour, replay from any checkpoint, or sync state to persistent storage. The example below shows how to fetch the most recent summary, list all checkpoints, and view the tuple-style payload.

```python
config = {"configurable": {"thread_id": "memory_demo_thread"}}

snapshot = graph.get_state(config)
print("Latest checkpoint:", snapshot.metadata.get("checkpoint_id"))

for snap in graph.get_state_history(config):
    print("History id:", snap.metadata.get("checkpoint_id"))

checkpoint_tuple = graph.checkpointer.get_checkpoint_tuple(config)
print("Checkpoint tuple:", checkpoint_tuple)

for entry in graph.checkpointer.iter_checkpoint_history(config):
    print("Tuple history entry:", entry)
```

If you are using a compiled graph (`CompiledGraph`), call `graph.graph.get_state(config)` and `graph.graph.get_state_history(config)` instead; the snippet above assumes `graph` is a `StateGraph`.

---

FILE: docs/core-concepts/tools.md

# Tools

Tools are **callable capabilities** that let agents interact with the outside worldâ€”APIs, databases, blockchains, file systems, and any other external service. Without tools, an LLM can only generate text. With tools, it can take action.

## Why Tools?

An LLM doesn't know today's Bitcoin price, can't send emails, and has no way to query your database. Tools bridge this gap:

```mermaid
graph LR
    A[Agent] -->|"call tool"| B[Tool: get_price]
    B -->|"API call"| C[Binance API]
    C -->|"$67,432"| B
    B -->|"return"| A
    A -->|"Bitcoin is $67,432"| D[User]
```

SpoonOS tools are:

- **Typed** â€” JSON-schema parameters prevent LLM hallucination of invalid inputs
- **Validated** â€” Runtime checks ensure data integrity before execution
- **Async** â€” Non-blocking I/O for high-performance agent loops
- **Composable** â€” Bundle tools into toolkits, share via MCP protocol

## Tool Anatomy

Every SpoonOS tool has three parts:

| Part | Purpose | Example |
|------|---------|---------|
| **name** | Unique identifier the LLM uses to call the tool | `"get_crypto_price"` |
| **description** | Natural language explanation of what the tool does | `"Get real-time price for a cryptocurrency"` |
| **parameters** | JSON-schema defining expected inputs | `{"symbol": {"type": "string"}}` |

The LLM reads the description to decide *when* to use a tool and the parameters to know *how* to call it.

## What Can You Build?

| Tool Type | Examples |
|-----------|----------|
| **Data retrieval** | Web search, database queries, API calls |
| **Crypto/Web3** | CEX trading, DEX swaps, on-chain reads, wallet operations |
| **File operations** | Read/write files, parse documents, generate reports |
| **Communication** | Send emails, post to Slack, create tickets |
| **Computation** | Run code, execute SQL, perform calculations |

## SpoonOS vs Other Tool Systems

| Feature | SpoonOS | LangChain | OpenAI Functions |
|---------|---------|-----------|------------------|
| **Definition** | `BaseTool` class | `@tool` decorator | JSON in API call |
| **Validation** | JSON-schema + runtime | Optional Pydantic | Server-side only |
| **Remote tools** | MCP protocol (stdio/SSE/WS) | API wrappers | N/A |
| **Discovery** | `ToolManager` + semantic search | `load_tools()` | Manual |
| **Crypto native** | Built-in CEX/DEX/on-chain | Third-party | N/A |

---

## Quick Start

```bash
pip install spoon-ai-sdk
```

```python
import asyncio
from spoon_ai.tools.base import BaseTool
from spoon_ai.tools import ToolManager

# Define a tool with JSON-schema parameters
class GreetTool(BaseTool):
    name: str = "greet"
    description: str = "Greet someone by name"
    parameters: dict = {
        "type": "object",
        "properties": {"name": {"type": "string"}},
        "required": ["name"]
    }

    async def execute(self, name: str) -> str:
        return f"Hello, {name}!"

# Register and execute
manager = ToolManager([GreetTool()])

async def main():
    result = await manager.execute(name="greet", tool_input={"name": "World"})
    print(result)  # Hello, World!

asyncio.run(main())
```

---

## Tool Types

### Local Tools (`BaseTool`)

All tools inherit from `BaseTool` with three required attributes and one method:

```python
from spoon_ai.tools.base import BaseTool

class MyTool(BaseTool):
    name: str = "my_tool"                    # Unique identifier
    description: str = "What this tool does" # LLM reads this to decide when to use it
    parameters: dict = {                      # JSON-schema for input validation
        "type": "object",
        "properties": {
            "arg1": {"type": "string", "description": "First argument"},
            "arg2": {"type": "integer", "default": 10}
        },
        "required": ["arg1"]
    }

    async def execute(self, arg1: str, arg2: int = 10) -> str:
        return f"Result: {arg1}, {arg2}"
```

The `__call__` method forwards to `execute()`, so `await tool(arg1="value")` works.

### ToolManager

Orchestrates tool registration, lookup, and execution:

```python
from spoon_ai.tools import ToolManager

manager = ToolManager([MyTool(), AnotherTool()])

# Execute by name
result = await manager.execute(name="my_tool", tool_input={"arg1": "hello"})

# Get tool specs for LLM function calling
specs = manager.to_params()  # List of OpenAI-compatible tool definitions
```

**Key methods:**

- `add_tool(tool)` / `add_tools([...])` â€” Register tools
- `remove_tool(name)` â€” Unregister by name
- `get_tool(name)` â€” Retrieve tool instance
- `to_params()` â€” Export OpenAI-compatible tool definitions
- `index_tools()` / `query_tools(query)` â€” Semantic search (requires Pinecone + OpenAI)

### Crypto toolkit (optional)

If you install `spoon-toolkits`, import the concrete tools you need:

```python
from spoon_toolkits import CryptoPowerDataPriceTool, CryptoPowerDataCEXTool
from spoon_ai.tools import ToolManager

crypto_tools = [
    CryptoPowerDataPriceTool(),
    CryptoPowerDataCEXTool(),
]
manager = ToolManager(crypto_tools)
```

Environment variables for these tools depend on the specific provider (e.g., `OKX_API_KEY`, `BITQUERY_API_KEY`, `RPC_URL`, etc.).

### MCP client tools (`MCPTool`)

`MCPTool` lets an agent call tools hosted on an MCP server.

```python
from spoon_ai.tools.mcp_tool import MCPTool

mcp_tool = MCPTool(
    mcp_config={
        "url": "http://localhost:8765",      # or ws://..., or command/args for stdio
        "transport": "sse",                  # optional: "sse" (default) | "http"
        "timeout": 30,
        "max_retries": 3,
    }
)
# The toolâ€™s schema/description is fetched dynamically from the MCP server.
```

`MCPTool.execute(...)` will fetch the serverâ€™s tool list, align the name/parameters, and perform retries and health checks.

### MCP clients

SpoonOS agents primarily use `MCPTool` (MCP client) to talk to remote MCP servers:

```python
from spoon_ai.tools.mcp_tool import MCPTool

# Example: connect to DeepWiki SSE MCP server
deepwiki = MCPTool(
    name="read_wiki_structure",  # Use the actual tool name from the server
    description="DeepWiki MCP tool for repository analysis",
    mcp_config={
        "url": "https://mcp.deepwiki.com/sse",
        "transport": "sse",
        "timeout": 30,
    },
)

async def main():
    # Pre-load parameters to get the correct schema
    print("Loading MCP tool parameters...")
    await deepwiki.ensure_parameters_loaded()
    
    # Use the correct parameter name: repoName (not repo)
    result = await deepwiki.execute(repoName="XSpoonAi/spoon-core")
    print(f"\nResult:\n{result}")

asyncio.run(main())
```

If you need to self-host an MCP server, follow that serverâ€™s own documentation; the cookbook focuses on the `spoon_ai` MCP client (`MCPTool`) rather than FastMCP server setup.

## Configuration

- **Core**: none required for basic tools.
- **Embedding index (optional)**: `OPENAI_API_KEY`, `PINECONE_API_KEY`.
- **Crypto/toolkit tools**: provider-specific keys (e.g., `OKX_API_KEY`, `BITQUERY_API_KEY`, `RPC_URL`, `GOPLUSLABS_API_KEY`).
- **MCP**: set transport target via `mcp_config` (`url` or `command` + `args`/`env`).

## Best Practices

- Keep tools single-purpose with clear `parameters` JSON schema.
- Validate inputs inside `execute`; raise rich errors for better agent feedback.
- Prefer async I/O in `execute` to avoid blocking the event loop.
- Reuse `ToolManager` for name-based dispatch and tool metadata generation.
- When using toolkit or MCP tools, fail gracefully if optional dependencies or servers are missing.

## See Also

- [API reference: Base Tool](../api-reference/spoon_ai/tools/base.md)
- [MCP protocol details](./mcp-protocol.md)
- [Custom tool guide](../how-to-guides/add-custom-tools.md)

---

FILE: docs/core-concepts/x402-payments.md

# x402 Payments

x402 enables **agents to pay for things autonomously**. When an agent hits a paywall (HTTP 402), it automatically signs a crypto payment, retries the request, and continuesâ€”no human intervention required. This creates a native monetization layer for AI services.

## Why x402?

Traditional payments don't work for autonomous agents:

| Problem | With Traditional Payments | With x402 |
|---------|---------------------------|-----------|
| Agent hits paywall | âŒ Wait for human to enter credit card | âœ… Auto-sign and retry |
| Micropayments ($0.001) | âŒ Fees exceed payment | âœ… Low-cost on L2s |
| Settlement | âŒ 1-3 days | âœ… Instant |
| Verification | âŒ Trust Stripe API | âœ… Cryptographic proof |

## How It Works

```mermaid
sequenceDiagram
    participant Agent
    participant API Server
    participant x402 Facilitator
    participant Blockchain

    Agent->>API Server: GET /premium-data
    API Server-->>Agent: 402 Payment Required (price, recipient)
    Agent->>Agent: Sign EIP-712 payment
    Agent->>API Server: GET /premium-data + PAYMENT-SIGNATURE (v2) / X-PAYMENT (v1)
    API Server->>x402 Facilitator: Verify signature
    x402 Facilitator->>Blockchain: Execute transfer
    Blockchain-->>x402 Facilitator: Confirmed
    x402 Facilitator-->>API Server: Valid
    API Server-->>Agent: 200 OK + data
```

| Step | What Happens |
|------|--------------|
| **1. Request** | Agent calls a paid API endpoint |
| **2. 402 Response** | Server returns payment requirements (v2 via `PAYMENT-REQUIRED`, legacy v1 via JSON body) |
| **3. Sign** | Agent signs an EIP-712 typed-data payload (no gas yet) |
| **4. Retry** | Agent sends request again with the signed payment header (`PAYMENT-SIGNATURE` for v2, `X-PAYMENT` for v1) |
| **5. Verify & Execute** | Facilitator verifies signature and executes transfer on-chain |
| **6. Success** | Server returns the requested data |

## x402 vs Alternatives

| Aspect | x402 | Stripe | Lightning |
|--------|------|--------|-----------|
| **Settlement** | Instant | 1-3 days | Instant |
| **Agent autonomy** | Auto-sign | Needs webhook | Manual channel |
| **Micropayments** | âœ… L2 fees | âŒ High fees | âœ… |
| **Verification** | Cryptographic | API call | Node verification |

---

## Quick Start

```bash
pip install spoon-ai x402
export PRIVATE_KEY="your-wallet-private-key"
export X402_RECEIVER_ADDRESS="0xYourReceiverWallet"
```

```python
import asyncio
from spoon_ai.payments import X402PaymentService, X402PaymentRequest

# Initialize the payment service
service = X402PaymentService()

async def main():
    # Create a payment request
    request = X402PaymentRequest(
        amount_usdc="0.01",  # Amount in USDC
        resource="/premium-data",
        description="Access to premium data"
    )

    # Sign and create payment receipt
    receipt = await service.sign_and_pay(request)
    print(f"Payment signed: {receipt}")

asyncio.run(main())
```

> **Note:** For agent-based x402 payments, the agent handles 402 responses automatically when configured with payment capabilities. See the full examples in the x402 package for complete integration patterns.

---

## Components

| Piece | Role inside SpoonOS |
| --- | --- |
| **x402 facilitator** | Public service (`https://x402.org/facilitator` by default) that verifies and settles signed payment payloads. |
| **Paywall server** | Your FastAPI router (`spoon_ai.payments.app`) that refuses unpaid requests with a 402 payload and forwards valid calls to agents. |
| **SpoonReact agent** | Issues HTTP probes, signs payments via tools, and stores payment receipts in memory. |
| **Signer** | Either the `PRIVATE_KEY` loaded in-process or a Turnkey identity configured via `TURNKEY_*` variables. |

## Configuration surfaces

Most deployments only need a `.env` entry and (optionally) config overrides:

```bash
X402_RECEIVER_ADDRESS=0xwallet-that-receives-fees
X402_FACILITATOR_URL=https://x402.org/facilitator
X402_DEFAULT_ASSET=
X402_DEFAULT_NETWORK=
X402_DEFAULT_SCHEME=exact
X402_DEFAULT_AMOUNT_USDC=
X402_PAYWALL_APP_NAME=SpoonOS Agent Services
X402_PAYWALL_APP_LOGO=https://your-domain.example/logo.png
X402_DEMO_URL=https://www.x402.org/protected
```

Key points:

- The system always prefers the local `PRIVATE_KEY`. If that variable is empty and Turnkey credentials (`TURNKEY_*`) exist, SpoonOS transparently switches to hosted signing.
- In CLI workflows (spoon-cli or the legacy `main.py` CLI), the `x402` block in the CLI `config.json` mirrors these defaults (branding, description, timeout, etc.). Update that file when you need per-environment variance. The core SDK still reads values from environment variables.
- Setting `X402_DEFAULT_ASSET` ensures all typed-data domains reference the real USDC contract so signatures pass facilitator validation.

## Runtime lifecycle

```mermaid
sequenceDiagram
    participant User
    participant Agent
    participant Paywall as Paywall Router (/x402)
    participant Facilitator

    User->>Agent: Task / query
    Agent->>Paywall: http_probe (unauthenticated)
    alt Paywall open
        Paywall-->>Agent: HTTP 200 (no payment)
        Agent-->>User: Return content / summary
    else Paywalled (HTTP 402)
        Paywall-->>Agent: 402 + requirements
        Agent->>Agent: Merge requirements + config overrides
        Agent->>Agent: Select signer (PRIVATE_KEY or Turnkey)
        Agent->>Agent: Build typed-data payload
        Agent->>Agent: Sign -> PAYMENT-SIGNATURE (v2) / X-PAYMENT (v1)
        Agent->>Paywall: Retry with signed payment header
        Paywall->>Facilitator: verify_payment (optional settle)
        Facilitator-->>Paywall: Valid? + receipt
        alt Invalid
            Paywall-->>Agent: 402 / error
        else Valid
            Paywall-->>Agent: 200 + PAYMENT-RESPONSE (v2) / X-PAYMENT-RESPONSE (v1)
            Agent->>Agent: Log receipt / update memory
            Agent-->>User: Protected content + summary
        end
    end
```

If the paid retry fails (for example `verify_payment` rejects the header or the facilitator reports an error), the paywall server immediately returns another `402` or error payload and the agent decides whether to run `x402_paywalled_request` again with corrected parameters. A successful verification moves straight into settlement and target agent execution, so there is no additional retry cycle once the payment header is accepted (`PAYMENT-SIGNATURE` in v2, `X-PAYMENT` in legacy v1).

## Operational checklist

1. Use [https://faucet.circle.com/](https://faucet.circle.com/) to mint 0.01 USDC for the public demo.
2. Keep `X402_RECEIVER_ADDRESS` aligned with the wallet that ultimately receives settlements.
3. Monitor facilitator responses. Any `invalid_exact_evm_payload_signature` errors typically mean the `asset`, `chainId`, or nonce encoding no longer matches the paywall challenge.
4. Use `X402PaymentService.decode_payment_response(header)` to archive payment receipts in logs or analytics pipelines.

---

FILE: docs/examples/graph-crypto-analysis.md

---
sidebar_position: 2
---

# Graph Crypto Analysis

This example implements a complete cryptocurrency research and analysis pipeline using the declarative graph building system, demonstrating end-to-end LLM-driven decision making for market analysis and investment recommendations.

#### ðŸ“Š **Workflow Diagram**

```mermaid
graph TD
    A[Start] --> B[Fetch Binance Market Data]
    B --> C[Select Top 10 Pairs by Volume]
    C --> D[Prepare Token List]

    D --> E[Parallel Token Analysis]
    E --> F1[Token 1: Technical + News Analysis]
    E --> F2[Token 2: Technical + News Analysis]
    E --> F3[Token 3: Technical + News Analysis]
    E --> F4[Token 4: Technical + News Analysis]
    E --> F5[Token 5: Technical + News Analysis]
    E --> F6[Token 6: Technical + News Analysis]
    E --> F7[Token 7: Technical + News Analysis]
    E --> F8[Token 8: Technical + News Analysis]
    E --> F9[Token 9: Technical + News Analysis]
    E --> F10[Token 10: Technical + News Analysis]

    F1 --> G[Aggregate All Results]
    F2 --> G
    F3 --> G
    F4 --> G
    F5 --> G
    F6 --> G
    F7 --> G
    F8 --> G
    F9 --> G
    F10 --> G

    G --> H[LLM Final Aggregation]
    H --> I[Generate Market Report]
    I --> J[END]

    style A fill:#e1f5fe
    style J fill:#c8e6c9
    style E fill:#fff3e0
    style G fill:#fce4ec

    subgraph "Technical Analysis"
        F1
        F2
        F3
        F4
        F5
        F6
        F7
        F8
        F9
        F10
    end
```

#### ðŸŽ¯ **Core Features**

**Intelligent Market Analysis:**
- LLM-driven token selection based on real-time market conditions
- Multi-timeframe analysis (1h, 4h) for comprehensive market view
- Dynamic decision flow guided by LLM analysis at each step

**Advanced Technical Analysis:**
- Real-time indicator calculation (RSI, MACD, EMA) using PowerData toolkit
- Market sentiment analysis and momentum evaluation
- Risk assessment and volatility metrics for each token

**LLM-Powered Synthesis:**
- Intelligent summarization of complex market data
- Data-driven investment recommendations with reasoning
- Short-term and macro-level market outlook generation

#### ðŸš€ **Key Capabilities**

- **Declarative Graph Building** - `GraphTemplate`, `NodeSpec`, `EdgeSpec` for modular workflows
- **High-Level API Integration** - `HighLevelGraphAPI` for automatic parameter inference
- **Complete Workflow** - End-to-end from data ingestion to final recommendations
- **Real API Integration** - Live Binance and cryptocurrency data via PowerData toolkit
- **LLM Decision Making** - Every major decision guided by LLM analysis
- **Advanced State Management** - Complex analysis state throughout the process
- **Error Recovery** - Robust error handling and fallback mechanisms

#### ðŸ“‹ **Prerequisites**

```bash
# Required environment variables
export OPENAI_API_KEY="your-openai-api-key"          # Primary LLM
export ANTHROPIC_API_KEY="your-anthropic-api-key"   # Alternative LLM
export TAVILY_API_KEY="your-tavily-api-key"       # Search engine
```

#### ðŸƒ **Quick Start**

```bash
# Navigate to examples directory
cd spoon-core/example

# Install dependencies
pip install -r requirements.txt

# Run the declarative crypto analysis
python graph_crypto_analysis.py
```

#### ðŸ” **What to Observe**

**Architecture:**
- How `GraphTemplate` and `NodeSpec` simplify workflow construction
- `HighLevelGraphAPI` automatically inferring parameters from queries
- Modular node implementations with better separation of concerns

**Data Flow:**
- Real market data fetching from Binance API and PowerData toolkit
- LLM analysis of raw data for intelligent decision making
- Step-by-step process from data collection to final recommendations

**Technical Analysis:**
- Real-time indicator calculation using PowerData toolkit
- Correlation of different data sources
- Market sentiment analysis and quantification

**LLM Decision Process:**
- Token evaluation and selection for analysis
- Synthesis combining technical and fundamental analysis
- Investment recommendations with detailed reasoning

#### ðŸ“Š **Sample Output**

```
ðŸ” MARKET ANALYSIS REPORT
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ðŸ“ˆ SELECTED TOKENS FOR ANALYSIS: BTC, ETH, SOL, ADA

ðŸ“Š TECHNICAL ANALYSIS:
â€¢ BTC/USDT: Bullish momentum, RSI: 68, MACD positive crossover
â€¢ ETH/USDT: Consolidation phase, approaching key resistance
â€¢ SOL/USDT: Strong uptrend, breaking previous highs
â€¢ ADA/USDT: Recovery phase, positive volume momentum

ðŸŽ¯ INVESTMENT RECOMMENDATIONS:
â€¢ SHORT-TERM: Consider BTC and SOL for momentum plays
â€¢ MEDIUM-TERM: Hold ETH through current consolidation
â€¢ RISK ASSESSMENT: Moderate volatility expected in next 24-48 hours

ðŸ’¡ MARKET OUTLOOK:
The current market shows strong bullish momentum with BTC leading...
```

#### ðŸ“ **Source Code**

- **Main Example**: [graph_crypto_analysis.py](https://github.com/XSpoonAi/spoon-core/blob/main/examples/graph_crypto_analysis.py)
- **Supporting Modules**:
  - `spoon_ai/graph/builder.py` - Declarative templates and high-level API
  - `toolkit/spoon_toolkits/crypto/crypto_powerdata/tools.py` - PowerData integration helpers
  - `spoon_ai/graph/` - Core engine and monitoring utilities
  - [Tool System Docs](../core-concepts/tools.md)

#### ðŸŽ“ **Learning Outcomes**

- Using declarative graph building (`GraphTemplate`, `NodeSpec`, `EdgeSpec`)
- Leveraging `HighLevelGraphAPI` for automatic parameter inference
- Implementing modular, maintainable node functions
- Building complete end-to-end analysis systems with LLM integration
- Advanced cryptocurrency market analysis techniques
- Real-time data processing and technical indicator calculation
- LLM-driven decision making in complex workflows
- Error handling and data validation in financial applications

#### ðŸ’¡ **Best Practices**

- Declarative architecture for improved modularity
- High-level API usage for automatic parameter inference
- Data validation and comprehensive error handling
- Performance optimization and efficient data processing
- Security considerations for API keys and financial data
- Modular architecture with clean separation of concerns

---

FILE: docs/examples/intent-graph-demo.md

---
sidebar_position: 1
---


# Intent Graph Demo

This example demonstrates an intelligent StateGraph workflow with advanced query routing, parallel execution, and memory management using the modern declarative graph building system.

#### ðŸ“Š **Workflow Diagram**

```mermaid
graph TD
    A[User Query] --> B[Bootstrap Session]
    B --> C[Load Memory]
    C --> D[Plan Analysis]
    D --> E{LLM Intent Analysis}
    E -->|general_qa| F[General Q&A]
    E -->|short_term_trend| G[Extract Symbol]
    E -->|macro_trend| H[Extract Symbol]
    E -->|deep_research| I[Deep Research Search]

    G --> J[Short-term Data Collection]
    H --> K[Macro Data Collection]
    I --> L[Research Sources]

    J --> M[Short-term Summary]
    K --> N[Macro Summary]
    L --> O[Research Report]

    M --> P[Review Trade]
    N --> P
    O --> P
    F --> P

    P --> Q[Update Memory]
    Q --> R[Finalize Response]
    R --> S[END]

    style A fill:#e1f5fe
    style S fill:#c8e6c9
    style E fill:#fff3e0
    style P fill:#fce4ec
```

#### ðŸŽ¯ **Core Features**

**Intelligent Query Routing:**
- LLM-powered intent classification into: `general_qa`, `short_term_trend`, `macro_trend`, or `deep_research`
- Dynamic routing based on detected intent and conversation history
- Context-aware decision making with market context

**Parallel Data Processing:**
- Concurrent data fetching across multiple timeframes (15m, 30m, 1h, 4h, daily, weekly)
- Real-time cryptocurrency data integration
- Performance optimization through parallel execution

**Advanced Memory Management:**
- Persistent conversation context across sessions
- Automatic storage of learned patterns and market insights
- State preservation for analysis results and routing decisions

#### ðŸš€ **Key Capabilities**

- **Declarative Graph Building** - `GraphTemplate`, `NodeSpec`, `EdgeSpec` for modular workflows
- **High-Level API Integration** - `HighLevelGraphAPI` for automatic parameter inference
- **LLM Integration** - Advanced prompt engineering and response processing
- **Tool Orchestration** - Multi-source data integration (PowerData, Tavily, EVM swap)
- **Error Handling** - Robust recovery with duplicate log prevention
- **Performance Monitoring** - Built-in metrics and execution tracking

#### ðŸ“‹ **Prerequisites**

```bash
# Required environment variables
export OPENAI_API_KEY="your-openai-api-key"
export TAVILY_API_KEY="your-tavily-api-key"       # Search engine
```

#### ðŸƒ **Quick Start**

```bash
# Navigate to examples directory
cd spoon-core/example

# Install dependencies
pip install -r requirements.txt

# Run the declarative intent graph demo
python intent_graph_demo.py
```

#### ðŸ” **What to Observe**

**Architecture:**
- How `GraphTemplate` and `NodeSpec` simplify workflow construction
- `HighLevelGraphAPI` automatically inferring parameters from queries
- Modular node implementations with better separation of concerns

**Execution Flow:**
- Intelligent routing to appropriate analysis paths based on query intent
- Parallel data fetching across multiple timeframes
- Memory loading and updates throughout the process

**Performance:**
- Execution times for different routing paths
- Parallel vs sequential processing performance
- Memory usage optimization and duplicate log prevention

**Advanced Behaviors:**
- LLM-powered routing decisions based on intent analysis
- Real-time data integration from multiple sources
- Context maintenance across complex workflows

#### ðŸ“ **Source Code**

- **Main Example**: [intent_graph_demo.py](https://github.com/XSpoonAi/spoon-core/blob/main/examples/intent_graph_demo.py)
- **Supporting Modules**:
  - `spoon_ai/graph/` - Core graph system and declarative builders
  - `spoon_ai/graph/builder.py` - High-level API and parameter inference
  - [Graph System Docs](../graph-system/index.md)

#### ðŸŽ“ **Learning Outcomes**

- Using declarative graph building (`GraphTemplate`, `NodeSpec`, `EdgeSpec`)
- Leveraging `HighLevelGraphAPI` for automatic parameter inference
- Implementing modular, maintainable node functions
- Advanced LLM integration and prompt engineering
- Parallel processing for performance optimization
- Memory management in long-running processes
- Error handling and recovery strategies

#### ðŸ’¡ **Best Practices**

- Declarative architecture for improved modularity
- High-level API usage for automatic parameter inference
- Scalable design for easy extension
- Resource-efficient implementation
- Maintainable, well-documented code

---

FILE: docs/examples/mcp-spoon-search-agent.md

---
sidebar_position: 3
---

# MCP Spoon Search Agent

This example demonstrates how to build an MCP (Model Context Protocol) enabled agent that seamlessly integrates web search capabilities with cryptocurrency analysis tools, creating a powerful research and analysis assistant.

#### ðŸŽ¯ **Core Functionality**

**Intelligent Web Search Integration:**
- **Tavily MCP integration** - Advanced web search capabilities through the Model Context Protocol
- **Real-time information retrieval** - Access to current news, articles, and market data from across the web
- **Context-aware search** - Searches are guided by user intent and current market context

**Cryptocurrency Analysis Tools:**
- **Crypto PowerData integration** - Professional-grade cryptocurrency market data and analysis
- **Multi-exchange support** - Access to data from major exchanges (Binance, Coinbase, etc.)
- **Technical indicators** - Real-time calculation of RSI, MACD, EMA, and other key indicators

**Unified Analysis System:**
- **Cross-referenced insights** - Combines web search results with technical analysis
- **Macro market analysis** - Provides comprehensive market outlook by correlating multiple data sources
- **Intelligent synthesis** - LLM-powered synthesis of diverse information sources into coherent analysis

#### ðŸš€ **Key Features Demonstrated**

- **MCP Protocol Implementation** - Complete MCP server integration and tool discovery
- **Multi-tool Orchestration** - Seamless coordination between search and analysis tools
- **Real-time Data Processing** - Live data integration from multiple APIs
- **Advanced Error Handling** - Robust error recovery and fallback mechanisms
- **Modular Architecture** - Clean separation between MCP tools and analysis logic

#### ðŸ“‹ **Prerequisites**

```bash
# Required environment variables
export TAVILY_API_KEY="your-tavily-api-key"        # Web search API
export OPENAI_API_KEY="your-openai-api-key"        # LLM responses
export ANTHROPIC_API_KEY="your-anthropic-api-key"  # Alternative LLM

# System requirements
npm install -g tavily-mcp  # Install Tavily MCP server
npx --version              # Ensure npx is available
```

#### ðŸƒ **Quick Start**

```bash
# Navigate to examples directory
cd spoon-core/example

# Install dependencies
pip install -r requirements.txt

# Run the MCP search agent
python spoon_search_agent.py
```

> â„¹ï¸ This example **only uses the MCP client in `spoon_ai`** (`MCPTool` and the `SpoonReactMCP` agent). You do **not** need `spoon-cli`. If you want to self-host a server, run a minimal FastMCP server with `FunctionTool` and connect via `MCPTool`.

#### ðŸ” **What to Observe**

**MCP Tool Discovery:**
- Watch how the system automatically discovers and connects to MCP servers
- Observe the dynamic tool loading process
- See how tools are validated and initialized

**Search-Analysis Integration:**
- Monitor how web search results are combined with market data
- Observe the correlation between news sentiment and technical indicators
- Track how the system synthesizes diverse information sources

**Real-time Processing:**
- See live data fetching from both web sources and crypto exchanges
- Watch the real-time analysis and recommendation generation
- Observe how the system handles API rate limits and errors

#### ðŸ“Š **Analysis Output Example**

```
ðŸ” COMPREHENSIVE MARKET ANALYSIS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ðŸ“° LATEST MARKET NEWS:
â€¢ Federal Reserve signals potential rate cut in Q4 2024
â€¢ Bitcoin ETF inflows reach record $2.1B this week
â€¢ Ethereum staking rewards hit 7.2% APY
â€¢ Major tech companies announce crypto payment integration

ðŸ“Š TECHNICAL ANALYSIS:
â€¢ BTC/USDT: Breaking above $45K resistance, volume spike detected
â€¢ ETH/USDT: Testing $2,800 support level, RSI showing oversold conditions
â€¢ Market-wide momentum: Bullish divergence across major altcoins

ðŸŽ¯ INVESTMENT INSIGHTS:
â€¢ SHORT-TERM: Bullish momentum favors BTC accumulation
â€¢ MEDIUM-TERM: ETH showing strong fundamental support
â€¢ RISK FACTORS: Monitor Federal Reserve policy decisions

ðŸ’¡ MARKET SENTIMENT:
Overall market sentiment is cautiously optimistic with strong institutional...
```

#### ðŸ“ **Source Code & Documentation**

- **GitHub Link**: [MCP Spoon Search Agent](https://github.com/XSpoonAi/spoon-core/blob/main/examples/mcp/spoon_search_agent.py)

#### ðŸŽ“ **Learning Objectives**

This example teaches you:
- How to integrate MCP (Model Context Protocol) servers into your agents
- Advanced multi-tool orchestration and data synthesis techniques
- Real-time web search integration with LLM-powered analysis
- Error handling and recovery in distributed tool systems
- Building research assistants that combine multiple data sources

#### ðŸ’¡ **Best Practices Demonstrated**

- **MCP Server Management** - Proper initialization and error handling for MCP servers
- **Tool Discovery** - Dynamic tool loading and validation
- **Data Correlation** - Effective synthesis of diverse information sources
- **API Rate Limiting** - Intelligent handling of API limitations and quotas
- **Fallback Mechanisms** - Robust error recovery when tools or APIs are unavailable

---

FILE: docs/examples/x402-react-agent.md

# x402 Agent

This walkthrough mirrors `core/examples/x402_agent_demo.py`, which shows a SpoonReact agent autonomously paying the official x402 paywall before summarising the protected content.

## Prerequisites

1. Install SpoonOS core dependencies (`uv pip install -r requirements.txt`).
2. Configure `core/.env` with:
   - `OPENAI_API_KEY`
   - `PRIVATE_KEY` (0x-prefixed; must hold â‰¥0.01 USDC). If omitted, set the Turnkey variables documented in the API reference.
   - `X402_RECEIVER_ADDRESS` (usually matches the private key address).
   - Optional: `X402_FACILITATOR_URL`, `X402_DEFAULT_NETWORK`, `X402_DEMO_URL`.
3. Acquire USDC Testnet Token (0.01 is enough) via [https://faucet.circle.com/](https://faucet.circle.com/).

## Run the demo

```bash
uv run python examples/x402_agent_demo.py
```

What happens:

1. The script prints signer details and the target resource (`https://www.x402.org/protected` by default).
2. A `SpoonReactAI` instance performs a ReAct loop:
   - Calls `web_scraper` (no payment) to capture the 402 challenge.
   - Calls `x402_paywalled_request` to sign and submit a 0.01 USDC payment.
   - Retrieves the protected payload (a SoundCloud embed) after settlement.
3. The console logs tool traces, the signed `PAYMENT-SIGNATURE` (x402 v2) header, and the decoded settlement receipt from `PAYMENT-RESPONSE` (transaction hash, payer, network).

## Troubleshooting

| Symptom | Likely cause | Fix |
| --- | --- | --- |
| `invalid_exact_evm_payload_signature` | Asset/network mismatch or stale nonce. | Ensure you copied the paywall's `asset` and `pay_to` fields, and confirm `PRIVATE_KEY` funds exist. |
| `Configuration error: API key is required for provider ...` | Missing `OPENAI_API_KEY`. | Export a valid LLM key before running the demo. |
| `x402 configuration error: Turnkey signing identity missing` | `X402_USE_TURNKEY` enabled but no `TURNKEY_SIGN_WITH`. | Provide the required Turnkey identifiers or disable Turnkey by setting `X402_USE_TURNKEY=0`. |

## Next steps

- Automate the payment header generation inside your own agents by reusing the `x402_paywalled_request` tool documented in the API reference.
- Expose your agents via the paywall router (`python -m spoon_ai.payments.app`) so external callers must submit verified x402 payments before invoking them.

---

FILE: docs/getting-started/configuration.md

# Configuration

SpoonOS is **env-first**. The core Python SDK only reads environment variables (including values from a `.env` file). The `spoon-cli` workflow is the only place `config.json` is read; the CLI loads that file and exports the values into the environment before starting agents.

## Configuration Priority

At runtime (latest wins):

1. Built-in defaults in the SDK  
2. Environment variables (`.env` or shell)  
3. Values exported by `spoon-cli` from `config.json` (CLI only)

## Environment Variables

Create a `.env` file in your project root:

```bash
# LLM Provider API Keys (set at least one)
GEMINI_API_KEY=your_gemini_key_here        # recommended for Quick Start
OPENAI_API_KEY=your_openai_key_here
ANTHROPIC_API_KEY=your_anthropic_key_here
DEEPSEEK_API_KEY=your_deepseek_key_here
OPENROUTER_API_KEY=your_openrouter_key_here

# Optional: Default LLM Settings
DEFAULT_LLM_PROVIDER=gemini                # or openai / anthropic / deepseek / openrouter
DEFAULT_MODEL=gemini-2.5-pro
GEMINI_MAX_TOKENS=20000                    # recommended context limit for Gemini

# Web3 Configuration (only needed for on-chain tools)
WEB3_PROVIDER_URL=https://mainnet.infura.io/v3/your_project_id
PRIVATE_KEY=your_private_key_here
```

## CLI Configuration File (optional)

If you use `spoon-cli`, manage CLI-specific settings in `config.json`. The CLI exports that file into environment variables automatically; the SDK does **not** read it directly. See `docs/cli/configuration.md` for the full schema and commands.

## API Key Setup

### OpenAI
1. Visit [OpenAI API Keys](https://platform.openai.com/api-keys)
2. Create a new API key
3. Add to your `.env` file

### Anthropic
1. Visit [Anthropic Console](https://console.anthropic.com/)
2. Generate an API key
3. Add to your `.env` file

### Google (Gemini)
1. Visit [Google AI Studio](https://aistudio.google.com/app/apikey)
2. Create an API key
3. Add to your `.env` file

## Verification

Test your configuration:

```bash
python -c "from spoon_ai.utils.config_manager import ConfigManager; print('âœ… Configuration loaded successfully')"
```

The framework automatically validates your configuration and provides helpful error messages if any issues are detected.

## Next Steps

- [Quick Start](./quick-start.md) - Build your first agent
- [Core Concepts](../core-concepts/agents.md) - Learn about agents

---

FILE: docs/getting-started/installation.md

# Installation

## Prerequisites

- Python 3.12 or higher
- Git
- Virtual environment (recommended)

## Quick Installation

### Fast path (uv â€” recommended)

`uv` gives faster, reproducible installs and works as a drop-in replacement for `pip`:

```bash
uv venv .venv
source .venv/bin/activate            # macOS/Linux
# .\\.venv\\Scripts\\Activate.ps1    # Windows (PowerShell)

# Install published packages
uv pip install spoon-ai-sdk          # core framework
uv pip install spoon-toolkits        # optional: extended blockchain & data toolkits
```

You can substitute `uv pip` with `pip` if you prefer the standard installer.

### Option A: Install from PyPI (recommended)

You can use the published PyPI packages without cloning the repository:

1. Create and activate a virtual environment

```bash
# macOS/Linux
python3 -m venv spoon-env
source spoon-env/bin/activate

# Windows (PowerShell)
python -m venv spoon-env
.\spoon-env\Scripts\Activate.ps1
```

2. Install the core SDK (and optionally the toolkits package)

```bash
pip install spoon-ai-sdk        # core framework
pip install spoon-toolkits      # optional: extended blockchain & data toolkits
```

### Option B: Use a local repository checkout

If you are working inside this monorepo (for example you already opened it in your IDE), you can install directly from the local folders without needing to `git clone` again.

1. Create Virtual Environment

```bash
# macOS/Linux
python3 -m venv spoon-env
source spoon-env/bin/activate

# Windows (PowerShell)
python -m venv spoon-env
.\spoon-env\Scripts\Activate.ps1
```

> ðŸ’¡ On newer Apple Silicon Macs the `python` shim may not point to Python 3.
> Use `python3` for all commands unless you have explicitly configured `python`
> to target Python 3.12 or later.

2. Install core package in editable mode

```bash
git clone https://github.com/XSpoonAi/spoon-core.git
cd spoon-core
uv pip install -e .    # or `pip install -e .` if you don't use uv
```

3. (Optional) Install Toolkits Package from local repo

If you want to use the extended blockchain and data tools from `spoon_toolkits`, install the **spoon-toolkits** package from the `spoon-toolkits` folder:

```bash
git clone https://github.com/XSpoonAi/spoon-toolkits.git
cd spoon-toolkits
pip install -e .
```

## Framework Validation

The SpoonOS framework includes built-in validation that automatically:

- Checks API key configuration
- Validates provider connectivity
- Ensures proper dependency installation
- Provides clear error messages if issues are found

## Next Steps

- [Configuration](./configuration.md) - Set up API keys and configuration
- [Quick Start](./quick-start.md) - Build your first agent

---

FILE: docs/getting-started/quick-start.md

# Quick Start

Get up and running with SpoonOS framework in under 5 minutes.

## Prerequisites

- [Installation](./installation.md) completed
- [Configuration](./configuration.md) set up with at least one provider API key (for example `OPENAI_API_KEY`)

## Your First Agent

### 1. Create a Simple Agent

Create a new Python file `my_first_agent.py`:

```python
import asyncio
from spoon_ai.agents.toolcall import ToolCallAgent
from spoon_ai.chat import ChatBot
from spoon_ai.tools import ToolManager
from spoon_ai.tools.base import BaseTool

# Define a custom tool
class GreetingTool(BaseTool):
    name: str = "greeting"
    description: str = "Generate personalized greetings"
    parameters: dict = {
        "type": "object",
        "properties": {
            "name": {"type": "string", "description": "Person's name"}
        },
        "required": ["name"]
    }

    async def execute(self, name: str) -> str:
        return f"Hello {name}! Welcome to SpoonOS! ðŸš€"

# Create your agent
class MyFirstAgent(ToolCallAgent):
    name: str = "my_first_agent"
    description: str = "A friendly assistant with greeting capabilities"

    system_prompt: str = """
    You are a helpful AI assistant built with SpoonOS framework.
    You can greet users and help with various tasks.
    """

    available_tools: ToolManager = ToolManager([GreetingTool()])

async def main():
    # Initialize agent with LLM
    agent = MyFirstAgent(
        llm=ChatBot(
            llm_provider="openai",         # or "anthropic", "gemini", "deepseek", "openrouter"
            model_name="gpt-5.1-chat-latest"   # Framework default for OpenAI
        )
    )

    # Run the agent - framework handles all error cases automatically
    response = await agent.run("Please greet me, my name is Alice")
    return response

if __name__ == "__main__":
    result = asyncio.run(main())
    # Agent response will be returned directly
```

### 2. Run Your Agent

```bash
python my_first_agent.py
```

The agent will respond with a personalized greeting and offer to help with various tasks.

### 3. Add Real Data Capabilities

Enhance your agent with search and web scraping tools:

```python
from spoon_toolkits import DesearchWebSearchTool, WebScraperTool

class ResearchAgent(ToolCallAgent):
    name: str = "research_agent"
    description: str = "AI agent with web research capabilities"

    system_prompt: str = """
    You are a research assistant with access to web search and content scraping.
    You can help find information, read articles, and analyze web content.
    """

    available_tools: ToolManager = ToolManager([
        GreetingTool(),
        # Research tools (requires `pip install spoon-toolkits`)
        DesearchWebSearchTool(),
        WebScraperTool(),
    ])

# Usage
async def research_demo():
    agent = ResearchAgent(
        llm=ChatBot(
            llm_provider="anthropic",
            model_name="claude-sonnet-4-20250514"  # Framework default
        )
    )

    # Framework automatically handles search and error cases
    response = await agent.run("Search for the latest AI news and summarize it")
    return response
```

### 4. Framework Features Overview

The SpoonOS framework provides:

- **Multiple LLM Providers**: OpenAI (`openai`), Anthropic (`anthropic`), Google Gemini (`gemini`), DeepSeek (`deepseek`), OpenRouter (`openrouter`)
- **Built-in Tools**: Web search, content scraping, data analysis, and more via `spoon-toolkits`
- **Agent Types**: ReAct, ToolCall, Graph-based agents
- **MCP Integration**: Dynamic tool discovery and execution

### Framework Simplicity

SpoonOS eliminates common development complexity:

```python
# Simple agent creation - no error handling needed
agent = ToolCallAgent(
    llm=ChatBot(llm_provider="openai", model_name="gpt-5.1-chat-latest"),
    available_tools=ToolManager([CryptoTool(), Web3Tool()])
)


response = await agent.run("Analyze Bitcoin trends and suggest trades")
```

## Framework Development Patterns

### Agent Composition

```python
# Combine multiple agents for complex workflows
from spoon_ai.agents.graph import GraphAgent

class MultiAgentSystem(GraphAgent):
    def __init__(self):
        super().__init__()
        self.add_agent("researcher", ResearchAgent())
        self.add_agent("analyst", AnalysisAgent())
        self.add_agent("trader", TradingAgent())
```

### Custom Tool Development

```python
# Create domain-specific tools
class BlockchainAnalysisTool(BaseTool):
    name: str = "blockchain_analysis"
    description: str = "Analyze blockchain transactions and patterns"

    async def execute(self, address: str, chain: str = "ethereum") -> str:
        # Your custom blockchain analysis logic
        return f"Analysis results for {address} on {chain}"
```

### MCP Integration

```python
# Use Model Context Protocol for dynamic tools
from spoon_ai.tools.mcp_tool import MCPTool
import os

# Create an MCP tool for web search
tavily_tool = MCPTool(
    name="tavily-search",
    description="Search the web using Tavily API",
    mcp_config={
        "command": "npx",
        "args": ["-y", "tavily-mcp"],
        "env": {"TAVILY_API_KEY": os.getenv("TAVILY_API_KEY")},
    },
)

agent = ToolCallAgent(
    llm=ChatBot(llm_provider="anthropic", model_name="claude-sonnet-4-20250514"),
    available_tools=ToolManager([tavily_tool])
)
```

## Next Steps

Now that you understand the framework basics:

- [Core Concepts](../core-concepts/agents.md) - Deep dive into agent architecture
- [Built-in Tools](../core-concepts/tools.md) - Explore Web3 and crypto tools
- [How-To Guides](../how-to-guides/build-first-agent.md) - Advanced agent patterns

---

FILE: docs/graph-system/advanced-features.md

---
sidebar_position: 5
title: Advanced Features
description: Routing strategies, parallel execution, human-in-the-loop, and error handling
---

# Advanced Features

This guide covers production-ready patterns for building robust, scalable graph workflows.

**You will learn:** Routing layers, parallel execution, HITL, retries, and monitoring patterns
**Best for:** Users ready for production hardening and debugging
**Time to complete:** ~8â€“12 minutes

## Routing Strategies

The Graph System evaluates routing in a priority order, giving you multiple layers of control.

### Routing Priority Stack

```mermaid
flowchart TD
    A[Node Complete] --> B{Explicit Edge?}
    B -->|Yes| C[Follow Static Edge]
    B -->|No| D{Routing Rule Match?}
    D -->|Yes| E[Apply Highest Priority Rule]
    D -->|No| F{Intelligent Router?}
    F -->|Yes| G[Call Router Function]
    F -->|No| H{LLM Router Enabled?}
    H -->|Yes| I[LLM Decides Target]
    H -->|No| J{Default Target?}
    J -->|Yes| K[Use Default]
    J -->|No| L[END]

    style C fill:#c8e6c9
    style E fill:#fff3e0
    style G fill:#e3f2fd
    style I fill:#fce4ec
```

**Pitfall:** Explicit edges are evaluated before routing rules. If you add `add_edge("entry", "fallback")`, routing rules for `entry` will never fire. For a fallback, prefer `graph.config.router.default_target = "fallback"` instead of a static edge from the source node.

### 1. Conditional Edges (Most Common)

Route based on state inspection:

```python
import asyncio
from typing import TypedDict

from spoon_ai.graph import END, StateGraph


class AnalysisState(TypedDict, total=False):
    confidence: float
    output: str


async def analyze(state: AnalysisState) -> dict:
    # Pretend we ran an analysis that produced a confidence score.
    return {"confidence": float(state.get("confidence", 0.0) or 0.0)}


async def generate_recommendation(state: AnalysisState) -> dict:
    return {"output": "generated recommendation"}


async def request_clarification(state: AnalysisState) -> dict:
    return {"output": "requested clarification"}


async def escalate_to_human(state: AnalysisState) -> dict:
    return {"output": "escalated to human"}


def route_by_confidence(state: AnalysisState) -> str:
    """Route based on analysis confidence level."""
    confidence = state.get("confidence", 0.0) or 0.0
    if confidence >= 0.8:
        return "high_confidence"
    if confidence >= 0.5:
        return "medium_confidence"
    return "low_confidence"


graph = StateGraph(AnalysisState)
graph.add_node("analyze", analyze)
graph.add_node("generate_recommendation", generate_recommendation)
graph.add_node("request_clarification", request_clarification)
graph.add_node("escalate_to_human", escalate_to_human)
graph.set_entry_point("analyze")

graph.add_conditional_edges(
    "analyze",
    route_by_confidence,
    {
        "high_confidence": "generate_recommendation",
        "medium_confidence": "request_clarification",
        "low_confidence": "escalate_to_human",
    },
)

graph.add_edge("generate_recommendation", END)
graph.add_edge("request_clarification", END)
graph.add_edge("escalate_to_human", END)

app = graph.compile()


async def main() -> None:
    for confidence in [0.9, 0.6, 0.2]:
        result = await app.invoke({"confidence": confidence})
        print(confidence, "->", result["output"])


if __name__ == "__main__":
    asyncio.run(main())
```

### 2. Routing Rules (Pattern-Based)

For pattern matching with priorities:

```python
import asyncio
from typing import TypedDict

from spoon_ai.graph import END, StateGraph


class RoutingState(TypedDict, total=False):
    user_query: str
    category: str
    output: str


async def entry(state: RoutingState) -> dict:
    # No explicit edge from this node; routing rules decide the next node.
    return {}


async def priority_handler(state: RoutingState) -> dict:
    return {"output": f"Priority: {state.get('user_query', '')}"}


async def trading_handler(state: RoutingState) -> dict:
    return {"output": f"Trading: {state.get('user_query', '')}"}


async def general_handler(state: RoutingState) -> dict:
    return {"output": f"Fallback: {state.get('user_query', '')}"}


graph = StateGraph(RoutingState)
graph.add_node("entry", entry)
graph.add_node("priority_handler", priority_handler)
graph.add_node("trading_handler", trading_handler)
graph.add_node("general_handler", general_handler)
graph.set_entry_point("entry")

# Priority 10 - Specific patterns first
graph.add_routing_rule(
    source_node="entry",
    condition=lambda state, query: "urgent" in query,
    target_node="priority_handler",
    priority=10,
)

# Priority 5 - Category-based
graph.add_routing_rule(
    source_node="entry",
    condition=lambda state, query: state.get("category") == "trading",
    target_node="trading_handler",
    priority=5,
)

# Priority 1 - Default fallback
graph.add_routing_rule(
    source_node="entry",
    condition=lambda state, query: True,  # Always matches
    target_node="general_handler",
    priority=1,
)

graph.add_edge("priority_handler", END)
graph.add_edge("trading_handler", END)
graph.add_edge("general_handler", END)

app = graph.compile()


async def main() -> None:
    for state in [
        {"user_query": "urgent: price drop", "category": ""},
        {"user_query": "btc outlook", "category": "trading"},
        {"user_query": "hello", "category": ""},
    ]:
        result = await app.invoke(state)
        print(result["output"])


if __name__ == "__main__":
    asyncio.run(main())
```

### 3. Intelligent Router (Custom Logic)

For complex routing logic:

```python
import asyncio
from typing import List, TypedDict

from spoon_ai.graph import END, StateGraph


class AnalysisState(TypedDict, total=False):
    user_query: str
    intent: str
    confidence: float
    routing_history: List[str]
    output: str


async def analyze(state: AnalysisState) -> dict:
    history = list(state.get("routing_history") or [])
    history.append("analyze")
    return {"routing_history": history}


async def human_review(state: AnalysisState) -> dict:
    return {"output": "needs human review"}


async def execute_trade(state: AnalysisState) -> dict:
    return {"output": "executed trade (stub)"}


async def general_handler(state: AnalysisState) -> dict:
    return {"output": "general handler"}


async def intelligent_router(state: AnalysisState, query: str) -> str:
    """Custom routing logic."""
    intent = state.get("intent", "")
    confidence = state.get("confidence", 0.0) or 0.0
    history = state.get("routing_history", []) or []

    # Avoid loops
    if "analyze" in history and intent == "retry":
        return "human_review"

    if intent == "trading" and confidence > 0.9:
        return "execute_trade"

    return "general_handler"


graph = StateGraph(AnalysisState)
graph.add_node("analyze", analyze)
graph.add_node("human_review", human_review)
graph.add_node("execute_trade", execute_trade)
graph.add_node("general_handler", general_handler)
graph.set_entry_point("analyze")
graph.set_intelligent_router(intelligent_router)

graph.add_edge("human_review", END)
graph.add_edge("execute_trade", END)
graph.add_edge("general_handler", END)

app = graph.compile()


async def main() -> None:
    result = await app.invoke({"user_query": "trade", "intent": "trading", "confidence": 0.95, "routing_history": []})
    print(result["output"])


if __name__ == "__main__":
    asyncio.run(main())
```

### 4. LLM-Powered Routing (Most Flexible)

Let an LLM decide the next step:

```python
from spoon_ai.graph.config import GraphConfig, RouterConfig
from typing import TypedDict

from spoon_ai.graph import StateGraph

config = GraphConfig(
    router=RouterConfig(
        allow_llm=True,
        llm_timeout=8.0,
        default_target="fallback_handler",
        allowed_targets=["price_handler", "trade_handler", "analysis_handler", "fallback_handler"],
    )
)

class AnalysisState(TypedDict, total=False):
    user_query: str
    output: str

graph = StateGraph(AnalysisState)
graph.config = config

# Or enable after creation
graph.enable_llm_routing(config={
    "model": "gpt-4",
    "temperature": 0.1,
    "max_tokens": 50,
})
```

### Routing Decision Matrix

| Routing Type | Complexity | Latency | Use Case |
|--------------|------------|---------|----------|
| Static Edge | None | ~0ms | Linear workflows |
| Conditional Edge | Low | ~0ms | State-based branching |
| Routing Rules | Medium | ~0ms | Pattern matching |
| Intelligent Router | Medium | ~1ms | Custom business logic |
| LLM Router | High | ~500ms | Natural language understanding |

---

## Parallel Execution

Execute multiple nodes concurrently for better performance.

### Basic Parallel Group

```python
from spoon_ai.graph.config import ParallelGroupConfig
from typing import Any, Dict, TypedDict

from spoon_ai.graph import END, StateGraph

class MarketState(TypedDict, total=False):
    symbol: str
    binance: Dict[str, Any]
    coinbase: Dict[str, Any]
    kraken: Dict[str, Any]
    output: str


async def fetch_binance(state: MarketState) -> dict:
    return {"binance": {"price": 45000}}


async def fetch_coinbase(state: MarketState) -> dict:
    return {"coinbase": {"price": 45050}}


async def fetch_kraken(state: MarketState) -> dict:
    return {"kraken": {"price": 44980}}


async def aggregate(state: MarketState) -> dict:
    prices = [state.get("binance", {}).get("price"), state.get("coinbase", {}).get("price"), state.get("kraken", {}).get("price")]
    prices = [p for p in prices if isinstance(p, (int, float))]
    avg = sum(prices) / len(prices) if prices else 0.0
    return {"output": f"avg price: {avg:.2f}"}


graph = StateGraph(MarketState)
graph.add_node("fetch_binance", fetch_binance)
graph.add_node("fetch_coinbase", fetch_coinbase)
graph.add_node("fetch_kraken", fetch_kraken)
graph.add_node("aggregate", aggregate)

graph.add_parallel_group(
    "market_data_fetch",
    nodes=["fetch_binance", "fetch_coinbase", "fetch_kraken"],
    config=ParallelGroupConfig(join_strategy="all", timeout=15.0),
)

graph.set_entry_point("fetch_binance")
graph.add_edge("fetch_binance", "aggregate")
graph.add_edge("fetch_coinbase", "aggregate")
graph.add_edge("fetch_kraken", "aggregate")
graph.add_edge("aggregate", END)

app = graph.compile()
```

### Join Strategies

| Strategy | Behavior | Use Case |
|----------|----------|----------|
| `"all"` | Wait for all nodes to complete | Need complete data from all sources |
| `"any"` | Return when first node completes | Redundant sources, want fastest |
| `"quorum"` | Wait for majority (configurable) | Fault-tolerant consensus |

```python
# These are standalone config examples.
from spoon_ai.graph.config import ParallelGroupConfig

# Quorum: Wait for 2 out of 3 (66%)
config = ParallelGroupConfig(
    join_strategy="quorum",
    quorum=0.66,  # 66% = 2 of 3 nodes
    timeout=15.0,
)

# Any: Return on first success
config = ParallelGroupConfig(
    join_strategy="any",
    timeout=10.0,
)
```

### Error Strategies

| Strategy | Behavior | Use Case |
|----------|----------|----------|
| `"fail_fast"` | Cancel all, raise exception | Critical path, must all succeed |
| `"collect_errors"` | Continue, store errors in `__errors__` | Best-effort, report issues |
| `"ignore_errors"` | Continue, discard failures | Non-critical enrichment |

```python
from spoon_ai.graph.config import ParallelGroupConfig, ParallelRetryPolicy

config = ParallelGroupConfig(
    join_strategy="quorum",
    quorum=0.66,
    timeout=30.0,

    # Error handling
    error_strategy="collect_errors",

    # Retry policy for individual nodes
    retry_policy=ParallelRetryPolicy(
        max_retries=2,
        backoff_initial=0.5,
        backoff_multiplier=2.0,
        backoff_max=5.0,
    ),

    # Resource controls
    max_in_flight=10,
    rate_limit_per_second=5.0,

    # Circuit breaker
    circuit_breaker_threshold=5,
    circuit_breaker_cooldown=30.0,
)
```

### Complete Parallel Example

```python
import asyncio
from typing import TypedDict, Dict, Any, List
from spoon_ai.graph import StateGraph, END
from spoon_ai.graph.config import ParallelGroupConfig

class MultiSourceState(TypedDict):
    symbol: str
    source_a_data: Dict[str, Any]
    source_b_data: Dict[str, Any]
    source_c_data: Dict[str, Any]
    aggregated_price: float
    errors: List[str]

async def fetch_source_a(state: MultiSourceState) -> dict:
    await asyncio.sleep(0.1)  # Simulate API call
    return {"source_a_data": {"price": 45000, "source": "A"}}

async def fetch_source_b(state: MultiSourceState) -> dict:
    await asyncio.sleep(0.2)
    return {"source_b_data": {"price": 45050, "source": "B"}}

async def fetch_source_c(state: MultiSourceState) -> dict:
    await asyncio.sleep(0.15)
    # Simulate occasional failure
    if state.get("symbol") == "FAIL":
        raise Exception("Source C unavailable")
    return {"source_c_data": {"price": 44980, "source": "C"}}

async def aggregate_prices(state: MultiSourceState) -> dict:
    prices = []
    for key in ["source_a_data", "source_b_data", "source_c_data"]:
        data = state.get(key, {})
        if data.get("price"):
            prices.append(data["price"])

    avg = sum(prices) / len(prices) if prices else 0
    return {"aggregated_price": avg}

# Build graph
graph = StateGraph(MultiSourceState)

graph.add_node("fetch_a", fetch_source_a)
graph.add_node("fetch_b", fetch_source_b)
graph.add_node("fetch_c", fetch_source_c)
graph.add_node("aggregate", aggregate_prices)

# Configure parallel group
graph.add_parallel_group(
    "data_fetch",
    nodes=["fetch_a", "fetch_b", "fetch_c"],
    config=ParallelGroupConfig(
        join_strategy="quorum",
        quorum=0.66,
        timeout=5.0,
        error_strategy="collect_errors",
    )
)

# Edges
graph.add_edge("fetch_a", "aggregate")
graph.add_edge("fetch_b", "aggregate")
graph.add_edge("fetch_c", "aggregate")
graph.add_edge("aggregate", END)

graph.set_entry_point("fetch_a")
app = graph.compile()
```

---

## Human-in-the-Loop

Interrupt execution to collect user input, then resume.

### Basic Interrupt Pattern

```python
from typing import Any, Dict, TypedDict

from spoon_ai.graph import interrupt


class TradeState(TypedDict, total=False):
    trade_details: Dict[str, Any]
    user_confirmed: bool
    trade_executed: bool
    execution_time: str


async def confirm_trade_node(state: TradeState) -> dict:
    """Node that requires user confirmation."""
    trade_details = state.get("trade_details", {})

    # Check if already confirmed
    if not state.get("user_confirmed"):
        # Interrupt execution and wait for user
        interrupt({
            "type": "confirmation_required",
            "question": f"Execute {trade_details['action']} {trade_details['amount']} {trade_details['symbol']}?",
            "trade_details": trade_details,
        })

    # This runs after resume with confirmation
    return {
        "trade_executed": True,
        "execution_time": "2024-01-15T10:30:00Z"
    }
```

### Handling Interrupts

```python
import asyncio

from typing import Any, Dict, TypedDict

from spoon_ai.graph import END, StateGraph, interrupt


async def main() -> None:
    class TradeState(TypedDict, total=False):
        trade_details: Dict[str, Any]
        user_confirmed: bool
        trade_executed: bool

    async def confirm_trade(state: TradeState) -> dict:
        details = state.get("trade_details", {})
        if not state.get("user_confirmed"):
            interrupt(
                {
                    "type": "confirmation_required",
                    "question": f"Execute {details.get('action')} {details.get('amount')} {details.get('symbol')}?",
                    "trade_details": details,
                }
            )
        return {}

    async def execute_trade(state: TradeState) -> dict:
        return {"trade_executed": True}

    graph = StateGraph(TradeState)
    graph.add_node("confirm_trade", confirm_trade)
    graph.add_node("execute_trade", execute_trade)
    graph.set_entry_point("confirm_trade")
    graph.add_edge("confirm_trade", "execute_trade")
    graph.add_edge("execute_trade", END)
    app = graph.compile()

    config = {"configurable": {"thread_id": "trade_session"}}
    initial_state = {
        "trade_details": {"action": "buy", "amount": 0.1, "symbol": "BTC"},
        "user_confirmed": False,
        "trade_executed": False,
    }

    # First execution - will interrupt
    result = await app.invoke(initial_state, config=config)
    if "__interrupt__" in result:
        interrupt_info = result["__interrupt__"][0]
        print(f"Question: {interrupt_info['value']['question']}")

        # In a real app, collect this from UI/CLI/API; keep docs non-interactive.
        user_confirmed = True

        # \"Resume\" by invoking again with updated state (this graph re-checks at the entry node).
        resume_state = {**initial_state, "user_confirmed": user_confirmed}
        result = await app.invoke(resume_state, config=config)

    print(f"Final result: {result}")


if __name__ == "__main__":
    asyncio.run(main())
```

### Multi-Step Approval Workflow

```python
import asyncio
from typing import TypedDict

from spoon_ai.graph import END, StateGraph, interrupt


class ApprovalState(TypedDict, total=False):
    request: str
    manager_approved: bool
    compliance_approved: bool
    final_status: str


async def request_manager_approval(state: ApprovalState) -> dict:
    if not state.get("manager_approved"):
        interrupt({"type": "manager_approval", "request": state.get("request", ""), "approver_role": "manager"})
    return {}


async def request_compliance_approval(state: ApprovalState) -> dict:
    if not state.get("compliance_approved"):
        interrupt({"type": "compliance_approval", "request": state.get("request", ""), "approver_role": "compliance"})
    return {}


async def execute_request(state: ApprovalState) -> dict:
    if state.get("manager_approved") and state.get("compliance_approved"):
        return {"final_status": "approved_and_executed"}
    return {"final_status": "rejected"}


# Wire up: manager â†’ compliance â†’ execute
graph = StateGraph(ApprovalState)
graph.add_node("manager", request_manager_approval)
graph.add_node("compliance", request_compliance_approval)
graph.add_node("execute", execute_request)
graph.set_entry_point("manager")
graph.add_edge("manager", "compliance")
graph.add_edge("compliance", "execute")
graph.add_edge("execute", END)
app = graph.compile()


async def main() -> None:
    config = {"configurable": {"thread_id": "approval_session"}}

    # 1) manager approval interrupt
    state = {"request": "approve transfer", "manager_approved": False, "compliance_approved": False}
    result = await app.invoke(state, config=config)
    print("step1:", result.get("__interrupt__"))

    # 2) compliance approval interrupt
    state = {**state, "manager_approved": True}
    result = await app.invoke(state, config=config)
    print("step2:", result.get("__interrupt__"))

    # 3) fully approved
    state = {**state, "compliance_approved": True}
    result = await app.invoke(state, config=config)
    print("final:", result.get("final_status"))


if __name__ == "__main__":
    asyncio.run(main())
```

### Interrupt Best Practices

:::tip Guidelines
1. **Always check state first** - Don't interrupt if already confirmed
2. **Include context** - Provide all info needed for decision
3. **Use thread IDs** - Required for resume functionality
4. **Handle timeout** - What if user never responds?
5. **Log interrupts** - Track for audit purposes
:::

---

## Error Handling

Production-ready error handling patterns.

### Retry Policies

```python
from spoon_ai.graph.config import ParallelRetryPolicy

retry_policy = ParallelRetryPolicy(
    max_retries=3,              # Try up to 3 times
    backoff_initial=0.5,        # Start with 0.5s delay
    backoff_multiplier=2.0,     # Double each time: 0.5, 1, 2
    backoff_max=10.0,           # Cap at 10 seconds
)
```

### Circuit Breakers

Prevent cascading failures:

```python
from spoon_ai.graph.config import ParallelGroupConfig

config = ParallelGroupConfig(
    # After 5 failures, disable the group
    circuit_breaker_threshold=5,

    # Re-enable after 30 seconds
    circuit_breaker_cooldown=30.0,
)
```

### Error Handling in Nodes

```python
import asyncio
from typing import TypedDict


class MyState(TypedDict, total=False):
    symbol: str


async def external_api_call(symbol: str) -> dict:
    # Minimal stub for docs. Replace with a real client call.
    if symbol == "TIMEOUT":
        raise TimeoutError("simulated timeout")
    if symbol == "DOWN":
        raise ConnectionError("simulated connection error")
    return {"symbol": symbol, "price": 123.45}


async def robust_api_node(state: MyState) -> dict:
    """Node with comprehensive error handling."""
    import logging
    logger = logging.getLogger(__name__)

    try:
        result = await external_api_call(state["symbol"])
        return {
            "data": result,
            "error": None,
            "status": "success"
        }

    except ConnectionError as e:
        logger.warning(f"Connection failed for {state['symbol']}: {e}")
        return {
            "data": None,
            "error": f"Connection error: {e}",
            "status": "retry_suggested"
        }

    except TimeoutError as e:
        logger.error(f"Timeout for {state['symbol']}: {e}")
        return {
            "data": None,
            "error": f"Timeout: {e}",
            "status": "timeout"
        }

    except Exception as e:
        logger.exception(f"Unexpected error for {state['symbol']}")
        return {
            "data": None,
            "error": f"Unexpected: {e}",
            "status": "failed"
        }


async def main() -> None:
    for symbol in ["BTC", "DOWN", "TIMEOUT"]:
        result = await robust_api_node({"symbol": symbol})
        print(symbol, "->", result["status"])


if __name__ == "__main__":
    asyncio.run(main())
```

### State Validation

```python
import asyncio
from typing import TypedDict

from spoon_ai.graph import END, StateGraph
from spoon_ai.graph.config import GraphConfig

class MyState(TypedDict, total=False):
    user_id: str
    amount: float
    output: str


def validate_state(state: dict) -> None:
    """Raise exception if state is invalid."""
    if not state.get("user_id"):
        raise ValueError("user_id is required")

    if state.get("amount", 0) < 0:
        raise ValueError("amount cannot be negative")

config = GraphConfig(
    state_validators=[validate_state],
    max_iterations=100,
)

graph = StateGraph(MyState)
graph.config = config


async def process(state: MyState) -> dict:
    return {"output": "ok"}


graph.add_node("process", process)
graph.set_entry_point("process")
graph.add_edge("process", END)
app = graph.compile()


async def main() -> None:
    try:
        await app.invoke({"amount": 1.0})  # missing user_id
    except Exception as e:
        print("validation failed:", str(e)[:80])

    result = await app.invoke({"user_id": "user_123", "amount": 1.0})
    print(result["output"])


if __name__ == "__main__":
    asyncio.run(main())
```

---

## Resource Controls

Manage system resources and prevent overload.

### Rate Limiting

```python
from spoon_ai.graph.config import ParallelGroupConfig

config = ParallelGroupConfig(
    rate_limit_per_second=10.0,  # Max 10 requests/second
    max_in_flight=5,            # Max 5 concurrent tasks
)
```

### Execution Limits

```python
from spoon_ai.graph.config import GraphConfig

config = GraphConfig(
    max_iterations=100,  # Prevent infinite loops
)
```

### Timeout Configuration

```python
from spoon_ai.graph.config import ParallelGroupConfig, RouterConfig

config = ParallelGroupConfig(
    timeout=30.0,  # 30 second timeout for parallel group
)

# Or for LLM routing
router_config = RouterConfig(
    llm_timeout=8.0,  # 8 second timeout for LLM calls
)
```

---

## Configuration Reference

### GraphConfig

```python
from spoon_ai.graph.config import GraphConfig, RouterConfig

config = GraphConfig(
    # Execution limits
    max_iterations=100,

    # Router configuration
    router=RouterConfig(
        allow_llm=False,
        allowed_targets=None,        # None = all nodes allowed
        default_target=None,         # Fallback when no route matches
        llm_timeout=8.0,
        enable_fallback_to_default=True,
    ),

    # Validation
    state_validators=[],

    # Pre-configured parallel groups
    parallel_groups={},
)
```

### ParallelGroupConfig

```python
from spoon_ai.graph.config import ParallelGroupConfig, ParallelRetryPolicy

config = ParallelGroupConfig(
    # Join behavior
    join_strategy="all",        # "all", "any", "quorum"
    quorum=None,                # For quorum: 0.0-1.0 or int

    # Timing
    timeout=None,               # None = unlimited

    # Error handling
    error_strategy="fail_fast", # "fail_fast", "collect_errors", "ignore_errors"

    # Retry
    retry_policy=ParallelRetryPolicy(
        max_retries=0,
        backoff_initial=0.5,
        backoff_multiplier=2.0,
        backoff_max=10.0,
    ),

    # Resource controls
    max_in_flight=None,
    rate_limit_per_second=None,

    # Circuit breaker
    circuit_breaker_threshold=None,
    circuit_breaker_cooldown=30.0,
)
```

---

## Next Steps

Learn how to integrate your graphs with the broader SpoonOS ecosystem:

**[Integration & Extensions â†’](./integration.md)** - GraphAgent, tools, MCP protocol, memory management

---

FILE: docs/graph-system/building-graphs.md

---
sidebar_position: 4
title: Building Graphs
description: Learn the three API styles - Imperative, Declarative, and High-Level
---

# Building Graphs

SpoonOS provides three ways to build graphs, each suited for different scenarios. Choose based on your workflow complexity and team needs.

**You will learn:** Imperative, declarative, and high-level APIs with when-to-use guidance
**Best for:** Users who understand core concepts and want to pick an API style
**Time to complete:** ~6â€“10 minutes

## API Comparison

| Feature | Imperative | Declarative | High-Level |
|---------|------------|-------------|------------|
| **Complexity** | Simple | Medium | Advanced |
| **Use Case** | Quick prototypes, simple workflows | Large workflows, team collaboration | LLM-driven, dynamic routing |
| **Serializable** | No | Yes | Yes |
| **Code Style** | Method chaining | Template objects | Automatic inference |
| **Best For** | Learning, small graphs | Production systems | Intelligent agents |

---

## Imperative API

The simplest way to build graphs. Use method calls to add nodes and edges directly.

### When to Use

- âœ… Quick prototypes and experiments
- âœ… Simple linear or branching workflows
- âœ… Learning the Graph System
- âŒ Large, complex workflows (hard to maintain)
- âŒ Team collaboration (no serialization)

### Basic Example

```python
import asyncio
from typing import TypedDict
from spoon_ai.graph import StateGraph, END

class WorkflowState(TypedDict):
    input: str
    step1_result: str
    step2_result: str
    final_result: str

async def step1(state: WorkflowState) -> dict:
    return {"step1_result": f"Step1 processed: {state['input']}"}

async def step2(state: WorkflowState) -> dict:
    return {"step2_result": f"Step2 processed: {state['step1_result']}"}

async def finalize(state: WorkflowState) -> dict:
    return {"final_result": f"Final: {state['step2_result']}"}

# Build graph imperatively
graph = StateGraph(WorkflowState)

# Add nodes
graph.add_node("step1", step1)
graph.add_node("step2", step2)
graph.add_node("finalize", finalize)

# Add edges (linear flow)
graph.add_edge("step1", "step2")
graph.add_edge("step2", "finalize")
graph.add_edge("finalize", END)

# Set entry point
graph.set_entry_point("step1")

# Compile and run
app = graph.compile()

async def main():
    result = await app.invoke({
        "input": "Hello",
        "step1_result": "",
        "step2_result": "",
        "final_result": ""
    })
    print(result["final_result"])
    # Output: Final: Step2 processed: Step1 processed: Hello

if __name__ == "__main__":
    asyncio.run(main())
```

### With Conditional Routing

```python
from spoon_ai.graph import StateGraph, END
from typing import TypedDict

class RouterState(TypedDict):
    query: str
    category: str
    result: str

async def classify(state: RouterState) -> dict:
    query = state["query"].lower()
    if "price" in query:
        return {"category": "price"}
    elif "news" in query:
        return {"category": "news"}
    return {"category": "general"}

async def handle_price(state: RouterState) -> dict:
    return {"result": f"Price handler: {state['query']}"}

async def handle_news(state: RouterState) -> dict:
    return {"result": f"News handler: {state['query']}"}

async def handle_general(state: RouterState) -> dict:
    return {"result": f"General handler: {state['query']}"}

def route_by_category(state: RouterState) -> str:
    return state.get("category", "general")

# Build graph
graph = StateGraph(RouterState)

graph.add_node("classify", classify)
graph.add_node("price", handle_price)
graph.add_node("news", handle_news)
graph.add_node("general", handle_general)

graph.set_entry_point("classify")

# Conditional routing
graph.add_conditional_edges(
    "classify",
    route_by_category,
    {
        "price": "price",
        "news": "news",
        "general": "general"
    }
)

# All handlers go to END
graph.add_edge("price", END)
graph.add_edge("news", END)
graph.add_edge("general", END)

app = graph.compile()
```

### API Reference

| Method | Description | Example |
|--------|-------------|---------|
| `add_node(name, fn)` | Add a node | `graph.add_node("process", my_fn)` |
| `add_edge(from, to)` | Add static edge | `graph.add_edge("a", "b")` |
| `add_conditional_edges(source, condition, path_map)` | Add conditional routing | See above |
| `set_entry_point(name)` | Set starting node | `graph.set_entry_point("start")` |
| `compile()` | Create executable | `app = graph.compile()` |

---

## Declarative API

Define graphs using template objects. Better for large workflows and team collaboration.

### When to Use

- âœ… Large, complex workflows
- âœ… Team collaboration (serializable templates)
- âœ… Version-controlled graph definitions
- âœ… Parallel execution groups
- âŒ Quick prototypes (more boilerplate)

### Template Components

```python
from spoon_ai.graph.builder import (
    DeclarativeGraphBuilder,
    GraphTemplate,
    NodeSpec,
    EdgeSpec,
    ParallelGroupSpec,
)
from spoon_ai.graph.config import GraphConfig, ParallelGroupConfig
```

| Component | Purpose |
|-----------|---------|
| `NodeSpec` | Define a node with name, function, and optional group |
| `EdgeSpec` | Define an edge between nodes |
| `ParallelGroupSpec` | Group nodes for concurrent execution |
| `GraphTemplate` | Container for all specifications |
| `DeclarativeGraphBuilder` | Builds StateGraph from template |

### Basic Declarative Example

```python
import asyncio
from typing import TypedDict, Dict, Any
from spoon_ai.graph import END
from spoon_ai.graph.builder import (
    DeclarativeGraphBuilder,
    GraphTemplate,
    NodeSpec,
    EdgeSpec,
)
from spoon_ai.graph.config import GraphConfig

class AnalysisState(TypedDict):
    query: str
    analysis: str
    summary: str

async def analyze(state: AnalysisState) -> dict:
    return {"analysis": f"Analysis of: {state['query']}"}

async def summarize(state: AnalysisState) -> dict:
    return {"summary": f"Summary: {state['analysis']}"}

# Define nodes
nodes = [
    NodeSpec("analyze", analyze),
    NodeSpec("summarize", summarize),
]

# Define edges
edges = [
    EdgeSpec("analyze", "summarize"),
    EdgeSpec("summarize", END),
]

# Create template
template = GraphTemplate(
    entry_point="analyze",
    nodes=nodes,
    edges=edges,
    config=GraphConfig(max_iterations=50),
)

# Build graph
builder = DeclarativeGraphBuilder(AnalysisState)
graph = builder.build(template)
app = graph.compile()

async def main():
    result = await app.invoke({
        "query": "Bitcoin trend",
        "analysis": "",
        "summary": ""
    })
    print(result["summary"])

if __name__ == "__main__":
    asyncio.run(main())
```

### With Parallel Groups

```python
from typing import Any, Dict, TypedDict

from spoon_ai.graph import END
from spoon_ai.graph.builder import (
    DeclarativeGraphBuilder,
    GraphTemplate,
    NodeSpec,
    EdgeSpec,
    ParallelGroupSpec,
)
from spoon_ai.graph.config import GraphConfig, ParallelGroupConfig

class DataState(TypedDict):
    symbol: str
    binance_data: Dict[str, Any]
    coinbase_data: Dict[str, Any]
    kraken_data: Dict[str, Any]
    aggregated: Dict[str, Any]

async def fetch_binance(state: DataState) -> dict:
    # Simulated API call
    return {"binance_data": {"source": "binance", "price": 45000}}

async def fetch_coinbase(state: DataState) -> dict:
    return {"coinbase_data": {"source": "coinbase", "price": 45050}}

async def fetch_kraken(state: DataState) -> dict:
    return {"kraken_data": {"source": "kraken", "price": 44980}}

async def aggregate(state: DataState) -> dict:
    prices = [
        state.get("binance_data", {}).get("price", 0),
        state.get("coinbase_data", {}).get("price", 0),
        state.get("kraken_data", {}).get("price", 0),
    ]
    avg_price = sum(prices) / len([p for p in prices if p > 0])
    return {"aggregated": {"average_price": avg_price}}

# Define nodes with parallel group assignment
nodes = [
    NodeSpec("fetch_binance", fetch_binance, parallel_group="data_fetch"),
    NodeSpec("fetch_coinbase", fetch_coinbase, parallel_group="data_fetch"),
    NodeSpec("fetch_kraken", fetch_kraken, parallel_group="data_fetch"),
    NodeSpec("aggregate", aggregate),
]

# Define edges
edges = [
    EdgeSpec("fetch_binance", "aggregate"),
    EdgeSpec("fetch_coinbase", "aggregate"),
    EdgeSpec("fetch_kraken", "aggregate"),
    EdgeSpec("aggregate", END),
]

# Define parallel group
parallel_groups = [
    ParallelGroupSpec(
        name="data_fetch",
        nodes=["fetch_binance", "fetch_coinbase", "fetch_kraken"],
        config=ParallelGroupConfig(
            join_strategy="all",      # Wait for all
            timeout=30.0,             # 30 second timeout
            error_strategy="collect_errors",
        )
    )
]

# Create template
template = GraphTemplate(
    entry_point="fetch_binance",  # Entry to parallel group
    nodes=nodes,
    edges=edges,
    parallel_groups=parallel_groups,
    config=GraphConfig(max_iterations=50),
)

# Build and compile
builder = DeclarativeGraphBuilder(DataState)
graph = builder.build(template)
app = graph.compile()
```

### Template Serialization

One key advantage of declarative templates is serialization:

```python
import json
import tempfile
from pathlib import Path
from typing import TypedDict, List, Dict, Any

from spoon_ai.graph import END
from spoon_ai.graph.builder import GraphTemplate, NodeSpec, EdgeSpec


class DemoState(TypedDict, total=False):
    input: str
    output: str


async def process(state: DemoState) -> dict:
    return {"output": state.get("input", "")}


template = GraphTemplate(
    entry_point="process",
    nodes=[NodeSpec("process", process)],
    edges=[EdgeSpec("process", END)],
)

# Serialize template (for storage/versioning)
template_dict = {
    "entry_point": template.entry_point,
    "nodes": [{"name": n.name, "parallel_group": n.parallel_group} for n in template.nodes],
    "edges": [{"start": e.start, "end": e.end} for e in template.edges],
}

# Save to a temp file (safe for repeated runs)
out_path = Path(tempfile.gettempdir()) / "workflow_template.json"
out_path.write_text(json.dumps(template_dict, indent=2), encoding="utf-8")
print(f"Wrote template: {out_path}")
```

---

## High-Level API

The most advanced approach. Uses an LLM (optional) to infer intent/parameters and helps you build and run a graph per user query.

### When to Use

- âœ… Complex, dynamic workflows
- âœ… LLM-driven decision making
- âœ… Natural language parameter extraction
- âœ… Intelligent agents with adaptive routing
- âŒ Simple, deterministic workflows (overkill)

### Key Components

```python
from spoon_ai.graph.builder import HighLevelGraphAPI, Intent, GraphTemplate, NodeSpec, EdgeSpec
from spoon_ai.graph.mcp_integration import MCPToolSpec
```

| Component | Purpose |
|-----------|---------|
| `HighLevelGraphAPI` | Main interface for intelligent graphs |
| `Intent` | Intent analysis result (`category`, `confidence`, `metadata`) |
| `GraphTemplate` / `NodeSpec` / `EdgeSpec` | Declarative graph definition used to build a `StateGraph` |
| `MCPToolSpec` | Register MCP tools for a given intent category |

### Minimal Example (choose a template by inferred intent)

```python
import asyncio
import json
from typing import Any, Dict, List, TypedDict

from spoon_ai.graph import END
from spoon_ai.graph.builder import HighLevelGraphAPI, Intent, GraphTemplate, NodeSpec, EdgeSpec
from spoon_ai.schema import Message


class AnalysisState(TypedDict, total=False):
    user_query: str
    query_intent: str
    result: str


async def price_handler(state: Dict[str, Any]) -> dict:
    return {"result": f"Price handler (stub): {state['user_query']}"}


async def general_handler(state: Dict[str, Any]) -> dict:
    return {"result": f"General handler (stub): {state['user_query']}"}


def intent_prompt_builder(query: str) -> List[Message]:
    return [
        Message(
            role="system",
            content='Return JSON only: {"category": "price_query|general_qa", "confidence": 0.0-1.0}',
        ),
        Message(role="user", content=query),
    ]


def intent_parser(text: str) -> Dict[str, Any]:
    try:
        return json.loads(text)
    except Exception:
        return {}


def build_template(intent: Intent) -> GraphTemplate:
    if intent.category == "price_query":
        return GraphTemplate(
            entry_point="price_handler",
            nodes=[NodeSpec("price_handler", price_handler)],
            edges=[EdgeSpec("price_handler", END)],
        )
    return GraphTemplate(
        entry_point="general_handler",
        nodes=[NodeSpec("general_handler", general_handler)],
        edges=[EdgeSpec("general_handler", END)],
    )


async def main():
    api = HighLevelGraphAPI(
        AnalysisState,
        intent_prompt_builder=intent_prompt_builder,
        intent_parser=intent_parser,
    )

    intent, state = await api.build_initial_state("What is BTC price?")
    template = build_template(intent)
    graph = api.build_graph(template)
    app = graph.compile()

    result = await app.invoke(state)
    print("intent:", intent.category)
    print("result:", result["result"])


if __name__ == "__main__":
    asyncio.run(main())
```

### Automatic Parameter Inference (optional)

The High-Level API can infer extra parameters and merge them into the initial state **only if** you provide:
- `parameter_prompt_builder(query, intent) -> List[Message]`
- `parameter_parser(text, intent) -> Dict[str, Any]`

```python
# Example:
# User query: "Analyze ETH trend for the past week"
# Your parameter parser could return:
# {"symbol": "ETH", "timeframe": "1w"}
```

### Integration with MCP Tools (optional)

```python
from typing import TypedDict

from spoon_ai.graph.builder import HighLevelGraphAPI
from spoon_ai.graph.mcp_integration import MCPToolSpec


class MyState(TypedDict, total=False):
    user_query: str


api = HighLevelGraphAPI(MyState)

api.register_mcp_tool(
    intent_category="research",
    spec=MCPToolSpec(name="tavily-search"),
    config={
        "command": "npx",
        "args": ["--yes", "tavily-mcp"],
        "env": {"TAVILY_API_KEY": "..."},
    },
)

tool = api.create_mcp_tool("tavily-search")
```

---

## Best Practices

### 1. Start Simple, Scale Up

```python
# Start with imperative for prototyping
from typing import TypedDict

from spoon_ai.graph import StateGraph, END
from spoon_ai.graph.builder import GraphTemplate, NodeSpec, EdgeSpec


class MyState(TypedDict, total=False):
    input: str
    output: str


async def process_fn(state: MyState) -> dict:
    return {"output": f"processed: {state.get('input', '')}"}


graph = StateGraph(MyState)
graph.add_node("process", process_fn)
graph.set_entry_point("process")

# Move to declarative when workflow stabilizes
template = GraphTemplate(
    entry_point="process",
    nodes=[NodeSpec("process", process_fn)],
    edges=[EdgeSpec("process", END)],
)
```

### 2. Use Meaningful Node Names

```python
# Good: Descriptive names
from typing import TypedDict

from spoon_ai.graph import StateGraph


class MyState(TypedDict, total=False):
    user_query: str


async def classify_fn(state: MyState) -> dict:
    return {}


async def fetch_fn(state: MyState) -> dict:
    return {}


async def recommend_fn(state: MyState) -> dict:
    return {}


graph = StateGraph(MyState)

graph.add_node("classify_user_intent", classify_fn)
graph.add_node("fetch_market_data", fetch_fn)
graph.add_node("generate_recommendation", recommend_fn)

# Bad: Generic names
graph.add_node("step1", classify_fn)
graph.add_node("step2", fetch_fn)
graph.add_node("step3", recommend_fn)
```

### 3. Group Related Functionality

```python
# Group data fetching nodes
from spoon_ai.graph.builder import ParallelGroupSpec
from spoon_ai.graph.config import ParallelGroupConfig

parallel_groups = [
    ParallelGroupSpec(
        name="market_data",
        nodes=["fetch_price", "fetch_volume", "fetch_sentiment"],
        config=ParallelGroupConfig(join_strategy="all")
    )
]
print(parallel_groups)
```

### 4. Handle All Routing Cases

```python
# Always have a fallback
from typing import TypedDict

from spoon_ai.graph import StateGraph, END


class RoutingState(TypedDict, total=False):
    route: str
    output: str


async def classifier(state: RoutingState) -> dict:
    # In real graphs, this would set an intent/category based on user input.
    return {"route": state.get("route", "unknown")}


async def handler_1(state: RoutingState) -> dict:
    return {"output": "handled by handler_1"}


async def handler_2(state: RoutingState) -> dict:
    return {"output": "handled by handler_2"}


async def fallback_handler(state: RoutingState) -> dict:
    return {"output": "handled by fallback_handler"}


def route_function(state: RoutingState) -> str:
    return state.get("route", "unknown")


graph = StateGraph(RoutingState)
graph.add_node("classifier", classifier)
graph.add_node("handler_1", handler_1)
graph.add_node("handler_2", handler_2)
graph.add_node("fallback_handler", fallback_handler)
graph.set_entry_point("classifier")

graph.add_conditional_edges(
    "classifier",
    route_function,
    {
        "known_intent_1": "handler_1",
        "known_intent_2": "handler_2",
        "unknown": "fallback_handler"  # Don't forget this!
    }
)

graph.add_edge("handler_1", END)
graph.add_edge("handler_2", END)
graph.add_edge("fallback_handler", END)
```

### 5. Document Your Templates

```python
from typing import TypedDict

from spoon_ai.graph import END
from spoon_ai.graph.builder import GraphTemplate, NodeSpec, EdgeSpec
from spoon_ai.graph.config import GraphConfig


class MyState(TypedDict, total=False):
    input: str
    output: str


async def start(state: MyState) -> dict:
    return {"output": "ok"}


nodes = [NodeSpec("start", start)]
edges = [EdgeSpec("start", END)]

template = GraphTemplate(
    entry_point="start",
    nodes=nodes,
    edges=edges,
    config=GraphConfig(
        max_iterations=100,
        # Document the purpose
        # This graph handles user queries about crypto prices
        # and market analysis with LLM-powered routing
    ),
)
```

---

## Comparison Summary

```mermaid
graph TD
    A[Choose API Style] --> B{Workflow Size?}
    B -->|Small/Medium| C{Need Serialization?}
    B -->|Large/Complex| D[Declarative API]

    C -->|No| E[Imperative API]
    C -->|Yes| D

    A --> F{LLM-Driven?}
    F -->|Yes| G[High-Level API]
    F -->|No| B
```

| Scenario | Recommended API |
|----------|-----------------|
| Learning/prototyping | Imperative |
| Production workflow | Declarative |
| Team collaboration | Declarative |
| Intelligent agent | High-Level |
| Simple automation | Imperative |
| Dynamic routing | High-Level |

## Next Steps

Ready for advanced patterns? Learn about:

**[Advanced Features â†’](./advanced-features.md)** - Routing strategies, parallel execution, human-in-the-loop, and error handling

---

FILE: docs/graph-system/core-concepts.md

---
sidebar_position: 3
title: Core Concepts
description: Understanding State, Nodes, Edges, and Checkpointing
---

# Core Concepts

This guide covers the fundamental building blocks of the SpoonOS Graph System. Master these concepts and you'll be able to build any LLM-powered workflow.

**You will learn:** State, nodes, edges, checkpointing, and merge behavior
**Best for:** Beginners who finished Quick Start
**Time to complete:** ~5â€“8 minutes

## Overview

```mermaid
graph TB
    subgraph Graph["StateGraph"]
        N1[Node A<br/>LLM Call] -->|Edge| N2[Node B<br/>Tool Call]
        N2 -->|Conditional Edge| N3[Node C]
        N2 -->|Conditional Edge| N4[Node D]
    end

    subgraph State["Shared State (TypedDict)"]
        S["query: str<br/>intent: str<br/>llm_response: str<br/>..."]
    end

    subgraph Checkpoint["Checkpointer"]
        CP["State snapshots<br/>for recovery"]
    end

    Graph -.->|reads/writes| State
    Graph -.->|saves| Checkpoint
```

| Concept | Description | Key Point |
|---------|-------------|-----------|
| **State** | Typed dictionary shared across all nodes | Each node reads state, returns updates |
| **Node** | Async function (often calling LLM) | Single responsibility, returns partial update |
| **Edge** | Connection between nodes | Static, conditional, or LLM-driven |
| **Checkpoint** | State snapshot before each node | Enables recovery and human-in-the-loop |

---

## State

**State** is a `TypedDict` that flows through your entire graph. Every node receives the current state and can return updates to merge back.

### Defining State for LLM Workflows

```python
from typing import TypedDict, List, Dict, Any, Optional

class ConversationState(TypedDict):
    # Input fields
    user_query: str
    user_id: str

    # LLM-related fields
    messages: List[dict]           # Chat history for context
    intent: str                    # Classified intent
    extracted_params: Dict[str, Any]  # Parameters extracted by LLM

    # Processing fields
    llm_analysis: str              # LLM's analysis output
    tool_results: Dict[str, Any]   # Results from tool calls

    # Output fields
    final_response: str            # Final response to user
    confidence: float              # LLM's confidence score

    # System fields
    execution_log: List[str]
```

### State Merge Behavior

When a node returns an update, SpoonOS **merges** it into the existing state:

| Field Type | Merge Strategy | Example |
|------------|----------------|---------|
| **Scalar** (str, int, float, bool) | Replace | `"old" â†’ "new"` |
| **dict** | Deep merge | `{a: 1} + {b: 2} â†’ {a: 1, b: 2}` |
| **list** | Append (capped at 100) | `[1, 2] + [3] â†’ [1, 2, 3]` |
| **None** | No change | Field keeps previous value |

```python
import asyncio
import os
from typing import Any, Dict, TypedDict

from spoon_ai.llm import LLMManager
from spoon_ai.schema import Message


class ConversationState(TypedDict, total=False):
    user_query: str
    intent: str
    extracted_params: Dict[str, Any]
    confidence: float


llm = LLMManager()


async def analyze_with_llm(state: ConversationState) -> dict:
    """Example: LLM node returns a partial update."""
    if os.getenv("DOC_SNIPPET_MODE") == "1":
        return {
            "intent": "price_query",
            "extracted_params": {"symbol": "BTC"},
            "confidence": 0.92,
        }

    await llm.chat(
        [
            Message(role="system", content="Analyze user intent and extract parameters."),
            Message(role="user", content=state["user_query"]),
        ],
        max_tokens=80,
    )

    # Only return fields that changed
    return {
        "intent": "price_query",
        "extracted_params": {"symbol": "BTC"},
        "confidence": 0.92,
    }


async def main() -> None:
    print(await analyze_with_llm({"user_query": "BTC price?"}))


if __name__ == "__main__":
    asyncio.run(main())
```

### State Best Practices

:::tip Guidelines
1. **Use TypedDict** - Get IDE autocomplete and type checking
2. **Include messages field** - For multi-turn LLM conversations
3. **Track confidence** - LLM outputs should include confidence scores
4. **Keep it JSON-serializable** - Required for checkpointing
5. **Initialize all fields** - Provide defaults at invoke time
:::

---

## Nodes

**Nodes** are async functions that perform workâ€”typically calling an LLM, executing tools, or processing data.

### Node Contract

```python
import asyncio
import os
from typing import Any, Dict, List, TypedDict

from spoon_ai.llm import LLMManager
from spoon_ai.schema import Message


class MyState(TypedDict, total=False):
    user_query: str
    messages: List[Dict[str, Any]]
    llm_response: str


llm = LLMManager()


async def my_llm_node(state: MyState) -> dict:
    """
    Node function signature:

    Args:
        state: Current graph state (read-only view)

    Returns:
        dict: Fields to update (merged into state)
    """
    # Read from state
    query = state.get("user_query", "")
    messages = state.get("messages", [])  # List of dicts for serialization

    if os.getenv("DOC_SNIPPET_MODE") == "1":
        response_text = f"(stub) response for: {query}"
    else:
        # Convert history to Message objects and call LLM
        history = [Message(role=m["role"], content=m["content"]) for m in messages]
        response = await llm.chat(history + [Message(role="user", content=query)], max_tokens=120)
        response_text = response.content

    # Return updates (partial, not full state)
    # Store messages as dicts for JSON serialization
    return {
        "llm_response": response_text,
        "messages": messages + [
            {"role": "user", "content": query},
            {"role": "assistant", "content": response_text}
        ]
    }


async def main() -> None:
    result = await my_llm_node({"user_query": "hello", "messages": []})
    print(result["llm_response"])


if __name__ == "__main__":
    asyncio.run(main())
```

### Node Patterns for LLM

#### Pattern 1: Intent Classification

```python
import asyncio
import os
from typing import Any, Dict, List, TypedDict

from spoon_ai.llm import LLMManager
from spoon_ai.schema import Message


class ConversationState(TypedDict, total=False):
    user_query: str
    intent: str
    confidence: float
    messages: List[Dict[str, Any]]


llm = LLMManager()


async def classify_intent_node(state: ConversationState) -> dict:
    """Use LLM to classify user intent."""
    if os.getenv("DOC_SNIPPET_MODE") == "1":
        return {"intent": "general_question", "confidence": 0.9}

    response = await llm.chat(
        [
            Message(
                role="system",
                content=(
                    'Respond with JSON only: {"intent": "price_query|analysis_request|trade_command|general_question", '
                    '"confidence": 0.0-1.0}'
                ),
            ),
            Message(role="user", content=state["user_query"]),
        ],
        max_tokens=80,
    )

    import json

    result = json.loads(response.content)
    return {"intent": result["intent"], "confidence": result["confidence"]}


async def main() -> None:
    print(await classify_intent_node({"user_query": "What is BTC price?"}))


if __name__ == "__main__":
    asyncio.run(main())
```

#### Pattern 2: Parameter Extraction

```python
import asyncio
import os
from typing import Any, Dict, TypedDict

from spoon_ai.llm import LLMManager
from spoon_ai.schema import Message


class ConversationState(TypedDict, total=False):
    user_query: str
    extracted_params: Dict[str, Any]


llm = LLMManager()


async def extract_params_node(state: ConversationState) -> dict:
    """Use LLM to extract parameters from natural language."""
    if os.getenv("DOC_SNIPPET_MODE") == "1":
        return {"extracted_params": {"symbol": "BTC", "action": "buy", "amount": 0.1, "price_type": "market"}}

    response = await llm.chat(
        [
            Message(
                role="system",
                content=(
                    'Extract trading parameters as JSON only. Example: '
                    '{"symbol": "BTC", "action": "buy", "amount": 0.1, "price_type": "market"}'
                ),
            ),
            Message(role="user", content=state["user_query"]),
        ],
        max_tokens=120,
    )

    import json

    params = json.loads(response.content)
    return {"extracted_params": params}


async def main() -> None:
    print(await extract_params_node({"user_query": "Buy 0.1 BTC at market"}))


if __name__ == "__main__":
    asyncio.run(main())
```

#### Pattern 3: Analysis with Context

```python
import asyncio
import os
from typing import Any, Dict, List, TypedDict

from spoon_ai.llm import LLMManager
from spoon_ai.schema import Message


class ConversationState(TypedDict, total=False):
    user_query: str
    intent: str
    tool_results: Dict[str, Any]
    messages: List[Dict[str, Any]]
    llm_analysis: str


llm = LLMManager()


async def analyze_with_context_node(state: ConversationState) -> dict:
    """LLM analysis using accumulated context."""
    context = f"""
User Query: {state.get('user_query')}
Intent: {state.get('intent')}
Market Data: {state.get('tool_results', {}).get('market_data', 'N/A')}
Recent Messages: {state.get('messages', [])[-3:]}
"""

    if os.getenv("DOC_SNIPPET_MODE") == "1":
        return {"llm_analysis": f"(stub) analysis for: {state.get('user_query', '')}"}

    response = await llm.chat(
        [
            Message(role="system", content="You are an expert crypto analyst. Provide detailed analysis."),
            Message(role="user", content=context),
        ],
        max_tokens=200,
    )

    return {"llm_analysis": response.content}


async def main() -> None:
    result = await analyze_with_context_node(
        {"user_query": "Analyze BTC", "intent": "analysis_request", "tool_results": {}, "messages": []}
    )
    print(result["llm_analysis"])


if __name__ == "__main__":
    asyncio.run(main())
```

#### Pattern 4: Response Generation

```python
import asyncio
import os
from typing import Any, Dict, List, TypedDict

from spoon_ai.llm import LLMManager
from spoon_ai.schema import Message


class ConversationState(TypedDict, total=False):
    user_query: str
    llm_analysis: str
    tool_results: Dict[str, Any]
    messages: List[Dict[str, Any]]
    final_response: str


llm = LLMManager()


async def generate_response_node(state: ConversationState) -> dict:
    """Generate final user-facing response."""
    if os.getenv("DOC_SNIPPET_MODE") == "1":
        response_text = f"(stub) response for: {state.get('user_query', '')}"
    else:
        response = await llm.chat(
            [
                Message(
                    role="system",
                    content="Generate a helpful, concise response. Be clear and actionable.",
                ),
                Message(
                    role="user",
                    content=(
                        f"Original Query: {state.get('user_query')}\n"
                        f"Analysis: {state.get('llm_analysis')}\n"
                        f"Data: {state.get('tool_results', {})}\n"
                    ),
                ),
            ],
            max_tokens=200,
        )
        response_text = response.content

    return {
        "final_response": response_text,
        "messages": state.get("messages", []) + [{"role": "assistant", "content": response_text}],
    }


async def main() -> None:
    result = await generate_response_node(
        {"user_query": "hello", "llm_analysis": "", "tool_results": {}, "messages": []}
    )
    print(result["final_response"])


if __name__ == "__main__":
    asyncio.run(main())
```

---

## Edges

**Edges** define how control flows between nodes. The Graph System supports multiple edge types.

### 1. Static Edges

Always transition from source to target:

```python
import asyncio
from typing import Any, Dict, List, TypedDict

from spoon_ai.graph import StateGraph, END


class ConversationState(TypedDict, total=False):
    user_query: str
    intent: str
    llm_analysis: str
    final_response: str
    messages: List[Dict[str, Any]]


async def classify_intent_node(state: ConversationState) -> dict:
    return {"intent": "general_question"}


async def analyze_with_context_node(state: ConversationState) -> dict:
    return {"llm_analysis": f"(stub) analysis of: {state.get('user_query', '')}"}


async def generate_response_node(state: ConversationState) -> dict:
    return {"final_response": f"(stub) response: {state.get('llm_analysis', '')}"}


graph = StateGraph(ConversationState)
graph.add_node("classify", classify_intent_node)
graph.add_node("analyze", analyze_with_context_node)
graph.add_node("respond", generate_response_node)

graph.set_entry_point("classify")
graph.add_edge("classify", "analyze")
graph.add_edge("analyze", "respond")
graph.add_edge("respond", END)

app = graph.compile()


async def main() -> None:
    result = await app.invoke({"user_query": "Explain Bitcoin"})
    print(result["final_response"])


if __name__ == "__main__":
    asyncio.run(main())
```

### 2. Conditional Edges (LLM-Driven Routing)

Route based on LLM classification:

```python
import asyncio
from typing import TypedDict

from spoon_ai.graph import StateGraph, END


class ConversationState(TypedDict, total=False):
    user_query: str
    intent: str
    confidence: float
    output: str


def route_by_intent(state: ConversationState) -> str:
    """Route based on LLM-classified intent."""
    intent = state.get("intent", "general")
    confidence = state.get("confidence", 0)

    # Low confidence â†’ ask for clarification
    if confidence < 0.7:
        return "clarify"

    return intent


async def classify(state: ConversationState) -> dict:
    q = (state.get("user_query") or "").lower()
    if "price" in q:
        return {"intent": "price_query", "confidence": 0.95}
    if "analy" in q:
        return {"intent": "analysis_request", "confidence": 0.9}
    if "buy" in q or "sell" in q:
        return {"intent": "trade_command", "confidence": 0.9}
    return {"intent": "general_question", "confidence": 0.9}


async def fetch_price(state: ConversationState) -> dict:
    return {"output": "price handler"}


async def deep_analysis(state: ConversationState) -> dict:
    return {"output": "analysis handler"}


async def confirm_trade(state: ConversationState) -> dict:
    return {"output": "trade confirmation handler"}


async def general_response(state: ConversationState) -> dict:
    return {"output": "general handler"}


async def ask_clarification(state: ConversationState) -> dict:
    return {"output": "clarification handler"}


graph = StateGraph(ConversationState)
graph.add_node("classify", classify)
graph.add_node("fetch_price", fetch_price)
graph.add_node("deep_analysis", deep_analysis)
graph.add_node("confirm_trade", confirm_trade)
graph.add_node("general_response", general_response)
graph.add_node("ask_clarification", ask_clarification)
graph.set_entry_point("classify")

graph.add_conditional_edges(
    "classify",
    route_by_intent,
    {
        "price_query": "fetch_price",
        "analysis_request": "deep_analysis",
        "trade_command": "confirm_trade",
        "general_question": "general_response",
        "clarify": "ask_clarification",
    },
)

for node in ["fetch_price", "deep_analysis", "confirm_trade", "general_response", "ask_clarification"]:
    graph.add_edge(node, END)

app = graph.compile()


async def main() -> None:
    result = await app.invoke({"user_query": "price of BTC?"})
    print(result["output"])


if __name__ == "__main__":
    asyncio.run(main())
```

```mermaid
graph TD
    A[Classify Intent<br/>LLM] -->|price_query| B[Fetch Price]
    A -->|analysis_request| C[Deep Analysis<br/>LLM]
    A -->|trade_command| D[Confirm Trade]
    A -->|general_question| E[General Response<br/>LLM]
    A -->|low confidence| F[Ask Clarification<br/>LLM]
```

### 3. LLM-Powered Routing (Dynamic)

Let the LLM itself decide the next step. The simplest way is to enable the built-in LLM router and let it select the next node name.

```python
import asyncio
from typing import TypedDict

from spoon_ai.graph import StateGraph, END
from spoon_ai.graph.config import GraphConfig, RouterConfig


class ConversationState(TypedDict, total=False):
    user_query: str
    result: str


async def route(state: ConversationState) -> dict:
    # No-op entry node. LLM routing runs after this node.
    return state


async def web_search(state: ConversationState) -> dict:
    return {"result": f"(stub) web search: {state['user_query']}"}


async def respond(state: ConversationState) -> dict:
    return {"result": f"(stub) response: {state['user_query']}"}


graph = StateGraph(ConversationState)
graph.add_node("route", route)
graph.add_node("web_search", web_search)
graph.add_node("respond", respond)
graph.set_entry_point("route")

# Once a handler runs, end the graph.
graph.add_edge("web_search", END)
graph.add_edge("respond", END)

# Enable LLM routing and restrict targets.
graph.config = GraphConfig(
    router=RouterConfig(
        allow_llm=True,
        allowed_targets=["web_search", "respond"],
        default_target="respond",
    )
)
graph.enable_llm_routing(config={"model": "gpt-4", "temperature": 0.1, "max_tokens": 50})

app = graph.compile()


async def main():
    result = await app.invoke({"user_query": "Any BTC news today?", "result": ""})
    print(result["result"])


if __name__ == "__main__":
    asyncio.run(main())
```

### Complete LLM Routing Example

```python
import asyncio
from typing import TypedDict
from spoon_ai.graph import StateGraph, END
from spoon_ai.llm import LLMManager
from spoon_ai.schema import Message

class RouterState(TypedDict):
    query: str
    intent: str
    confidence: float
    result: str

llm = LLMManager()

async def classify_intent(state: RouterState) -> dict:
    """LLM classifies intent with confidence."""
    response = await llm.chat([
        Message(role="system", content="""Classify and respond with JSON:
        {"intent": "price|news|analysis|general", "confidence": 0.0-1.0}"""),
        Message(role="user", content=state["query"])
    ])

    import json
    result = json.loads(response.content)
    return {"intent": result["intent"], "confidence": result["confidence"]}

async def handle_price(state: RouterState) -> dict:
    response = await llm.chat([
        Message(role="system", content="Provide cryptocurrency price information."),
        Message(role="user", content=state["query"])
    ])
    return {"result": response.content}

async def handle_news(state: RouterState) -> dict:
    response = await llm.chat([
        Message(role="system", content="Summarize relevant crypto news."),
        Message(role="user", content=state["query"])
    ])
    return {"result": response.content}

async def handle_analysis(state: RouterState) -> dict:
    response = await llm.chat([
        Message(role="system", content="Provide detailed market analysis."),
        Message(role="user", content=state["query"])
    ])
    return {"result": response.content}

async def handle_general(state: RouterState) -> dict:
    response = await llm.chat([
        Message(role="system", content="You are a helpful crypto assistant."),
        Message(role="user", content=state["query"])
    ])
    return {"result": response.content}

def route_by_intent(state: RouterState) -> str:
    return state.get("intent", "general")

# Build graph
graph = StateGraph(RouterState)

graph.add_node("classify", classify_intent)
graph.add_node("price_handler", handle_price)
graph.add_node("news_handler", handle_news)
graph.add_node("analysis_handler", handle_analysis)
graph.add_node("general_handler", handle_general)

graph.set_entry_point("classify")

graph.add_conditional_edges(
    "classify",
    route_by_intent,
    {
        "price": "price_handler",
        "news": "news_handler",
        "analysis": "analysis_handler",
        "general": "general_handler"
    }
)

graph.add_edge("price_handler", END)
graph.add_edge("news_handler", END)
graph.add_edge("analysis_handler", END)
graph.add_edge("general_handler", END)

app = graph.compile()


async def main():
    result = await app.invoke(
        {"query": "What is the current price of Bitcoin?", "intent": "", "confidence": 0.0, "result": ""}
    )
    print(result["result"])


if __name__ == "__main__":
    asyncio.run(main())
```

---

## Checkpointing

**Checkpointing** automatically saves state snapshots before each node execution. This enables:

- **Recovery**: Resume from last successful node after failure
- **Multi-turn conversations**: Maintain LLM context across sessions
- **Human-in-the-loop**: Pause for user input, resume with new data

### Configuring Checkpointing

```python
from typing import Any, Dict, List, TypedDict

from spoon_ai.graph import StateGraph, InMemoryCheckpointer

checkpointer = InMemoryCheckpointer(
    max_checkpoints_per_thread=100
)

class ConversationState(TypedDict, total=False):
    user_query: str
    messages: List[Dict[str, Any]]
    llm_analysis: str

graph = StateGraph(
    ConversationState,
    checkpointer=checkpointer
)
print("checkpointer configured:", graph.checkpointer is checkpointer)
```

### Multi-Turn LLM Conversations

```python
import asyncio

from typing import Any, Dict, List, TypedDict

from spoon_ai.graph import StateGraph, END, InMemoryCheckpointer


class ConversationState(TypedDict, total=False):
    user_query: str
    messages: List[Dict[str, Any]]
    llm_response: str


checkpointer = InMemoryCheckpointer(max_checkpoints_per_thread=100)


async def respond(state: ConversationState) -> dict:
    # Minimal, deterministic "LLM" for docs.
    user_query = state.get("user_query", "")
    response_text = f"(stub) answer to: {user_query}"
    messages = state.get("messages", [])
    return {
        "llm_response": response_text,
        "messages": messages
        + [{"role": "user", "content": user_query}, {"role": "assistant", "content": response_text}],
    }


graph = StateGraph(ConversationState, checkpointer=checkpointer)
graph.add_node("respond", respond)
graph.set_entry_point("respond")
graph.add_edge("respond", END)
app = graph.compile()


async def main() -> None:
    # First turn
    result = await app.invoke(
        {"user_query": "What is Bitcoin?", "messages": []},
        config={"configurable": {"thread_id": "user_123_session"}},
    )

    # Second turn - LLM has context from first turn
    result = await app.invoke(
        {"user_query": "What about its price trend?", "messages": result["messages"]},
        config={"configurable": {"thread_id": "user_123_session"}},
    )

    # The LLM knows "its" refers to Bitcoin from the conversation history
    print(result)


if __name__ == "__main__":
    asyncio.run(main())
```

### Recovery from Failure

```python
import asyncio
from typing import TypedDict

from spoon_ai.graph import StateGraph, END, InMemoryCheckpointer


class ConversationState(TypedDict, total=False):
    user_query: str
    llm_analysis: str
    should_fail: bool


checkpointer = InMemoryCheckpointer(max_checkpoints_per_thread=100)


async def maybe_fail(state: ConversationState) -> dict:
    if state.get("should_fail"):
        raise RuntimeError("simulated failure")
    return {"llm_analysis": f"(stub) analysis for: {state.get('user_query', '')}"}


graph = StateGraph(ConversationState, checkpointer=checkpointer)
graph.add_node("maybe_fail", maybe_fail)
graph.set_entry_point("maybe_fail")
graph.add_edge("maybe_fail", END)
app = graph.compile()


async def main() -> None:
    config = {"configurable": {"thread_id": "analysis_session"}}
    try:
        initial_state = {"user_query": "Analyze BTC", "llm_analysis": "", "should_fail": True}
        result = await app.invoke(initial_state, config=config)
        print(result)
    except Exception as e:
        print(f"Failed: {e}")

        # Get last successful state
        last_state = graph.get_state(config)

        if last_state:
            print(f"Last node: {last_state.metadata.get('node')}")
            print(f"Checkpoint values: {last_state.values}")


if __name__ == "__main__":
    asyncio.run(main())
```

---

## Key Takeaways

1. **State carries LLM context** - Messages, extracted params, analysis results flow through
2. **Nodes encapsulate LLM calls** - Each node does one LLM task well
3. **Edges route based on LLM output** - Intent classification drives workflow
4. **Checkpoints enable conversations** - Multi-turn context preserved across calls

## Next Steps

Learn how to construct graphs using different API styles:

**[Building Graphs â†’](./building-graphs.md)** - Imperative, Declarative, and High-Level APIs

---

FILE: docs/graph-system/examples.md

---
sidebar_position: 7
title: Practical Examples
description: Complete working examples demonstrating LLM-powered Graph patterns
---

# Practical Examples

This page provides complete, runnable examples demonstrating key Graph System patterns with LLM integration. Each example shows real LLM calls, not fixed outputs.

## Example 1: LLM Intent Router

Route user queries to specialized LLM handlers based on intent classification.

```python
"""
LLM Intent Router Example

Demonstrates:
- LLM-based intent classification
- Conditional routing based on LLM output
- Specialized handler nodes for different intents
"""

import asyncio
import json
from typing import TypedDict
from spoon_ai.graph import StateGraph, END
from spoon_ai.llm import LLMManager
from spoon_ai.schema import Message


class RouterState(TypedDict):
    query: str
    intent: str
    confidence: float
    result: str


llm = LLMManager()


async def classify_intent(state: RouterState) -> dict:
    """LLM classifies the user's intent with confidence score."""
    response = await llm.chat([
        Message(role="system", content="""You are an intent classifier for a crypto assistant.
        Classify the user query into one of these categories:
        - price: asking about cryptocurrency prices
        - news: asking about crypto news or updates
        - analysis: requesting market analysis or trends
        - general: other questions

        Respond with JSON only: {"intent": "category", "confidence": 0.0-1.0}"""),
        Message(role="user", content=state["query"])
    ])

    try:
        result = json.loads(response.content)
        return {
            "intent": result.get("intent", "general"),
            "confidence": result.get("confidence", 0.5)
        }
    except json.JSONDecodeError:
        return {"intent": "general", "confidence": 0.5}


async def handle_price(state: RouterState) -> dict:
    """LLM generates price-related response."""
    response = await llm.chat([
        Message(role="system", content="""You are a crypto price expert.
        Provide helpful information about cryptocurrency prices.
        Be concise and include relevant data points."""),
        Message(role="user", content=state["query"])
    ])
    return {"result": response.content}


async def handle_news(state: RouterState) -> dict:
    """LLM generates news-related response."""
    response = await llm.chat([
        Message(role="system", content="""You are a crypto news analyst.
        Summarize relevant cryptocurrency news and updates.
        Focus on recent developments and their implications."""),
        Message(role="user", content=state["query"])
    ])
    return {"result": response.content}


async def handle_analysis(state: RouterState) -> dict:
    """LLM generates market analysis."""
    response = await llm.chat([
        Message(role="system", content="""You are a crypto market analyst.
        Provide detailed market analysis including:
        - Current trends
        - Technical indicators
        - Risk assessment
        Be analytical and data-driven."""),
        Message(role="user", content=state["query"])
    ])
    return {"result": response.content}


async def handle_general(state: RouterState) -> dict:
    """LLM handles general questions."""
    response = await llm.chat([
        Message(role="system", content="""You are a helpful crypto assistant.
        Answer questions clearly and accurately.
        If you're unsure, say so."""),
        Message(role="user", content=state["query"])
    ])
    return {"result": response.content}


def route_by_intent(state: RouterState) -> str:
    """Route based on classified intent."""
    return state.get("intent", "general")


# Build the graph
graph = StateGraph(RouterState)

graph.add_node("classify", classify_intent)
graph.add_node("price_handler", handle_price)
graph.add_node("news_handler", handle_news)
graph.add_node("analysis_handler", handle_analysis)
graph.add_node("general_handler", handle_general)

graph.set_entry_point("classify")

graph.add_conditional_edges(
    "classify",
    route_by_intent,
    {
        "price": "price_handler",
        "news": "news_handler",
        "analysis": "analysis_handler",
        "general": "general_handler",
    }
)

graph.add_edge("price_handler", END)
graph.add_edge("news_handler", END)
graph.add_edge("analysis_handler", END)
graph.add_edge("general_handler", END)

app = graph.compile()


async def main():
    """Test the LLM router with different queries."""
    test_queries = [
        "What is the current price of Bitcoin?",
        "Any news about Ethereum updates?",
        "Analyze the SOL market trend",
        "What is a blockchain?",
    ]

    for query in test_queries:
        print(f"\n{'='*60}")
        print(f"Query: {query}")
        print('='*60)

        result = await app.invoke({
            "query": query,
            "intent": "",
            "confidence": 0.0,
            "result": ""
        })

        print(f"Intent: {result['intent']} (confidence: {result['confidence']:.0%})")
        print(f"\nResponse:\n{result['result']}")

    return True


if __name__ == "__main__":
    asyncio.run(main())
```

**Key Learnings:**
- LLM classifies intent and returns structured JSON
- Confidence score enables quality-based routing
- Specialized handlers provide better responses

---

## Example 2: Multi-Step Analysis Pipeline

Chain multiple LLM calls for deep analysis with context accumulation.

```python
"""
Multi-Step LLM Analysis Pipeline

Demonstrates:
- Sequential LLM calls with context building
- State accumulation across nodes
- Progressive refinement of analysis
"""

import asyncio
from typing import TypedDict, List, Dict, Any
from spoon_ai.graph import StateGraph, END
from spoon_ai.llm import LLMManager
from spoon_ai.schema import Message


class AnalysisState(TypedDict):
    symbol: str
    user_question: str
    market_context: str
    technical_analysis: str
    risk_assessment: str
    final_recommendation: str
    confidence: float


llm = LLMManager()


async def gather_context(state: AnalysisState) -> dict:
    """LLM gathers market context for the symbol."""
    response = await llm.chat([
        Message(role="system", content="""You are a market context specialist.
        Provide relevant market context for the cryptocurrency including:
        - Current market sentiment
        - Recent price movements
        - Key events affecting the asset
        Be factual and concise."""),
        Message(role="user", content=f"Provide market context for {state['symbol']}")
    ])
    return {"market_context": response.content}


async def analyze_technicals(state: AnalysisState) -> dict:
    """LLM provides technical analysis based on context."""
    response = await llm.chat([
        Message(role="system", content="""You are a technical analyst.
        Based on the market context, provide technical analysis including:
        - Support and resistance levels
        - Key indicators (RSI, MACD trends)
        - Chart patterns
        Be specific and analytical."""),
        Message(role="user", content=f"""
        Symbol: {state['symbol']}
        Market Context: {state['market_context']}

        Provide technical analysis:""")
    ])
    return {"technical_analysis": response.content}


async def assess_risk(state: AnalysisState) -> dict:
    """LLM assesses risk based on all gathered information."""
    response = await llm.chat([
        Message(role="system", content="""You are a risk assessment specialist.
        Based on the context and technical analysis, assess:
        - Risk level (Low/Medium/High)
        - Key risk factors
        - Potential downside scenarios
        Also provide a confidence score (0-100) for your assessment.

        End your response with: Confidence: XX%"""),
        Message(role="user", content=f"""
        Symbol: {state['symbol']}
        Market Context: {state['market_context']}
        Technical Analysis: {state['technical_analysis']}

        Provide risk assessment:""")
    ])

    # Extract confidence from response
    content = response.content
    confidence = 0.7  # default
    if "Confidence:" in content:
        try:
            conf_str = content.split("Confidence:")[-1].strip().replace("%", "")
            confidence = float(conf_str) / 100
        except:
            pass

    return {
        "risk_assessment": content,
        "confidence": confidence
    }


async def generate_recommendation(state: AnalysisState) -> dict:
    """LLM generates final recommendation."""
    response = await llm.chat([
        Message(role="system", content="""You are a senior investment advisor.
        Based on all the analysis, provide a clear recommendation.
        Structure your response as:
        1. Summary recommendation (Buy/Hold/Sell)
        2. Key reasoning points
        3. Suggested actions
        4. Caveats and disclaimers"""),
        Message(role="user", content=f"""
        User Question: {state['user_question']}
        Symbol: {state['symbol']}

        Analysis Summary:
        - Market Context: {state['market_context'][:500]}...
        - Technical: {state['technical_analysis'][:500]}...
        - Risk: {state['risk_assessment'][:500]}...
        - Confidence: {state['confidence']:.0%}

        Generate final recommendation:""")
    ])
    return {"final_recommendation": response.content}


# Build graph: context -> technical -> risk -> recommendation
graph = StateGraph(AnalysisState)

graph.add_node("gather_context", gather_context)
graph.add_node("analyze_technicals", analyze_technicals)
graph.add_node("assess_risk", assess_risk)
graph.add_node("generate_recommendation", generate_recommendation)

graph.set_entry_point("gather_context")
graph.add_edge("gather_context", "analyze_technicals")
graph.add_edge("analyze_technicals", "assess_risk")
graph.add_edge("assess_risk", "generate_recommendation")
graph.add_edge("generate_recommendation", END)

app = graph.compile()


async def main():
    """Run the analysis pipeline."""
    print("="*60)
    print("MULTI-STEP LLM ANALYSIS PIPELINE")
    print("="*60)

    result = await app.invoke({
        "symbol": "BTC",
        "user_question": "Should I buy Bitcoin now?",
        "market_context": "",
        "technical_analysis": "",
        "risk_assessment": "",
        "final_recommendation": "",
        "confidence": 0.0
    })

    print("\nðŸ“Š MARKET CONTEXT:")
    print("-"*40)
    print(result["market_context"][:500] + "...")

    print("\nðŸ“ˆ TECHNICAL ANALYSIS:")
    print("-"*40)
    print(result["technical_analysis"][:500] + "...")

    print("\nâš ï¸ RISK ASSESSMENT:")
    print("-"*40)
    print(result["risk_assessment"][:500] + "...")

    print(f"\nðŸŽ¯ CONFIDENCE: {result['confidence']:.0%}")

    print("\nðŸ’¡ FINAL RECOMMENDATION:")
    print("-"*40)
    print(result["final_recommendation"])

    return True


if __name__ == "__main__":
    asyncio.run(main())
```

**Key Learnings:**
- Each LLM node builds on previous outputs
- Context accumulates through the pipeline
- Final recommendation incorporates all analysis

---

## Example 3: LLM-Powered Human-in-the-Loop

Pause for human approval with LLM-generated summaries.

```python
"""
LLM-Powered Human Approval Workflow

Demonstrates:
- LLM generates approval summary
- Interrupt for human decision
- Resume with user input
"""

import asyncio
from typing import TypedDict, Optional, Dict, Any
from spoon_ai.graph import StateGraph, END, interrupt, Command
from spoon_ai.graph import InMemoryCheckpointer
from spoon_ai.llm import LLMManager
from spoon_ai.schema import Message


class ApprovalState(TypedDict):
    request_type: str
    request_details: Dict[str, Any]
    llm_summary: str
    risk_level: str
    user_approved: Optional[bool]
    execution_result: str


llm = LLMManager()


async def analyze_request(state: ApprovalState) -> dict:
    """LLM analyzes the request and generates summary."""
    response = await llm.chat([
        Message(role="system", content="""You are a request analyzer.
        Analyze the request and provide:
        1. A clear summary of what will happen
        2. Risk level: LOW, MEDIUM, or HIGH
        3. Key points to consider

        Format your response as:
        SUMMARY: [summary]
        RISK: [LOW/MEDIUM/HIGH]
        CONSIDERATIONS: [key points]"""),
        Message(role="user", content=f"""
        Request Type: {state['request_type']}
        Details: {state['request_details']}

        Analyze this request:""")
    ])

    content = response.content
    risk_level = "MEDIUM"  # default
    if "RISK: LOW" in content:
        risk_level = "LOW"
    elif "RISK: HIGH" in content:
        risk_level = "HIGH"

    return {
        "llm_summary": content,
        "risk_level": risk_level
    }


async def request_approval(state: ApprovalState) -> dict:
    """Interrupt for user approval."""
    if state.get("user_approved") is not None:
        return {}  # Already have decision

    interrupt({
        "type": "approval_required",
        "summary": state["llm_summary"],
        "risk_level": state["risk_level"],
        "request_type": state["request_type"],
        "details": state["request_details"],
        "requires_response": ["user_approved"]
    })
    return {}


async def execute_request(state: ApprovalState) -> dict:
    """LLM generates execution result based on approval."""
    if state.get("user_approved"):
        response = await llm.chat([
            Message(role="system", content="Generate a confirmation message for the executed request."),
            Message(role="user", content=f"""
            Request Type: {state['request_type']}
            Details: {state['request_details']}

            The user approved this request. Generate execution confirmation:""")
        ])
        return {"execution_result": f"âœ… APPROVED\n{response.content}"}
    else:
        response = await llm.chat([
            Message(role="system", content="Generate a professional rejection acknowledgment."),
            Message(role="user", content=f"The user rejected the {state['request_type']} request.")
        ])
        return {"execution_result": f"âŒ REJECTED\n{response.content}"}


# Build graph with checkpointer
checkpointer = InMemoryCheckpointer()
graph = StateGraph(ApprovalState, checkpointer=checkpointer)

graph.add_node("analyze", analyze_request)
graph.add_node("approve", request_approval)
graph.add_node("execute", execute_request)

graph.set_entry_point("analyze")
graph.add_edge("analyze", "approve")
graph.add_edge("approve", "execute")
graph.add_edge("execute", END)

app = graph.compile()


async def main():
    """Test the approval workflow."""
    test_requests = [
        {"type": "transfer", "details": {"amount": 500, "to": "wallet_abc", "currency": "USDT"}},
        {"type": "trade", "details": {"action": "buy", "amount": 0.5, "symbol": "ETH", "price": "market"}},
    ]

    for i, request in enumerate(test_requests):
        print(f"\n{'#'*60}")
        print(f"Processing Request {i + 1}: {request['type']}")
        print('#'*60)

        session_id = f"approval_session_{i}"

        # Initial invocation - will get LLM analysis then interrupt
        result = await app.invoke(
            {
                "request_type": request["type"],
                "request_details": request["details"],
                "llm_summary": "",
                "risk_level": "",
                "user_approved": None,
                "execution_result": ""
            },
            config={"configurable": {"thread_id": session_id}}
        )

        # Check for interrupt
        if "__interrupt__" in result:
            interrupt_data = result["__interrupt__"][0]["value"]

            print("\nðŸ”” APPROVAL REQUIRED")
            print("="*50)
            print(f"Risk Level: {interrupt_data['risk_level']}")
            print(f"\nLLM Analysis:\n{interrupt_data['summary']}")
            print("="*50)

            # Simulate user decision based on risk
            approved = interrupt_data["risk_level"] != "HIGH"
            print(f"\nSimulated Decision: {'APPROVED' if approved else 'REJECTED'}")

            # Resume with decision
            result = await app.invoke(
                Command(resume={"user_approved": approved}),
                config={"configurable": {"thread_id": session_id}}
            )

        print(f"\nðŸ“‹ Final Result:\n{result.get('execution_result', 'Unknown')}")

    return True


if __name__ == "__main__":
    asyncio.run(main())
```

**Key Learnings:**
- LLM generates human-readable summaries for approval
- Risk assessment helps users make informed decisions
- Interrupt/resume pattern enables async human interaction

---

## Example 4: Parallel LLM Analysis

Execute multiple LLM calls in parallel and aggregate results.

```python
"""
Parallel LLM Analysis Example

Demonstrates:
- Multiple LLM calls running concurrently
- Different analytical perspectives
- Result aggregation with LLM
"""

import asyncio
from typing import TypedDict, Dict, Any
from spoon_ai.graph import StateGraph, END
from spoon_ai.graph.config import ParallelGroupConfig
from spoon_ai.llm import LLMManager
from spoon_ai.schema import Message


class ParallelAnalysisState(TypedDict):
    symbol: str
    query: str
    bullish_view: str
    bearish_view: str
    neutral_view: str
    aggregated_analysis: str


llm = LLMManager()


async def bullish_analyst(state: ParallelAnalysisState) -> dict:
    """LLM analyzes from bullish perspective."""
    response = await llm.chat([
        Message(role="system", content="""You are a bullish crypto analyst.
        Find and present the positive aspects and upside potential.
        Focus on growth catalysts, adoption metrics, and bullish indicators.
        Be optimistic but grounded in facts."""),
        Message(role="user", content=f"Analyze {state['symbol']}: {state['query']}")
    ])
    return {"bullish_view": response.content}


async def bearish_analyst(state: ParallelAnalysisState) -> dict:
    """LLM analyzes from bearish perspective."""
    response = await llm.chat([
        Message(role="system", content="""You are a bearish crypto analyst.
        Find and present the risks and downside potential.
        Focus on threats, competition, and bearish indicators.
        Be cautious but fair in your assessment."""),
        Message(role="user", content=f"Analyze {state['symbol']}: {state['query']}")
    ])
    return {"bearish_view": response.content}


async def neutral_analyst(state: ParallelAnalysisState) -> dict:
    """LLM provides balanced neutral analysis."""
    response = await llm.chat([
        Message(role="system", content="""You are a neutral crypto analyst.
        Provide balanced analysis considering both sides.
        Focus on factual data and objective metrics.
        Present pros and cons equally."""),
        Message(role="user", content=f"Analyze {state['symbol']}: {state['query']}")
    ])
    return {"neutral_view": response.content}


async def aggregate_analysis(state: ParallelAnalysisState) -> dict:
    """LLM aggregates all perspectives into final analysis."""
    response = await llm.chat([
        Message(role="system", content="""You are a senior analyst aggregating multiple perspectives.
        Synthesize the bullish, bearish, and neutral views into a comprehensive analysis.
        Structure your response:
        1. Executive Summary
        2. Key Bullish Points
        3. Key Bearish Points
        4. Balanced Assessment
        5. Final Verdict"""),
        Message(role="user", content=f"""
        Symbol: {state['symbol']}
        Query: {state['query']}

        BULLISH VIEW:
        {state['bullish_view']}

        BEARISH VIEW:
        {state['bearish_view']}

        NEUTRAL VIEW:
        {state['neutral_view']}

        Synthesize these perspectives:""")
    ])
    return {"aggregated_analysis": response.content}


# Build graph with parallel execution
graph = StateGraph(ParallelAnalysisState)

graph.add_node("bullish", bullish_analyst)
graph.add_node("bearish", bearish_analyst)
graph.add_node("neutral", neutral_analyst)
graph.add_node("aggregate", aggregate_analysis)

# Configure parallel group
graph.add_parallel_group(
    "perspectives",
    nodes=["bullish", "bearish", "neutral"],
    config=ParallelGroupConfig(
        join_strategy="all",
        timeout=60.0,
    )
)

# All parallel nodes converge to aggregate
graph.add_edge("bullish", "aggregate")
graph.add_edge("bearish", "aggregate")
graph.add_edge("neutral", "aggregate")
graph.add_edge("aggregate", END)

graph.set_entry_point("bullish")
app = graph.compile()


async def main():
    """Run parallel LLM analysis."""
    print("="*60)
    print("PARALLEL LLM ANALYSIS")
    print("="*60)

    result = await app.invoke({
        "symbol": "ETH",
        "query": "What's the outlook for Ethereum in the next year?",
        "bullish_view": "",
        "bearish_view": "",
        "neutral_view": "",
        "aggregated_analysis": ""
    })

    print("\nðŸ‚ BULLISH VIEW:")
    print("-"*40)
    print(result["bullish_view"][:400] + "...")

    print("\nðŸ» BEARISH VIEW:")
    print("-"*40)
    print(result["bearish_view"][:400] + "...")

    print("\nâš–ï¸ NEUTRAL VIEW:")
    print("-"*40)
    print(result["neutral_view"][:400] + "...")

    print("\nðŸ“Š AGGREGATED ANALYSIS:")
    print("-"*40)
    print(result["aggregated_analysis"])

    return True


if __name__ == "__main__":
    asyncio.run(main())
```

**Key Learnings:**
- Multiple LLM calls run in parallel for speed
- Different "personas" provide varied perspectives
- Aggregation LLM synthesizes diverse inputs

---

## Example 5: Conversational Agent with Memory

Multi-turn conversation with context preservation.

```python
"""
Conversational LLM Agent with Memory

Demonstrates:
- Multi-turn conversation state
- Message history accumulation
- Context-aware responses
"""

import asyncio
from typing import TypedDict, List
from spoon_ai.graph import StateGraph, END, InMemoryCheckpointer
from spoon_ai.llm import LLMManager
from spoon_ai.schema import Message


class ConversationState(TypedDict):
    messages: List[dict]  # Store as dicts for serialization
    user_input: str
    assistant_response: str
    turn_count: int


llm = LLMManager()


async def process_message(state: ConversationState) -> dict:
    """LLM processes message with full conversation history."""
    messages = state.get("messages", [])
    user_input = state["user_input"]

    # Build conversation with history (convert dicts to Message objects)
    conversation = [
        Message(role="system", content="""You are a helpful crypto trading assistant.
        You have memory of the entire conversation.
        Reference previous messages when relevant.
        Be conversational and helpful.""")
    ] + [
        Message(role=m["role"], content=m["content"]) for m in messages
    ] + [
        Message(role="user", content=user_input)
    ]

    response = await llm.chat(conversation)

    # Update message history (store as dicts for JSON serialization)
    new_messages = messages + [
        {"role": "user", "content": user_input},
        {"role": "assistant", "content": response.content}
    ]

    return {
        "assistant_response": response.content,
        "messages": new_messages,
        "turn_count": state.get("turn_count", 0) + 1
    }


# Build graph
checkpointer = InMemoryCheckpointer()
graph = StateGraph(ConversationState, checkpointer=checkpointer)

graph.add_node("chat", process_message)
graph.set_entry_point("chat")
graph.add_edge("chat", END)

app = graph.compile()


async def main():
    """Simulate multi-turn conversation."""
    print("="*60)
    print("CONVERSATIONAL LLM AGENT WITH MEMORY")
    print("="*60)

    session_id = "conversation_demo"

    # Conversation turns
    turns = [
        "Hi! I'm interested in Bitcoin.",
        "What's its current market situation?",
        "You mentioned Bitcoin earlier - what about Ethereum compared to it?",
        "Based on our conversation, what would you recommend for a beginner?",
    ]

    state = {
        "messages": [],
        "user_input": "",
        "assistant_response": "",
        "turn_count": 0
    }

    for turn in turns:
        print(f"\nðŸ‘¤ User: {turn}")

        state["user_input"] = turn
        result = await app.invoke(
            state,
            config={"configurable": {"thread_id": session_id}}
        )

        print(f"\nðŸ¤– Assistant: {result['assistant_response']}")
        print(f"   [Turn {result['turn_count']}, History: {len(result['messages'])} messages]")

        # Preserve state for next turn
        state = result

    return True


if __name__ == "__main__":
    asyncio.run(main())
```

**Key Learnings:**
- Message history enables context-aware responses
- LLM references previous conversation naturally
- Checkpointing preserves conversation across sessions

---

## Running the Examples

All examples are available in the repository:

```bash
cd spoon-core/examples/docs

# Run individual examples
python llm_router.py           # Example 1: Intent Router
python analysis_pipeline.py    # Example 2: Multi-Step Analysis
python llm_approval.py         # Example 3: Human-in-the-Loop
python parallel_analysis.py    # Example 4: Parallel LLM
python conversational.py       # Example 5: Conversational Agent
```

:::tip API Keys Required
These examples require LLM API keys configured in your environment.
See the [Getting Started](/docs/getting-started) guide for setup.
:::

---

## Next Steps

Explore the full examples in the Examples section:

- **[Intent Graph Demo](../examples/intent-graph-demo.md)** - Advanced routing with LLM intent classification
- **[Graph Crypto Analysis](../examples/graph-crypto-analysis.md)** - Complete market analysis pipeline

---

FILE: docs/graph-system/index.md

---
sidebar_position: 1
title: Graph System Overview
description: Build stateful, multi-step AI agent workflows with the SpoonOS Graph System
---

# Graph System

The SpoonOS Graph System is a powerful library for building **stateful, multi-step AI agent workflows**. It models applications as directed graphs where **nodes** represent actions (calling an LLM, executing a tool, processing data) and **edges** define how control flows between themâ€”including conditional branching, parallel fan-out, and cycles for iterative reasoning.

## Start Here

- Just getting started? Go straight to **[Quick Start](./quick-start.md)** for a runnable example in under 2 minutes.
- Want to understand the building blocks? Read **[Core Concepts](./core-concepts.md)** after the quick start.
- Need API styles and patterns? See **[Building Graphs](./building-graphs.md)** and **[Examples](./examples.md)**.
- Looking for routing/parallel/HITL quick snippets? Jump to **[Advanced Features](./advanced-features.md)**
- Need setup? Follow **[Getting Started / Installation](../getting-started/installation.md)** before running examples.
- **You will learn:** How to build, run, and checkpoint simple graphs
- **Best for:** First-time users; time to complete: 2â€“5 minutes

### Common Pitfalls

- Checkpoint APIs (`get_state`, `get_state_history`) require a `thread_id` in `config.configurable`
- Use the exported constants (`from spoon_ai.graph import END`) instead of string literals
- When adding parallel groups, ensure all member nodes are registered before grouping
- Conditional routes must return existing node names (or `END`) to avoid configuration errors

## What is the Graph System?

Think of the Graph System as a workflow orchestration engine for AI agents. Instead of writing linear chains of prompts and responses, you define a graph where each node is a specific task, and edges determine the flow between tasks based on data, conditions, or even LLM decisions.

```mermaid
graph LR
    START((START)) --> A[Analyze Intent]
    A -->|search| B[Web Search]
    A -->|calculate| C[Math Tool]
    A -->|chat| D[LLM Response]
    B --> E[Summarize]
    C --> E
    D --> END((END))
    E --> END

    style START fill:#e1f5fe
    style END fill:#c8e6c9
    style A fill:#fff3e0
```

This simple graph shows how a single user query can be routed to different processing paths based on detected intent, with results flowing to appropriate next steps.

## Why Use Graphs for Agents?

Traditional LLM applications are often simple chains: prompt â†’ response â†’ done. But real-world AI agents need more sophisticated capabilities:

### Core Capabilities

| Capability | Description | Use Case |
|-----------|-------------|----------|
| **State Persistence** | Remember context across multiple steps and interactions | Multi-turn conversations, data accumulation |
| **Conditional Logic** | Take different paths based on LLM outputs or external data | Intent routing, quality checks |
| **Parallel Execution** | Run multiple tasks simultaneously and combine results | Multi-source data fetching, redundancy |
| **Human-in-the-Loop** | Pause for user input, approval, or correction | Trade confirmations, content review |
| **Error Recovery** | Handle failures gracefully without losing progress | Retry with backoff, circuit breakers |
| **Iterative Refinement** | Loop back to improve results based on validation | Quality improvement, self-correction |

### Real-World Scenarios

**Autonomous Research Agent**: Search â†’ Grade relevance â†’ Regenerate query if needed â†’ Synthesize findings
- Without graphs: Hard-coded retry logic, tangled control flow
- With graphs: Clean conditional edges, checkpointed state for recovery

**Multi-Source Data Analysis**: Fetch from APIs A, B, C in parallel â†’ Wait for quorum â†’ Aggregate â†’ Generate report
- Without graphs: Manual thread management, complex error handling
- With graphs: Built-in parallel execution with join strategies and timeouts

**Trading Assistant**: Analyze market â†’ Generate recommendation â†’ *Human approval* â†’ Execute trade â†’ Confirm
- Without graphs: Interrupt/resume logic scattered across codebase
- With graphs: First-class interrupt nodes with stateful resumption

## Graph System vs LangGraph

SpoonOS Graph System is inspired by [LangGraph](https://github.com/langchain-ai/langgraph) and shares similar concepts. Here's how they compare:

| Feature | SpoonOS Graph | LangGraph |
|---------|---------------|-----------|
| **Core Paradigm** | StateGraph with typed state dictionary | StateGraph with message passing |
| **Parallel Groups** | Native `add_parallel_group()` with quorum joins, timeouts, circuit breakers | Manual asyncio or branching |
| **Routing Stack** | Priority-based: explicit â†’ rules â†’ intelligent â†’ LLM â†’ fallback | Conditional edges only |
| **Declarative Definition** | `GraphTemplate` / `NodeSpec` / `EdgeSpec` for serializable, composable graphs | Imperative builder only |
| **Resource Control** | Built-in rate limiting, max concurrency, circuit breakers | External implementation |
| **Web3/Crypto** | Native integration with SpoonOS toolkits (CEX, DEX, on-chain) | Via third-party tools |
| **High-Level API** | Automatic parameter inference from LLM analysis | Manual parameter extraction |

**When to choose SpoonOS Graph**:
- âœ… Need production-grade parallel execution with sophisticated join strategies
- âœ… Require multi-layer routing (rules â†’ intelligent â†’ LLM fallback)
- âœ… Building crypto/DeFi/Web3 agents with native blockchain integration
- âœ… Want declarative graph templates for version control and team collaboration
- âœ… Need automatic parameter inference from natural language queries

**When to consider LangGraph**:
- LangChain ecosystem integration is primary requirement
- Simpler message-passing paradigm fits your use case
- Want to leverage LangSmith for tracing and debugging

## When to Use Graph vs ReAct Agents

| Decision Factor | Use Graph System | Use ReAct Agent |
|----------------|------------------|-----------------|
| **Workflow Complexity** | Multi-step, branching, or parallel workflows | Single-shot or simple tool-calling |
| **State Management** | Need to persist state across multiple steps | Stateless or simple context |
| **Control Flow** | Conditional routing, loops, human-in-the-loop | Linear execution with tool calls |
| **Scalability** | Multiple concurrent operations, resource limits | Simple, fast responses |
| **Error Handling** | Sophisticated retry, recovery, circuit breakers | Basic error messages |
| **Use Case Examples** | Research pipelines, trading systems, multi-agent collaboration | Q&A, simple task execution, one-shot analysis |

**Rule of Thumb**: If you need more than 3 sequential steps OR any kind of branching/parallel execution OR state that persists across interactions â†’ use Graph System.

## What Can You Build?

### Autonomous Agents
Multi-step reasoning with tool calls, observation loops, and adaptive planning. Example: Research agent that searches â†’ evaluates source quality â†’ decides to search again or synthesize.

### RAG Pipelines
Retrieve â†’ Grade â†’ Regenerate cycles with conditional routing based on relevance scores. Example: Keep searching until you find high-quality sources or hit iteration limit.

### Multi-Agent Systems
Multiple specialized agents collaborating via shared state and handoffs. Example: Analyst agent â†’ Risk agent â†’ Execution agent with approval gates.

### Trading Workflows
Market analysis â†’ Strategy generation â†’ *Human approval* â†’ Order execution â†’ Monitoring. Example: Crypto trading assistant with human-in-the-loop for all trades.

### Parallel Analysis
Fan-out to multiple data sources, join results with configurable strategies. Example: Fetch price data from 5 exchanges, use quorum (3/5) for consensus.

## Architecture at a Glance

```mermaid
flowchart TB
    subgraph Build["1. Build Phase"]
        SG[StateGraph Builder]
        SG -->|add_node| N[Define Nodes]
        SG -->|add_edge| E[Define Edges]
        SG -->|add_parallel_group| P[Parallel Groups]
    end

    subgraph Compile["2. Compile Phase"]
        SG -->|compile| CG[CompiledGraph Runtime]
    end

    subgraph Execute["3. Execute Phase"]
        CG -->|invoke| R1[Invoke - Get final state]
        CG -->|stream| R2[Stream - Get state updates]
    end

    subgraph State["Shared State"]
        ST["TypedDict flows through all nodes"]
    end

    Build --> Compile --> Execute
    Execute -.->|reads/writes| State
```

**Three phases**:
1. **Build**: Define your workflow topology (nodes, edges, routing)
2. **Compile**: Create executable runtime with optimizations
3. **Execute**: Run the workflow with `invoke()` or `stream()`

Throughout execution, a **shared state** (TypedDict) flows through the graph, with each node reading and updating relevant fields.

## Next Steps

Ready to build your first graph? Start with the **[Quick Start Guide](./quick-start.md)** to see a working example in under 2 minutes.

Or dive deeper into **[Core Concepts](./core-concepts.md)** to understand the building blocks: State, Nodes, Edges, and Checkpointing.

---

**Quick Links**:
- [Quick Start](./quick-start.md) - Build your first graph in 2 minutes
- [Core Concepts](./core-concepts.md) - Understand State, Nodes, Edges
- [Building Graphs](./building-graphs.md) - Learn the three API styles
- [Advanced Features](./advanced-features.md) - Routing, parallel execution, HITL
- [Integration](./integration.md) - Connect with tools, MCP, memory
- [Examples](./examples.md) - Practical patterns and use cases

**External Resources**:
- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)
- [LangGraph Tutorial](https://www.datacamp.com/tutorial/langgraph-tutorial)
- [Real Python LangGraph Guide](https://realpython.com/langgraph-python/)

---

FILE: docs/graph-system/integration.md

---
sidebar_position: 6
title: Integration & Extensions
description: GraphAgent, tools, MCP protocol, and memory management
---

# Integration & Extensions

The Graph System integrates seamlessly with the broader SpoonOS ecosystem. This guide covers how to connect graphs with agents, tools, MCP servers, and memory systems.

## GraphAgent Integration

`GraphAgent` wraps graph execution with SpoonOS agent lifecycle, persistent memory, and session management.

### Basic GraphAgent Usage

```python
import asyncio
from typing import TypedDict

from spoon_ai.graph import END, StateGraph, GraphAgent


class AnalysisState(TypedDict, total=False):
    input: str
    output: str


async def analyze(state: AnalysisState) -> dict:
    return {"output": f"Analyzed: {state.get('input', '')}"}


def build_analysis_graph() -> StateGraph:
    graph = StateGraph(AnalysisState)
    graph.add_node("analyze", analyze)
    graph.set_entry_point("analyze")
    graph.add_edge("analyze", END)
    return graph  # IMPORTANT: uncompiled StateGraph


async def main():
    agent = GraphAgent(
        name="crypto_analyzer",
        graph=build_analysis_graph(),
        memory_path="./agent_memory",
        session_id="user_123_session",
        preserve_state=True,  # Preserve state between runs
    )

    result = await agent.run("Analyze BTC price trends")
    print(result)


if __name__ == "__main__":
    asyncio.run(main())
```

### Agent Configuration

```python
from typing import TypedDict

from spoon_ai.graph import END, StateGraph, GraphAgent


class AnalysisState(TypedDict, total=False):
    input: str
    output: str


async def analyze(state: AnalysisState) -> dict:
    return {"output": f"Analyzed: {state.get('input', '')}"}


def build_analysis_graph() -> StateGraph:
    graph = StateGraph(AnalysisState)
    graph.add_node("analyze", analyze)
    graph.set_entry_point("analyze")
    graph.add_edge("analyze", END)
    return graph  # IMPORTANT: uncompiled StateGraph


agent = GraphAgent(
    name="trading_assistant",
    graph=build_analysis_graph(),  # StateGraph (uncompiled)
    preserve_state=True,  # Keep state between runs
    memory_path="./memory",  # Directory for memory storage
    session_id="session_abc123",  # Unique session identifier
    max_metadata_size=1024,  # Optional: cap stored metadata
)

print(f"Configured agent session: {agent.memory.session_id}")
```

### Execution Metadata

```python
import asyncio
from typing import TypedDict

from spoon_ai.graph import END, StateGraph, GraphAgent


async def main() -> None:
    class AnalysisState(TypedDict, total=False):
        input: str
        output: str

    async def analyze(state: AnalysisState) -> dict:
        return {"output": f"Analyzed: {state.get('input', '')}"}

    graph = StateGraph(AnalysisState)
    graph.add_node("analyze", analyze)
    graph.set_entry_point("analyze")
    graph.add_edge("analyze", END)

    agent = GraphAgent(
        name="trading_assistant",
        graph=graph,  # uncompiled StateGraph
        memory_path="./memory",
        session_id="session_abc123",
        preserve_state=True,
    )

    # Run the agent
    result = await agent.run("What's the BTC outlook?")
    print(result)

    # Access execution metadata
    metadata = agent.get_execution_metadata()

    print(f"Successful: {metadata.get('execution_successful')}")
    print(f"Last request: {metadata.get('last_request')}")
    print(f"Timestamp: {metadata.get('execution_time')}")


if __name__ == "__main__":
    asyncio.run(main())
```

### Session Management

```python
from typing import TypedDict

from spoon_ai.graph import END, StateGraph, GraphAgent


class AnalysisState(TypedDict, total=False):
    input: str
    output: str


async def analyze(state: AnalysisState) -> dict:
    return {"output": f"Analyzed: {state.get('input', '')}"}


def build_analysis_graph() -> StateGraph:
    graph = StateGraph(AnalysisState)
    graph.add_node("analyze", analyze)
    graph.set_entry_point("analyze")
    graph.add_edge("analyze", END)
    return graph


agent = GraphAgent(
    name="session_manager",
    graph=build_analysis_graph(),
    memory_path="./memory",
    session_id="user_123_session",
)

# Current session
print(f"Current session: {agent.memory.session_id}")

# Switch sessions
agent.load_session("user_456_session")
print(f"Current session: {agent.memory.session_id}")
```

---

## Tool Integration

Use SpoonOS tools within graph nodes for external capabilities.

### Using Built-in Tools

```python
from typing import Any, TypedDict

from spoon_toolkits.crypto.crypto_powerdata.tools import CryptoPowerDataCEXTool


class MarketState(TypedDict, total=False):
    symbol: str
    market_data: Any
    data_source: str
    tool_error: str


async def fetch_market_data(state: MarketState) -> dict:
    """Node that uses the CryptoPowerData tool."""
    symbol = state.get("symbol", "BTC")
    tool = CryptoPowerDataCEXTool()

    result = await tool.execute(
        exchange="binance",
        symbol=f"{symbol}/USDT",
        timeframe="1h",
        limit=24,
    )

    if getattr(result, "error", None):
        return {"market_data": {}, "data_source": "binance", "tool_error": result.error}

    return {"market_data": result.output, "data_source": "binance"}
```

### Tool Result Handling

```python
import asyncio
import os
from typing import Any, TypedDict


try:
    from spoon_toolkits.crypto.crypto_data_tools.price_data import GetTokenPriceTool
except Exception:  # pragma: no cover - optional dependency
    GetTokenPriceTool = None


class ProcessState(TypedDict, total=False):
    symbol: str
    tool_status: str
    tool_error: str
    tool_output: Any


async def process_with_tool(state: ProcessState) -> dict:
    """Robust tool usage with error handling."""
    if os.getenv("DOC_SNIPPET_MODE") == "1" or GetTokenPriceTool is None:
        return {"tool_status": "skipped", "tool_output": {"reason": "DOC_SNIPPET_MODE or missing toolkit"}}

    tool = GetTokenPriceTool()
    try:
        result = await tool.execute(symbol=state.get("symbol", "ETH-USDC"))
        if getattr(result, "error", None):
            return {"tool_status": "error", "tool_error": result.error}
        return {"tool_status": "success", "tool_output": result.output}
    except Exception as e:
        return {"tool_status": "error", "tool_error": str(e)}


async def main() -> None:
    print(await process_with_tool({"symbol": "ETH-USDC"}))


if __name__ == "__main__":
    asyncio.run(main())
```

### Multiple Tools in One Node

```python
import asyncio
import os
from typing import Any, Dict, TypedDict


try:
    from spoon_toolkits.crypto.crypto_powerdata.tools import CryptoPowerDataCEXTool
except Exception:  # pragma: no cover - optional dependency
    CryptoPowerDataCEXTool = None

try:
    from spoon_toolkits.crypto.crypto_data_tools.price_data import GetTokenPriceTool
except Exception:  # pragma: no cover - optional dependency
    GetTokenPriceTool = None


class AnalysisState(TypedDict, total=False):
    symbol: str
    price_data: Any
    dex_price: Any


async def comprehensive_analysis(state: AnalysisState) -> dict:
    """Node that orchestrates multiple tools."""
    symbol = state.get("symbol", "BTC")

    if os.getenv("DOC_SNIPPET_MODE") == "1" or CryptoPowerDataCEXTool is None or GetTokenPriceTool is None:
        return {
            "price_data": {"source": "stub", "symbol": symbol},
            "dex_price": {"source": "stub", "symbol": symbol},
        }

    results: Dict[str, Any] = {}

    # Tool 1: Price data
    price_tool = CryptoPowerDataCEXTool()
    price_data = await price_tool.execute(
        exchange="binance",
        symbol=f"{symbol}/USDT",
        timeframe="1h",
        limit=24,
    )
    results["price_data"] = price_data.output if not getattr(price_data, "error", None) else {"error": price_data.error}

    # Tool 2: DEX spot price snapshot
    dex_tool = GetTokenPriceTool()
    dex_price = await dex_tool.execute(symbol=f"{symbol}-USDC", exchange="uniswap")
    results["dex_price"] = dex_price.output if not getattr(dex_price, "error", None) else {"error": dex_price.error}

    return results


async def main() -> None:
    result = await comprehensive_analysis({"symbol": "BTC"})
    print(result)


if __name__ == "__main__":
    asyncio.run(main())
```

---

## MCP Protocol Integration

Connect to MCP (Model Context Protocol) servers for dynamic tool discovery.

### MCP Overview

```mermaid
graph LR
    A[Graph Node] --> B[MCP Client]
    B --> C[MCP Server]
    C --> D[External Tools]
    C --> E[Data Sources]
    C --> F[APIs]
```

### Using MCP Tools in Graphs

```python
import os
from typing import Any, TypedDict

from spoon_ai.tools.mcp_tool import MCPTool


class SearchState(TypedDict, total=False):
    query: str
    search_results: Any


tavily_tool = MCPTool(
    name="tavily-search",
    description="Web search via Tavily",
    mcp_config={
        "command": "npx",
        "args": ["--yes", "tavily-mcp"],
        "env": {"TAVILY_API_KEY": os.getenv("TAVILY_API_KEY", "")},
    },
)


async def mcp_search_node(state: SearchState) -> dict:
    """Node that calls an MCP tool."""
    if not os.getenv("TAVILY_API_KEY"):
        return {"search_results": "Skipping: TAVILY_API_KEY not set"}

    result = await tavily_tool.execute(query=state.get("query", ""), max_results=5)
    return {"search_results": result}
```

### High-Level MCP Integration

```python
from typing import TypedDict

from spoon_ai.graph.builder import HighLevelGraphAPI
from spoon_ai.graph.mcp_integration import MCPToolSpec


class MyState(TypedDict, total=False):
    user_query: str


api = HighLevelGraphAPI(MyState)

api.register_mcp_tool(
    intent_category="research",
    spec=MCPToolSpec(name="tavily-search"),
    config={
        "command": "npx",
        "args": ["--yes", "tavily-mcp"],
        "env": {"TAVILY_API_KEY": "..."},
    },
)

tool = api.create_mcp_tool("tavily-search")
```

### For More MCP Information

See the dedicated **[MCP Protocol Guide](/docs/core-concepts/mcp-protocol)** for:
- Server types (stdio, HTTP, WebSocket)
- Authentication and security
- Custom MCP server development
- Advanced tool discovery patterns

---

## Memory Management

Persist state and conversation history across sessions.

### GraphAgent Memory Operations

```python
from typing import TypedDict

from spoon_ai.graph import END, StateGraph, GraphAgent


class AnalysisState(TypedDict, total=False):
    input: str
    output: str


async def analyze(state: AnalysisState) -> dict:
    return {"output": f"Analyzed: {state.get('input', '')}"}


def build_analysis_graph() -> StateGraph:
    graph = StateGraph(AnalysisState)
    graph.add_node("analyze", analyze)
    graph.set_entry_point("analyze")
    graph.add_edge("analyze", END)
    return graph


# Create agent with memory
agent = GraphAgent(
    name="assistant",
    graph=build_analysis_graph(),
    memory_path="./memory",
    session_id="user_123",
)

# Set metadata
agent.set_memory_metadata("last_analysis_time", "2024-01-15T10:30:00Z")
agent.set_memory_metadata("user_preferences", {
    "risk_tolerance": "medium",
    "favorite_tokens": ["BTC", "ETH", "SOL"]
})

# Get metadata
last_time = agent.get_memory_metadata("last_analysis_time")
prefs = agent.get_memory_metadata("user_preferences")

# Get statistics
stats = agent.get_memory_statistics()
print(f"Total messages: {stats['total_messages']}")
print(f"Session id: {stats['session_id']}")
print(f"Storage path: {stats['storage_path']}")
print(f"File size: {stats['file_size']} bytes")
```

### Memory Search

```python
from typing import TypedDict

from spoon_ai.graph import END, StateGraph, GraphAgent


class AnalysisState(TypedDict, total=False):
    input: str
    output: str


async def analyze(state: AnalysisState) -> dict:
    return {"output": f"Analyzed: {state.get('input', '')}"}


def build_analysis_graph() -> StateGraph:
    graph = StateGraph(AnalysisState)
    graph.add_node("analyze", analyze)
    graph.set_entry_point("analyze")
    graph.add_edge("analyze", END)
    return graph


agent = GraphAgent(
    name="assistant",
    graph=build_analysis_graph(),
    memory_path="./memory",
    session_id="user_123",
)

# Add an example message, then search.
agent.memory.add_message({"role": "assistant", "content": "Here is a short bitcoin analysis..."})

matches = agent.search_memory(query="bitcoin analysis", limit=5)
for match in matches:
    print(f"Content: {str(match.get('content', ''))[:100]}...")
    print(f"Timestamp: {match.get('timestamp')}")
    print("---")
```

### State Persistence

```python
import asyncio
from typing import TypedDict

from spoon_ai.graph import END, StateGraph, GraphAgent


class AgentState(TypedDict, total=False):
    input: str
    favorite_token: str
    output: str


async def process(state: AgentState) -> dict:
    text = (state.get("input") or "").lower()
    if "remember" in text and "favorite token" in text and " is " in text:
        token = state["input"].split(" is ", 1)[-1].strip().upper()
        return {"favorite_token": token, "output": f"Okay. I'll remember your favorite token is {token}."}
    if "favorite token" in text:
        token = state.get("favorite_token") or "not set yet"
        return {"output": f"Your favorite token is {token}."}
    return {"output": f"Echo: {state.get('input', '')}"}


graph = StateGraph(AgentState)
graph.add_node("process", process)
graph.set_entry_point("process")
graph.add_edge("process", END)

# Enable state persistence at the agent layer.
agent = GraphAgent(
    name="persistent_agent",
    graph=graph,  # uncompiled StateGraph
    preserve_state=True,  # Key setting
    memory_path="./memory",
)


async def main() -> None:
    # First run
    print(await agent.run("Remember my favorite token is SOL"))

    # Later run (state is preserved)
    print(await agent.run("What's my favorite token?"))


if __name__ == "__main__":
    asyncio.run(main())
```

### Memory with Checkpointing

```python
from spoon_ai.graph import InMemoryCheckpointer
from typing import TypedDict

from spoon_ai.graph import END, StateGraph, GraphAgent

# Combine graph checkpointing with agent memory
checkpointer = InMemoryCheckpointer(max_checkpoints_per_thread=50)

class MyState(TypedDict, total=False):
    input: str
    output: str


async def process(state: MyState) -> dict:
    return {"output": f"processed: {state.get('input', '')}"}


graph = StateGraph(MyState, checkpointer=checkpointer)
graph.add_node("process", process)
graph.set_entry_point("process")
graph.add_edge("process", END)

agent = GraphAgent(
    name="full_memory_agent",
    graph=graph,
    memory_path="./memory",
    preserve_state=True
)

# Now you have:
# 1. Graph-level checkpoints (for recovery within a run)
# 2. Agent-level memory (for persistence across runs)
print("checkpointing enabled:", agent.graph.graph.checkpointer is checkpointer)
```

---

## Monitoring and Debugging

Track execution and diagnose issues.

### Enable Monitoring

```python
from typing import TypedDict

from spoon_ai.graph import END, StateGraph


class MonitorState(TypedDict, total=False):
    input: str
    output: str


async def process(state: MonitorState) -> dict:
    return {"output": f"processed: {state.get('input', '')}"}


graph = StateGraph(MonitorState)
graph.add_node("process", process)
graph.set_entry_point("process")
graph.add_edge("process", END)

graph.enable_monitoring(["execution_time", "success_rate", "routing_performance", "node_stats"])
app = graph.compile()
```

### Execution Metrics

```python
import asyncio
from typing import TypedDict

from spoon_ai.graph import END, StateGraph


async def main() -> None:
    class MonitorState(TypedDict, total=False):
        input: str
        output: str

    async def process(state: MonitorState) -> dict:
        return {"output": f"processed: {state.get('input', '')}"}

    graph = StateGraph(MonitorState)
    graph.enable_monitoring(["execution_time", "success_rate", "routing_performance", "node_stats"])
    graph.add_node("process", process)
    graph.set_entry_point("process")
    graph.add_edge("process", END)

    app = graph.compile()
    initial_state = {"input": "hello", "output": ""}

    # Run the graph
    result = await app.invoke(initial_state)
    print(result)

    # Get metrics
    metrics = app.get_execution_metrics()

    print("Execution Summary:")
    print(f"  Total executions: {metrics['total_executions']}")
    print(f"  Success rate: {metrics['success_rate']:.1%}")
    print(f"  Avg execution time: {metrics['avg_execution_time']:.3f}s")

    # Per-node statistics
    print("Per-Node Statistics:")
    for node, stats in metrics.get("node_stats", {}).items():
        print(f"  {node}:")
        print(f"    Calls: {stats['count']}")
        print(f"    Avg time: {stats['avg_time']:.3f}s")
        print(f"    Error rate: {stats['error_rate']:.1%}")


if __name__ == "__main__":
    asyncio.run(main())
```

### Execution History

```python
import asyncio
from typing import TypedDict

from spoon_ai.graph import END, StateGraph


class MonitorState(TypedDict, total=False):
    input: str
    output: str


async def process(state: MonitorState) -> dict:
    return {"output": f"processed: {state.get('input', '')}"}


graph = StateGraph(MonitorState)
graph.add_node("process", process)
graph.set_entry_point("process")
graph.add_edge("process", END)
app = graph.compile()


async def main() -> None:
    await app.invoke({"input": "hello"})
    for step in app.execution_history:
        print(
            f"Node: {step.get('node_name')}\n"
            f"  Success: {step.get('success')}\n"
            f"  Execution time: {step.get('execution_time', 0.0):.3f}s\n"
        )


if __name__ == "__main__":
    asyncio.run(main())
```

### Debugging with Checkpoints

```python
# Enable verbose logging
import logging
logging.getLogger("spoon_ai.graph").setLevel(logging.DEBUG)

import asyncio
from typing import TypedDict

from spoon_ai.graph import END, StateGraph


async def main() -> None:
    class DebugState(TypedDict, total=False):
        input: str
        output: str

    async def process(state: DebugState) -> dict:
        return {"output": f"processed: {state.get('input', '')}"}

    graph = StateGraph(DebugState)
    graph.add_node("process", process)
    graph.set_entry_point("process")
    graph.add_edge("process", END)
    app = graph.compile()
    initial_state = {"input": "hello"}

    # Run with checkpointing
    result = await app.invoke(
        initial_state,
        config={"configurable": {"thread_id": "debug_session"}},
    )
    print(result)

    # Inspect checkpoint history
    config = {"configurable": {"thread_id": "debug_session"}}
    for checkpoint in graph.get_state_history(config):
        print(f"After node: {checkpoint.metadata.get('node')}")
        print(f"State: {checkpoint.values}")
        print("---")


if __name__ == "__main__":
    asyncio.run(main())
```

### Streaming for Real-Time Monitoring

```python
# Stream execution for real-time updates
import asyncio
from typing import TypedDict

from spoon_ai.graph import END, StateGraph


async def main() -> None:
    class StreamState(TypedDict, total=False):
        input: str
        output: str

    async def process(state: StreamState) -> dict:
        return {"output": f"processed: {state.get('input', '')}"}

    graph = StateGraph(StreamState)
    graph.add_node("process", process)
    graph.set_entry_point("process")
    graph.add_edge("process", END)
    app = graph.compile()
    initial_state = {"input": "hello"}

    async for update in app.stream(initial_state):
        node = update.get("__node__", "unknown")
        print(f"[{node}] State update: {list(update.keys())}")

        # Check for specific conditions
        if update.get("error"):
            print(f"  ERROR: {update['error']}")

        if update.get("confidence", 0) < 0.5:
            print(f"  LOW CONFIDENCE: {update.get('confidence')}")


if __name__ == "__main__":
    asyncio.run(main())
```

---

## Integration Patterns

### Pattern 1: Full-Stack Agent

```python
import asyncio
from typing import Any, TypedDict

from spoon_ai.graph import END, StateGraph, GraphAgent, InMemoryCheckpointer

try:
    # Optional: toolkit-based market data tool
    from spoon_toolkits.crypto.crypto_powerdata.tools import CryptoPowerDataCEXTool
except Exception:  # pragma: no cover - optional dependency
    CryptoPowerDataCEXTool = None


class FullStackState(TypedDict, total=False):
    input: str
    symbol: str
    price_data: Any
    output: str


async def extract_symbol(state: FullStackState) -> dict:
    text = (state.get("input") or "").upper()
    if "ETH" in text:
        symbol = "ETH"
    elif "SOL" in text:
        symbol = "SOL"
    else:
        symbol = "BTC"
    return {"symbol": symbol}


async def fetch_price(state: FullStackState) -> dict:
    if CryptoPowerDataCEXTool is None:
        return {"output": "Skipping: spoon_toolkits is not installed"}

    tool = CryptoPowerDataCEXTool()
    result = await tool.execute(
        exchange="binance",
        symbol=f"{state.get('symbol', 'BTC')}/USDT",
        timeframe="1h",
        limit=10,
    )
    if getattr(result, "error", None):
        return {"output": f"Tool error: {result.error}", "price_data": {}}
    return {"price_data": result.output}


async def format_output(state: FullStackState) -> dict:
    if state.get("output"):
        return {"output": state["output"]}
    preview = str(state.get("price_data", {}))[:200]
    return {"output": f"Fetched price data for {state.get('symbol')}: {preview}..."}


def build_graph() -> StateGraph:
    graph = StateGraph(FullStackState, checkpointer=InMemoryCheckpointer())
    graph.enable_monitoring(["execution_time", "node_stats"])

    graph.add_node("extract_symbol", extract_symbol)
    graph.add_node("fetch_price", fetch_price)
    graph.add_node("format_output", format_output)

    graph.set_entry_point("extract_symbol")
    graph.add_edge("extract_symbol", "fetch_price")
    graph.add_edge("fetch_price", "format_output")
    graph.add_edge("format_output", END)
    return graph


async def main() -> None:
    agent = GraphAgent(
        name="full_stack_agent",
        graph=build_graph(),  # IMPORTANT: pass StateGraph (uncompiled)
        memory_path="./memory",
        preserve_state=True,
    )

    result = await agent.run("Analyze BTC")
    print(result)
    print(agent.graph.get_execution_metrics())


if __name__ == "__main__":
    asyncio.run(main())
```

### Pattern 2: Multi-Agent Handoff

```python
import asyncio
from typing import TypedDict

from spoon_ai.graph import END, StateGraph, GraphAgent


class HandoffState(TypedDict, total=False):
    input: str
    output: str


def build_research_graph() -> StateGraph:
    graph = StateGraph(HandoffState)

    async def research(state: HandoffState) -> dict:
        return {"output": f"Research findings: {state.get('input', '')}"}

    graph.add_node("research", research)
    graph.set_entry_point("research")
    graph.add_edge("research", END)
    return graph


def build_analysis_graph() -> StateGraph:
    graph = StateGraph(HandoffState)

    async def analyze(state: HandoffState) -> dict:
        return {"output": f"Analysis: {state.get('input', '')}"}

    graph.add_node("analyze", analyze)
    graph.set_entry_point("analyze")
    graph.add_edge("analyze", END)
    return graph


def build_execution_graph() -> StateGraph:
    graph = StateGraph(HandoffState)

    async def execute(state: HandoffState) -> dict:
        return {"output": f"Execution plan: {state.get('input', '')}"}

    graph.add_node("execute", execute)
    graph.set_entry_point("execute")
    graph.add_edge("execute", END)
    return graph


async def main() -> None:
    # Agents are isolated. Hand off results explicitly via inputs.
    session_id = "handoff_session"
    memory_path = "./memory"

    research_agent = GraphAgent(
        name="researcher",
        graph=build_research_graph(),
        session_id=session_id,
        memory_path=memory_path,
    )

    analysis_agent = GraphAgent(
        name="analyst",
        graph=build_analysis_graph(),
        session_id=session_id,
        memory_path=memory_path,
    )

    execution_agent = GraphAgent(
        name="executor",
        graph=build_execution_graph(),
        session_id=session_id,
        memory_path=memory_path,
    )

    research_result = await research_agent.run("Research BTC market")
    analysis_result = await analysis_agent.run(f"Analyze: {research_result}")
    execution_result = await execution_agent.run(f"Execute: {analysis_result}")

    print(research_result)
    print(analysis_result)
    print(execution_result)


if __name__ == "__main__":
    asyncio.run(main())
```

### Pattern 3: Event-Driven Graph

```python
import asyncio
import json
from typing import Any, Dict, TypedDict

from spoon_ai.graph import END, StateGraph, GraphAgent


class EventState(TypedDict, total=False):
    input: str
    event_type: str
    event_data: Dict[str, Any]
    output: str


def build_event_graph() -> StateGraph:
    graph = StateGraph(EventState)

    async def parse_event(state: EventState) -> dict:
        payload = json.loads(state.get("input", "{}"))
        return {"event_type": payload.get("event_type", ""), "event_data": payload.get("data", {})}

    async def process_event(state: EventState) -> dict:
        return {"output": f"Processed {state.get('event_type')}: {state.get('event_data')}"}

    graph.add_node("parse_event", parse_event)
    graph.add_node("process_event", process_event)
    graph.set_entry_point("parse_event")
    graph.add_edge("parse_event", "process_event")
    graph.add_edge("process_event", END)
    return graph


async def handle_event(event_type: str, data: dict) -> str:
    agent = GraphAgent(
        name="event_processor",
        graph=build_event_graph(),
        session_id=f"event_{event_type}",
        memory_path="./memory",
    )
    payload = json.dumps({"event_type": event_type, "data": data})
    return await agent.run(payload)


async def main() -> None:
    print(await handle_event("price_alert", {"symbol": "BTC", "price": 50000}))
    print(await handle_event("trade_signal", {"symbol": "ETH", "side": "buy"}))


if __name__ == "__main__":
    asyncio.run(main())
```

---

## Next Steps

See practical implementations of these integration patterns:

**[Examples â†’](./examples.md)** - Complete working examples with routing, parallel execution, and human-in-the-loop patterns

---

FILE: docs/graph-system/quick-start.md

---
sidebar_position: 2
title: Quick Start
description: Build your first LLM-powered graph in 2 minutes
---

# Quick Start

Get up and running with the SpoonOS Graph System in under 2 minutes. This guide shows you how to create an LLM-powered graph workflow.

**You will learn:** how to define state, add a node, run a graph, and retrieve a checkpoint
**Best for:** first-time users
**Time to complete:** ~2 minutes

## Prerequisites

- Complete the steps in **[Getting Started / Installation](../getting-started/installation.md)**
- Import public APIs from `spoon_ai.graph` (e.g., `from spoon_ai.graph import StateGraph, END`)

## 2-Minute Hello World (no LLM)

The smallest runnable graph: one node, one edge, and a checkpoint read with `thread_id`.

```python
import asyncio
from typing import TypedDict
from spoon_ai.graph import StateGraph, END

class HelloState(TypedDict):
    name: str
    message: str

async def say_hello(state: HelloState) -> dict:
    return {"message": f"Hello, {state['name']}!"}

graph = StateGraph(HelloState)
graph.add_node("hello", say_hello)
graph.set_entry_point("hello")
graph.add_edge("hello", END)
app = graph.compile()

async def main():
    config = {"configurable": {"thread_id": "hello-demo"}}
    result = await app.invoke({"name": "Graph", "message": ""}, config=config)
    print(result["message"])  # Hello, Graph!

    # Checkpoint read: requires thread_id (captures state before node execution)
    snapshot = graph.get_state(config)
    print("checkpoint values:", snapshot.values)

if __name__ == "__main__":
    asyncio.run(main())
```

**Run it:**
```bash
python my_first_graph.py
```

## Your First LLM Graph

Here's a complete graph that uses LLM to analyze user queries:

```python
import asyncio
from typing import TypedDict, List
from spoon_ai.graph import StateGraph, END
from spoon_ai.llm import LLMManager
from spoon_ai.schema import Message

class ChatState(TypedDict):
    messages: List[dict]
    user_query: str
    llm_response: str

# Initialize LLM
llm = LLMManager()

async def analyze_query(state: ChatState) -> dict:
    """Use LLM to analyze the user query."""
    response = await llm.chat([
        Message(role="system", content="You are a helpful crypto assistant."),
        Message(role="user", content=state["user_query"])
    ])
    return {"llm_response": response.content}

# Build graph
graph = StateGraph(ChatState)
graph.add_node("analyze", analyze_query)
graph.set_entry_point("analyze")
graph.add_edge("analyze", END)
app = graph.compile()

async def main():
    result = await app.invoke({
        "messages": [],
        "user_query": "What is Bitcoin?",
        "llm_response": ""
    })
    print(result["llm_response"])

if __name__ == "__main__":
    asyncio.run(main())
```

**Run it:**
```bash
python my_first_graph.py
```

## Understanding the Code

### 1. Define Your State Schema

```python
from typing import Any, Dict, List, TypedDict


class ChatState(TypedDict):
    messages: List[Dict[str, Any]]   # Conversation history
    user_query: str                  # User input
    llm_response: str                # LLM output
```

**State** flows through your entire graph. Every node can read from it and write to it.

:::tip TypedDict Benefits
- IDE autocomplete for state fields
- Type checking to catch bugs early
- Self-documenting code
:::

### 2. Create an LLM-Powered Node

```python
import os
from typing import Any, Dict, List, TypedDict

from spoon_ai.llm import LLMManager
from spoon_ai.schema import Message


class ChatState(TypedDict, total=False):
    messages: List[Dict[str, Any]]
    user_query: str
    llm_response: str


llm = LLMManager()


async def analyze_query(state: ChatState) -> dict:
    """Use LLM to analyze the user query."""
    if os.getenv("DOC_SNIPPET_MODE") == "1":
        return {"llm_response": f"(stub) analyzed: {state.get('user_query', '')}"}

    response = await llm.chat([
        Message(role="system", content="You are a helpful crypto assistant."),
        Message(role="user", content=state["user_query"])
    ], max_tokens=200)
    return {"llm_response": response.content}
```

**Nodes** are async functions that:
- **Input**: Receive the full state dictionary
- **Process**: Call LLM, tools, or external APIs
- **Output**: Return a partial update (only changed fields)

### 3. Build and Execute

```python
from typing import Any, Dict, List, TypedDict

from spoon_ai.graph import StateGraph, END


class ChatState(TypedDict, total=False):
    messages: List[Dict[str, Any]]
    user_query: str
    llm_response: str


async def analyze_query(state: ChatState) -> dict:
    # Keep this snippet runnable without requiring an LLM key.
    return {"llm_response": f"(stub) analyzed: {state.get('user_query', '')}"}


graph = StateGraph(ChatState)
graph.add_node("analyze", analyze_query)
graph.set_entry_point("analyze")
graph.add_edge("analyze", END)
app = graph.compile()
```

**Three essential steps**:
1. Create `StateGraph` with your state schema
2. Add nodes with `.add_node(name, function)`
3. Set entry point and compile

## Multi-Step LLM Workflow

Here's a more realistic example with multiple LLM calls:

```python
import asyncio
import os
from typing import TypedDict, List
from spoon_ai.graph import StateGraph, END
from spoon_ai.llm import LLMManager
from spoon_ai.schema import Message

class AnalysisState(TypedDict):
    user_query: str
    intent: str
    analysis: str
    final_response: str

llm = LLMManager()

async def classify_intent(state: AnalysisState) -> dict:
    """LLM classifies the user's intent."""
    if os.getenv("DOC_SNIPPET_MODE") == "1":
        return {"intent": "analysis_request"}
    response = await llm.chat([
        Message(role="system", content="""Classify the user query into one of:
        - price_query: asking about price
        - analysis_request: asking for market analysis
        - general_question: other questions
        Reply with only the category name."""),
        Message(role="user", content=state["user_query"])
    ])
    return {"intent": response.content.strip().lower()}

async def generate_analysis(state: AnalysisState) -> dict:
    """LLM generates detailed analysis."""
    if os.getenv("DOC_SNIPPET_MODE") == "1":
        return {"analysis": f"(stub) analysis for: {state['user_query']}"}
    response = await llm.chat([
        Message(role="system", content="You are a crypto analyst. Provide detailed analysis."),
        Message(role="user", content=f"Analyze: {state['user_query']}")
    ])
    return {"analysis": response.content}

async def format_response(state: AnalysisState) -> dict:
    """LLM formats the final response."""
    if os.getenv("DOC_SNIPPET_MODE") == "1":
        return {"final_response": f"(stub) summary: {state.get('analysis', '')[:80]}..."}
    response = await llm.chat([
        Message(role="system", content="Format this analysis into a concise, user-friendly response."),
        Message(role="user", content=f"Intent: {state['intent']}\nAnalysis: {state['analysis']}")
    ])
    return {"final_response": response.content}

# Build graph: classify -> analyze -> format
graph = StateGraph(AnalysisState)
graph.add_node("classify", classify_intent)
graph.add_node("analyze", generate_analysis)
graph.add_node("format", format_response)

graph.set_entry_point("classify")
graph.add_edge("classify", "analyze")
graph.add_edge("analyze", "format")
graph.add_edge("format", END)

app = graph.compile()

async def main():
    result = await app.invoke({
        "user_query": "What do you think about Bitcoin's price trend?",
        "intent": "",
        "analysis": "",
        "final_response": ""
    })
    print(f"Intent: {result['intent']}")
    print(f"Response: {result['final_response']}")

if __name__ == "__main__":
    asyncio.run(main())
```

## What Just Happened?

```mermaid
graph LR
    A[User Query] --> B[Classify Intent<br/>LLM Call 1]
    B --> C[Generate Analysis<br/>LLM Call 2]
    C --> D[Format Response<br/>LLM Call 3]
    D --> E[Final Output]

    style A fill:#e1f5fe
    style E fill:#c8e6c9
    style B fill:#fff3e0
    style C fill:#fff3e0
    style D fill:#fff3e0
```

1. User provides a query
2. First LLM call classifies the intent
3. Second LLM call generates detailed analysis
4. Third LLM call formats the response
5. Final state contains all intermediate and final results

## Quick Reference

| Component | Purpose | Example |
|-----------|---------|---------|
| `StateGraph(schema)` | Create a new graph | `graph = StateGraph(MyState)` |
| `.add_node(name, fn)` | Add an LLM-powered step | `graph.add_node("analyze", llm_fn)` |
| `.add_edge(from, to)` | Connect nodes | `graph.add_edge("a", "b")` |
| `.set_entry_point(name)` | Set starting node | `graph.set_entry_point("start")` |
| `.compile()` | Prepare for execution | `app = graph.compile()` |
| `.invoke(state)` | Run the graph | `result = await app.invoke({...})` |

## What's Next?

Now that you've built your first LLM-powered graph:

### [Core Concepts â†’](./core-concepts.md)
Deep dive into State, Nodes, Edges, and Checkpointing.

### [Building Graphs â†’](./building-graphs.md)
Learn the three API styles: Imperative, Declarative, and High-Level.

### [Examples â†’](./examples.md)
See practical patterns like LLM routing, parallel execution, and human-in-the-loop.

---

FILE: docs/how-to-guides/add-custom-tools.md

# Adding Custom Tools to SpoonOS

This guide shows you how to create and integrate custom tools into the SpoonOS framework. Tools extend agent capabilities by providing specific functionality like API integrations, data processing, or blockchain interactions.

## Tool Architecture Overview

SpoonOS uses a modular tool system where:

- **BaseTool**: Abstract base class defining the tool interface
- **ToolManager**: Manages tool collections and execution
- **MCP Integration**: Exposes tools via Model Context Protocol
- **Dynamic Loading**: Tools can be added at runtime

## Creating a Basic Tool

### Step 1: Define Your Tool Class

Create a new tool by inheriting from `BaseTool`:

```python
from spoon_ai.tools.base import BaseTool, ToolResult
from typing import Any, Dict

class MyCustomTool(BaseTool):
    name: str = "my_custom_tool"
    description: str = "A custom tool that processes data"
    parameters: dict = {
        "type": "object",
        "properties": {
            "input_data": {
                "type": "string",
                "description": "The data to process"
            },
            "options": {
                "type": "object",
                "description": "Optional processing parameters",
                "properties": {
                    "format": {"type": "string", "default": "json"}
                }
            }
        },
        "required": ["input_data"]
    }

    async def execute(self, input_data: str, options: Dict[str, Any] = None) -> ToolResult:
        """Execute the tool logic - framework handles errors automatically"""
        # Your tool logic here
        processed_data = self.process_data(input_data, options or {})

        return ToolResult(
            output=processed_data,
            system=f"Successfully processed {len(input_data)} characters"
        )

    def process_data(self, data: str, options: Dict[str, Any]) -> str:
        """Your custom processing logic"""
        # Example: simple data transformation
        format_type = options.get("format", "json")
        if format_type == "uppercase":
            return data.upper()
        return f'{{"processed": "{data}"}}'
```

### Step 2: Tool Parameters Schema

The `parameters` field defines the JSON schema for tool inputs:

```python
parameters: dict = {
    "type": "object",
    "properties": {
        "required_param": {
            "type": "string",
            "description": "A required parameter"
        },
        "optional_param": {
            "type": "integer",
            "description": "An optional parameter",
            "default": 42
        },
        "enum_param": {
            "type": "string",
            "enum": ["option1", "option2", "option3"],
            "description": "Choose from predefined options"
        }
    },
    "required": ["required_param"]
}
```

## Advanced Tool Examples

### API Integration Tool

```python
import aiohttp
from spoon_ai.tools.base import BaseTool, ToolResult

class APITool(BaseTool):
    name: str = "api_fetcher"
    description: str = "Fetches data from external APIs"
    parameters: dict = {
        "type": "object",
        "properties": {
            "url": {"type": "string", "description": "API endpoint URL"},
            "method": {"type": "string", "enum": ["GET", "POST"], "default": "GET"},
            "headers": {"type": "object", "description": "HTTP headers"}
        },
        "required": ["url"]
    }

    async def execute(self, url: str, method: str = "GET", headers: dict = None) -> ToolResult:
        # Framework provides automatic error handling and retry logic
        async with aiohttp.ClientSession() as session:
            async with session.request(method, url, headers=headers) as response:
                data = await response.json()
                return ToolResult(
                    output=data,
                    system=f"API call successful: {response.status}"
                )
```

### Blockchain Tool Example

```python
from web3 import Web3
from spoon_ai.tools.base import BaseTool, ToolResult

class BlockchainTool(BaseTool):
    name: str = "get_eth_balance"
    description: str = "Gets Ethereum balance for an address"
    parameters: dict = {
        "type": "object",
        "properties": {
            "address": {
                "type": "string",
                "description": "Ethereum address to check"
            },
            "network": {
                "type": "string",
                "enum": ["mainnet", "goerli", "sepolia"],
                "default": "mainnet"
            }
        },
        "required": ["address"]
    }

    def __init__(self):
        super().__init__()
        self.w3 = Web3(Web3.HTTPProvider("https://eth-mainnet.alchemyapi.io/v2/YOUR_KEY"))

    async def execute(self, address: str, network: str = "mainnet") -> ToolResult:
        # Framework handles validation and error cases automatically
        if not self.w3.is_address(address):
            return ToolResult(error="Invalid Ethereum address")

        balance_wei = self.w3.eth.get_balance(address)
        balance_eth = self.w3.from_wei(balance_wei, 'ether')

        return ToolResult(
            output={
                "address": address,
                "balance_eth": str(balance_eth),
                "balance_wei": str(balance_wei),
                "network": network
            }
        )
```

## Integrating Tools

### Method 1: Add to Tool Manager

```python
from spoon_ai.tools.tool_manager import ToolManager
from your_module import MyCustomTool

# Create tool manager with existing tools
tool_manager = ToolManager([])

# Add your custom tool
custom_tool = MyCustomTool()
tool_manager.add_tool(custom_tool)

# Or add multiple tools at once
tool_manager.add_tools(
    MyCustomTool(),
    APITool(),
    BlockchainTool()
)
```

### Method 2: Create Tool Collection

```python
# tools/my_tools.py
from typing import List
from spoon_ai.tools.base import BaseTool
from .my_custom_tool import MyCustomTool
from .api_tool import APITool

def get_my_tools() -> List[BaseTool]:
    """Return collection of custom tools"""
    return [
        MyCustomTool(),
        APITool(),
        # Add more tools here
    ]

def create_my_tool_manager() -> ToolManager:
    """Create tool manager with custom tools"""
    from spoon_ai.tools.tool_manager import ToolManager
    return ToolManager(get_my_tools())
```

### Method 3: MCP Integration

Expose your custom tools as an MCP server using core + `fastmcp` (no spoon-cli):

```python
# mcp_server.py
import asyncio
from fastmcp import FastMCP
from fastmcp.tools.tool import FunctionTool
from spoon_ai.tools.base import BaseTool

class MyTool(BaseTool):
    name = "my_custom_tool"
    description = "Echo input text"
    parameters = {
        "type": "object",
        "properties": {"text": {"type": "string"}},
        "required": ["text"],
    }
    async def execute(self, text: str):
        return {"echo": text}

async def main():
    mcp = FastMCP("My MCP Server")
    mcp.add_tool(FunctionTool(
        name=MyTool.name,
        description=MyTool.description,
        fn=MyTool().execute,
        parameters=MyTool.parameters,
    ))
    print("Starting MCP server on port 8766...")
    await mcp.run_async(transport="sse", port=8766)

if __name__ == "__main__":
    asyncio.run(main())
```

To connect to an MCP server from an agent, use `MCPTool`:

```python
from spoon_ai.tools.mcp_tool import MCPTool

# Connect to your custom MCP server
custom_mcp = MCPTool(
    name="my_custom_tools",
    description="My custom tools exposed via MCP",
    mcp_config={
        "url": "http://localhost:8766/sse",
        "transport": "sse"
    }
)

# Load available tools
await custom_mcp.ensure_parameters_loaded()
```

## Tool Configuration

### Environment Variables

```python
import os
from spoon_ai.tools.base import BaseTool, ToolResult

class ConfigurableTool(BaseTool):
    name: str = "configurable_tool"
    description: str = "Tool that uses environment configuration"

    def __init__(self):
        super().__init__()
        self.api_key = os.getenv("MY_API_KEY")
        self.base_url = os.getenv("MY_API_URL", "https://api.example.com")

        if not self.api_key:
            raise ValueError("MY_API_KEY environment variable required")

    async def execute(self, query: str) -> ToolResult:
        # Use self.api_key and self.base_url
        pass
```

### Configuration Class

```python
from pydantic import BaseModel
from typing import Optional

class ToolConfig(BaseModel):
    api_key: str
    base_url: str = "https://api.example.com"
    timeout: int = 30
    retries: int = 3

class ConfigurableTool(BaseTool):
    def __init__(self, config: ToolConfig):
        super().__init__()
        self.config = config

    async def execute(self, **kwargs) -> ToolResult:
        # Use self.config.api_key, etc.
        pass
```

## Error Handling Best Practices

### Framework Error Handling

```python
async def execute(self, **kwargs) -> ToolResult:
    # Framework provides automatic input validation and error handling
    if not kwargs.get("required_param"):
        return ToolResult(error="Missing required parameter")

    # Execute tool logic - framework handles network errors, timeouts, etc.
    result = await self.do_work(**kwargs)

    return ToolResult(
        output=result,
        system="Operation completed successfully"
    )
```

### Framework Monitoring

```python
from spoon_ai.tools.base import BaseTool, ToolResult

class MonitoredTool(BaseTool):
    async def execute(self, **kwargs) -> ToolResult:
        # Framework provides automatic logging and monitoring
        result = await self.do_work(**kwargs)

        # Framework tracks:
        # - Execution time and performance metrics
        # - Success/failure rates
        # - Parameter usage patterns
        # - Error frequencies and types
        return ToolResult(output=result)
```

## Testing Your Tools

### Unit Testing

```python
import pytest
from your_tools import MyCustomTool

@pytest.mark.asyncio
async def test_my_custom_tool():
    tool = MyCustomTool()

    # Test successful execution
    result = await tool.execute(input_data="test data")
    assert result.output is not None
    assert result.error is None

    # Test error handling
    result = await tool.execute(input_data="")
    assert result.error is not None

@pytest.mark.asyncio
async def test_tool_parameters():
    tool = MyCustomTool()

    # Test with optional parameters
    result = await tool.execute(
        input_data="test",
        options={"format": "uppercase"}
    )
    assert "TEST" in result.output
```

### Integration Testing

```python
from spoon_ai.tools.tool_manager import ToolManager
from your_tools import MyCustomTool

@pytest.mark.asyncio
async def test_tool_manager_integration():
    tool_manager = ToolManager([MyCustomTool()])

    # Test tool execution through manager
    result = await tool_manager.execute(
        name="my_custom_tool",
        tool_input={"input_data": "test"}
    )

    assert result.output is not None
```

## Tool Discovery and Documentation

### Auto-generating Tool Docs

```python
def generate_tool_docs(tools: List[BaseTool]) -> str:
    """Generate markdown documentation for tools"""
    docs = "# Available Tools"

    for tool in tools:
        docs += f"## {tool.name}"
        docs += f"{tool.description}"
        docs += "### Parameters"


        for param, config in tool.parameters.get("properties", {}).items():
            required = param in tool.parameters.get("required", [])
            docs += f"- **{param}** ({'required' if required else 'optional'}): {config.get('description', '')}"

        docs += "\n"

    return docs
```

### Tool Registry

```python
class ToolRegistry:
    """Central registry for tool discovery"""

    def __init__(self):
        self._tools = {}

    def register(self, tool_class: type):
        """Register a tool class"""
        tool = tool_class()
        self._tools[tool.name] = tool_class
        return tool_class

    def get_tool(self, name: str) -> BaseTool:
        """Get tool instance by name"""
        if name not in self._tools:
            raise ValueError(f"Tool {name} not found")
        return self._tools[name]()

    def list_tools(self) -> List[str]:
        """List all registered tool names"""
        return list(self._tools.keys())

# Usage
registry = ToolRegistry()

@registry.register
class MyTool(BaseTool):
    # Tool implementation
    pass
```

## Best Practices

### 1. Tool Naming

- Use descriptive, action-oriented names
- Follow snake_case convention
- Avoid generic names like "tool" or "helper"

### 2. Parameter Design

- Provide clear descriptions for all parameters
- Use appropriate data types and validation
- Set sensible defaults for optional parameters

### 3. Error Messages

- Be specific about what went wrong
- Include suggestions for fixing issues
- Don't expose sensitive information in errors

### 4. Performance

- Use async/await for I/O operations
- Leverage framework's built-in timeout handling
- Cache results when appropriate

### 5. Security

- Validate all inputs thoroughly
- Use environment variables for secrets
- Rely on framework's rate limiting features

## Next Steps

### ðŸ“š **Custom Tool Examples**

#### ðŸ” [MCP Spoon Search Agent](../examples/mcp-spoon-search-agent.md)
**GitHub**: [View Source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/mcp/spoon_search_agent.py)

**Custom tool integration demonstrated:**
- MCP server integration with custom search tools
- Web search capabilities using Tavily MCP
- Custom error handling for external API calls
- Real-world custom tool deployment patterns

**Key learning points:**
- How to wrap external APIs as custom tools
- MCP server integration patterns
- Error handling for unreliable external services
- Tool validation and testing strategies

#### ðŸ“Š [Graph Crypto Analysis](../examples/graph-crypto-analysis.md)
**GitHub**: [View Source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/graph_crypto_analysis.py)

**Financial tool development:**
- Custom cryptocurrency data processing tools
- Real-time technical indicator calculations
- Multi-source data aggregation and validation
- Financial data error handling and recovery

**Key learning points:**
- Domain-specific tool development patterns
- Financial data validation techniques
- Multi-API integration strategies
- Performance optimization for data-intensive tools

#### ðŸŽ¯ [Intent Graph Demo](../examples/intent-graph-demo.md)
**GitHub**: [View Source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/intent_graph_demo.py)

**Advanced tool orchestration:**
- Custom routing and decision-making tools
- Memory management and context preservation tools
- Parallel processing coordination tools
- Performance monitoring and metrics tools

**Key learning points:**
- Complex tool interaction patterns
- State management in custom tools
- Performance optimization techniques
- Error recovery in multi-tool workflows

### ðŸ› ï¸ **Development Resources**

- **[Core Concepts: Tools](../core-concepts/tools.md)** - Complete tool system understanding
- **[MCP Protocol](../core-concepts/mcp-protocol.md)** - Advanced integration patterns
- **[Tool API Reference](../api-reference/spoon_ai/tools/)** - Complete development documentation

### ðŸ“– **Additional Resources**

- **[Built-in Tools Reference](../api-reference/spoon_ai/tools/)** - Explore existing tool implementations
- **[Graph System](../graph-system/index.md)** - Advanced workflow orchestration
- **[Agent Architecture](../core-concepts/agents.md)** - Tool-agent integration patterns

## Troubleshooting

### Common Issues

**Tool not found in manager:**

- Ensure tool is properly added to ToolManager
- Check tool name matches exactly
- Verify tool class inherits from BaseTool

**Parameter validation errors:**

- Check JSON schema syntax in parameters
- Ensure required parameters are marked correctly
- Validate parameter types match schema

**Execution failures:**

- Leverage framework's automatic error handling
- Check for missing dependencies or API keys
- Use framework's built-in debugging features

---

FILE: docs/how-to-guides/build-first-agent.md

# Build Your First Agent

This quickstart takes you from a simple setup to a fully functional AI agent in just a few minutes.

> **ðŸ’¡ New to AI-assisted development?**
>
> Check out the [Vibe Coding Guide](./vibe-coding.md) to learn how to use AI coding tools like Cursor or Windsurf to build SpoonOS agents more efficiently.

## Requirements

For these examples, you will need to:

- Install the SpoonOS SDK packages
- Set up an API key for your chosen LLM provider (OpenAI, Anthropic, Google, etc.)
- Set the appropriate environment variables:

```bash
# LLM Provider (choose one)
export OPENAI_API_KEY="your-openai-key"
# or export ANTHROPIC_API_KEY="your-anthropic-key"

# For Desearch tools (required for search examples)
export DESEARCH_API_KEY="your-desearch-key"
```

Although these examples use OpenAI by default, you can use any supported provider by changing the `llm_provider` parameter and setting up the appropriate API key.

### Installation

```bash
# Using uv (recommended)
uv venv .venv
source .venv/bin/activate            # macOS/Linux
# .\.venv\Scripts\Activate.ps1       # Windows (PowerShell)

uv pip install spoon-ai-sdk          # Core SDK
uv pip install spoon-toolkits        # Tools (web search, blockchain, etc.)

# Or using pip
pip install spoon-ai-sdk spoon-toolkits
```

## Build a Basic Agent

Start by creating a simple agent that can scrape web pages and answer questions. The agent uses the `WebScraperTool` to fetch real content from any URL.

> **Note:** This basic example only requires `OPENAI_API_KEY` - no additional API keys needed.

```python
import asyncio
from spoon_ai.agents import SpoonReactAI
from spoon_ai.chat import ChatBot
from spoon_toolkits import WebScraperTool

# Create your agent with web scraping capability
agent = SpoonReactAI(
    llm=ChatBot(llm_provider="openai", model_name="gpt-5.1-chat-latest"),
    tools=[WebScraperTool()],
    system_prompt="You are a helpful assistant that can read web pages."
)

# Run the agent
async def main():
    response = await agent.run(
        "Scrape https://news.ycombinator.com and tell me the top 3 stories"
    )
    print(response)

if __name__ == "__main__":
    asyncio.run(main())
```

## Build a Real-World Agent

Next, build a practical research assistant agent that demonstrates key production concepts:

1. **Detailed system prompts** for better agent behavior
2. **Multiple tools** that fetch real data from the web
3. **Model configuration** with built-in conversational memory
4. **Create and run the agent** as a fully functional assistant

Let's walk through each step:

### Step 1: Define the System Prompt

The system prompt defines your agent's role and behavior. Keep it specific and actionable:

```python
SYSTEM_PROMPT = """You are an expert research assistant who helps users find and analyze information.

You have access to these tools:
- desearch_web_search: Search the web for general information
- desearch_ai_search: Search across web, Reddit, Wikipedia, YouTube, and arXiv
- web_scraper: Fetch and read full content from any URL

When a user asks for information:
1. Use the appropriate search tool to find relevant sources
2. If needed, use web_scraper to read full articles
3. Synthesize the information and provide clear, cited answers

Always cite your sources with URLs when providing information."""
```

### Step 2: Create Tools

[Tools](/how-to-guides/add-custom-tools) let a model interact with external systems by calling functions you define.

SpoonOS provides pre-built tools that fetch real data:

```python
from spoon_toolkits import DesearchWebSearchTool, DesearchAISearchTool, WebScraperTool

# Web search - searches the web and returns real results
web_search = DesearchWebSearchTool()

# AI search - searches across multiple platforms (web, Reddit, Wikipedia, YouTube, arXiv)
ai_search = DesearchAISearchTool()

# Web scraper - fetches and cleans content from any URL
scraper = WebScraperTool()
```

> **ðŸ’¡ Tip**
>
> Tools should be well-documented: their name, description, and argument names become part of the model's prompt. Use clear, descriptive names and comprehensive parameter descriptions.

### Step 3: Configure Your Model

Set up your language model with built-in memory management:

```python
from spoon_ai.chat import ChatBot

llm = ChatBot(
    llm_provider="openai",
    model_name="gpt-5.1-chat-latest",
    enable_short_term_memory=True,
    short_term_memory_config={
        "max_tokens": 8000,
        "strategy": "summarize",  # or "trim"
        "messages_to_keep": 6,
    }
)
```

The `enable_short_term_memory=True` option automatically manages conversation history:

- **summarize**: Summarizes older messages when token limit is reached
- **trim**: Removes older messages to stay within token limits

SpoonOS supports multiple providers out of the box:

| Provider | Example Models | Documentation |
|----------|---------------|---------------|
| `openai` | gpt-5.1-chat-latest, gpt-5-mini, gpt-4.1, o3, o4-mini | [OpenAI Models](https://platform.openai.com/docs/models) |
| `anthropic` | claude-opus-4-5, claude-sonnet-4-5, claude-haiku-4-5 | [Anthropic Models](https://docs.anthropic.com/en/docs/about-claude/models) |
| `gemini` | gemini-3-pro-preview, gemini-2.5-flash | [Gemini Models](https://ai.google.dev/gemini-api/docs/models/gemini) |
| `deepseek` | deepseek-chat, deepseek-reasoner | [DeepSeek Models](https://api-docs.deepseek.com/quick_start/pricing) |
| `openrouter` | 100+ models | [OpenRouter Models](https://openrouter.ai/models) |

### Step 4: Create and Run the Agent

Now assemble your agent with all the components and run it!

```python
import asyncio
from spoon_ai.agents import SpoonReactAI
from spoon_ai.chat import ChatBot
from spoon_toolkits import DesearchWebSearchTool, DesearchAISearchTool, WebScraperTool

# System prompt
SYSTEM_PROMPT = """You are an expert research assistant who helps users find and analyze information.

You have access to these tools:
- desearch_web_search: Search the web for general information
- desearch_ai_search: Search across web, Reddit, Wikipedia, YouTube, and arXiv
- web_scraper: Fetch and read full content from any URL

Always cite your sources with URLs when providing information."""

# Configure model with memory
llm = ChatBot(
    llm_provider="openai",
    model_name="gpt-5.1-chat-latest",
    enable_short_term_memory=True,
    short_term_memory_config={
        "max_tokens": 8000,
        "strategy": "summarize",
        "messages_to_keep": 6,
    }
)

# Create agent with real data tools
agent = SpoonReactAI(
    llm=llm,
    system_prompt=SYSTEM_PROMPT,
    tools=[
        DesearchWebSearchTool(),
        DesearchAISearchTool(),
        WebScraperTool(),
    ],
    max_steps=10,
)

# Run agent with conversation
async def main():
    # First query - searches the web for real data
    response = await agent.run(
        "What are the latest developments in AI agents?"
    )
    print("Agent:", response)

    # Follow-up query - agent remembers context
    response = await agent.run(
        "Can you find academic papers about that topic on arXiv?"
    )
    print("Agent:", response)

if __name__ == "__main__":
    asyncio.run(main())
```

### Optional: Stream Responses

If you want token-by-token output for a more interactive experience:

```python
async def stream_demo():
    llm = ChatBot(
        llm_provider="openai",
        model_name="gpt-5.1-chat-latest"
    )

    messages = [
        {"role": "user", "content": "Explain machine learning in 3 sentences"}
    ]

    async for chunk in llm.astream(messages=messages):
        print(chunk.delta or "", end="", flush=True)
    print()  # New line at end
```

## Full Example Code

<details>
<summary>Click to expand complete production-ready example</summary>

```python
"""
Production-ready SpoonOS research assistant agent.

This example demonstrates:
- Real-time web search via Desearch tools
- Web page scraping for full content
- Conversation memory with automatic summarization
"""
import asyncio
import logging

from spoon_ai.agents import SpoonReactAI
from spoon_ai.chat import ChatBot
from spoon_toolkits import DesearchWebSearchTool, DesearchAISearchTool, WebScraperTool

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Define system prompt
SYSTEM_PROMPT = """You are an expert research assistant who helps users find and analyze information.

You have access to these tools:
- desearch_web_search: Search the web for general information
- desearch_ai_search: Search across web, Reddit, Wikipedia, YouTube, and arXiv platforms
- web_scraper: Fetch and read full content from any URL

When a user asks for information:
1. Use the appropriate search tool to find relevant sources
2. If needed, use web_scraper to read full articles for deeper analysis
3. Synthesize the information and provide clear, cited answers

Always cite your sources with URLs when providing information.
Be thorough but concise in your analysis."""


def create_agent() -> SpoonReactAI:
    """Create and configure the research assistant agent."""

    # Configure model with memory management
    llm = ChatBot(
        llm_provider="openai",
        model_name="gpt-5.1-chat-latest",
        enable_short_term_memory=True,
        short_term_memory_config={
            "max_tokens": 8000,
            "strategy": "summarize",
            "messages_to_keep": 6,
        }
    )

    # Assemble real data tools
    tools = [
        DesearchWebSearchTool(),
        DesearchAISearchTool(),
        WebScraperTool(),
    ]

    # Create agent
    agent = SpoonReactAI(
        llm=llm,
        system_prompt=SYSTEM_PROMPT,
        tools=tools,
        max_steps=10,
    )

    logger.info("Agent created with %d tools", len(tools))
    return agent


async def interactive_session():
    """Run an interactive chat session with the agent."""
    agent = create_agent()

    print("\nResearch Assistant Agent Ready!")
    print("Type 'quit' or 'exit' to end the session.\n")

    while True:
        try:
            user_input = input("You: ").strip()

            if not user_input:
                continue

            if user_input.lower() in ['quit', 'exit', 'bye']:
                print("Goodbye!")
                break

            # Run agent and get response
            response = await agent.run(user_input)
            print(f"\nAgent: {response}\n")

        except KeyboardInterrupt:
            print("\nSession ended.")
            break
        except Exception as e:
            logger.error("Error: %s", e)
            print(f"Error occurred: {e}")


async def demo_queries():
    """Demonstrate agent capabilities with sample queries."""
    agent = create_agent()

    queries = [
        "Search for the latest news about large language models",
        "Find academic papers about transformer architectures on arXiv",
        "Scrape https://news.ycombinator.com and summarize the top stories",
    ]

    print("\nRunning Demo Queries\n" + "=" * 50)

    for query in queries:
        print(f"\nUser: {query}")
        response = await agent.run(query)
        print(f"Agent: {response}")

    print("\n" + "=" * 50 + "\nDemo Complete!")


if __name__ == "__main__":
    import sys

    if "--demo" in sys.argv:
        asyncio.run(demo_queries())
    else:
        asyncio.run(interactive_session())
```

</details>

## What You've Built

Congratulations! You now have an AI agent that can:

- **Search the web** in real-time using Desearch tools
- **Read full web pages** with the WebScraper tool
- **Find academic papers** across platforms like arXiv
- **Remember conversations** via short-term memory
- **Use multiple tools** intelligently based on user queries
- **Stream responses** for real-time output

## Next Steps

Now that you've built your first agent, explore these advanced topics:

- [Add Custom Tools](./add-custom-tools.md) - Create specialized tools for your domain
- [Graph-Based Workflows](./graph-based-workflows.md) - Build complex multi-agent systems
- [MCP Integration](./mcp-integration.md) - Connect to external tool servers
- [Long-Term Memory](./long-term-memory.md) - Integrate Mem0 for persistent knowledge

---

**ðŸ“ Edit this page** â€” Found an issue? [Edit the source on GitHub](https://github.com/XSpoonAi/spoon-ai/edit/main/cookbook/docs/how-to-guides/build-first-agent.md).

---

FILE: docs/how-to-guides/vibe-coding.md

# Vibe Coding / IDE AI Guide

Give Cursor, Codex, Claude Code, and similar assistants the right context to generate accurate XSpoonAi code. Pick **one** of the four methods below; they are independent, ordered from most guided to most manual.

## Method 1: Use MCP / online retrieval (no local clone)

- Configure MCP connectors (e.g., deepwiki/context7) to fetch files from GitHub or the web.
- Target upstream repos directly: `XSpoonAi/spoon-core` and `XSpoonAi/toolkit` (source + examples) plus this Cookbook site for docs.
- Instruct the assistant to: list relevant files, fetch the exact file contents it will mimic, then cite paths/lines. No need to mirror or clone locally.

## Method 2: Supply the bundled docs file (`cookbook/llm.txt`)

- The repo ships an auto-generated bundle of every Markdown doc at `cookbook/llm.txt` (CI keeps it fresh).
- Share that single file with your LLM to give it the full Cookbook context for Vibe Coding.
- GitHub copy (fallback): [`llm.txt`](https://github.com/XSpoonAi/xspoonai.github.io/blob/main/llm.txt).

## Method 3: Point to installed package paths

- If you installed editable/local packages, expose their locations instead of the repo:
  ```bash
  python -c "import importlib.util, os; spec = importlib.util.find_spec('spoon_ai'); print('spoon_ai:', os.path.dirname(spec.origin) if spec and spec.origin else 'not installed')"
  python -c "import importlib.util, os; spec = importlib.util.find_spec('spoon_toolkits'); print('spoon_toolkits:', os.path.dirname(spec.origin) if spec and spec.origin else 'not installed')"
  ```
- Provide those paths to the assistant as read-only references so it can scan the actual shipped code and examples inside the installed packages.
- If a package isnâ€™t installed (output shows â€œnot installedâ€), just share the corresponding repo directory (e.g., `core/spoon_ai` or `toolkit/spoon_toolkits`) instead.

## Method 4: Share the workspace directly

- Allow the assistant to read this repo (especially `core/`, `toolkit/`, `cookbook/docs/`). No extra cloning required.
- Need a local checkout first? From your working directory run:
  ```bash
  git clone https://github.com/XSpoonAi/spoon-core.git
  git clone https://github.com/XSpoonAi/spoon-toolkit.git
  ```
- Keep the repo up to date (`git pull`) so the assistant always sees current code and docs.
- Ask it to open real source first, e.g. `core/spoon_ai/**` and runnable samples in `core/examples/` and `toolkit/**/examples`.
- Tell the AI to derive function signatures and configs from code files or examples, not invented abstractions.

## Hallucination-reduction tips (tell the AI)

- "Read the source or examples you will copy from; cite file path and line range before coding."
- "Confirm tool/agent/LLM signatures from code, not from guesswork; show the imports you plan to use."
- "If unsure, fetch the specific file via MCP and restate the interface before implementing."

---

FILE: docs/how-to-guides/x402-payments.md

# x402 Integration Guide

The x402 documentation is now split by audience so you can jump directly to the right level of detail.

## 1. Core concepts

Read [Core Concepts: x402 payments](../core-concepts/x402-payments.md) to understand:

- Why SpoonOS uses x402 to gate agent actions.
- Required environment variables and configuration fallbacks.
- The end-to-end payment lifecycle (probe -> sign -> retry -> settle) plus the flow diagram.

## 2. API reference

Consult [API Reference: x402](../api-reference/spoon_ai/payments/) for:

- Python services (`X402PaymentService`, request/response models, helper methods).
- Built-in tools (`x402_create_payment`, `x402_paywalled_request`) with parameter tables.
- CLI commands (`requirements`, `sign`) and the FastAPI paywall endpoints.
- Environment and Turnkey integration notes.

## 3. Example

Follow [Example: x402 ReAct agent](../examples/x402-react-agent.md) to run the ReAct demo (`uv run python examples/x402_agent_demo.py`). It walks through:

- Preparing `.env` and funding the signer.
- Observing the agent call `web_scraper`, then `x402_paywalled_request`, retrieve the protected page, and print the signed `PAYMENT-SIGNATURE` + settlement receipt.
- Troubleshooting common facilitator or configuration errors.

> Note: x402 v2 uses `PAYMENT-REQUIRED` / `PAYMENT-SIGNATURE` / `PAYMENT-RESPONSE` headers. SpoonOS tools still support the legacy `X-PAYMENT` / `X-PAYMENT-RESPONSE` headers for older v1 paywalls.

## Quick checklist

- Fund the signer via [Circle faucet](https://faucet.circle.com/).
- Keep `PRIVATE_KEY` populated; rely on Turnkey only when the local key is absent.
- Reuse `X402PaymentService.decode_payment_response()` anywhere you archive facilitator receipts.

---

FILE: docs/troubleshooting/common-issues.md

# Common Issues and Solutions

This guide covers the most frequently encountered issues when working with SpoonOS and their solutions.

## Installation Issues

### Python Version Compatibility

**Problem:** `ImportError` or `ModuleNotFoundError` when importing SpoonOS modules

**Symptoms:**
```bash
ImportError: No module named 'spoon_ai'
ModuleNotFoundError: No module named 'asyncio'
```

**Solution:**
1. Ensure Python 3.12+ is installed:
   ```bash
   python --version
   # Should show Python 3.12.0 or higher
   ```

2. Create a new virtual environment:
   ```bash
   python -m venv spoon-env
   source spoon-env/bin/activate  # Linux/macOS
   # or
   spoon-env\\Scripts\\activate     # Windows
   ```

3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

### Dependency Conflicts

**Problem:** Package version conflicts during installation

**Symptoms:**
```bash
ERROR: pip's dependency resolver does not currently have a solution
Conflicting dependencies: package-a requires package-b>=2.0, but package-c requires package-b<2.0
```

**Solution:**
1. Use a fresh virtual environment
2. Install packages one by one to identify conflicts
3. Check `requirements.txt` for version pinning issues
4. Use `pip install --upgrade` for outdated packages

## Configuration Issues

### API Key Problems

**Problem:** Authentication errors with LLM providers

**Symptoms:**
```bash
AuthenticationError: Invalid API key
Unauthorized: API key not found
```

**Solution:**
1. Verify API keys are set correctly:
   ```bash
   echo $OPENAI_API_KEY
   echo $ANTHROPIC_API_KEY
   ```

2. Check `.env` file format:
   ```bash
   # Correct format
   OPENAI_API_KEY=sk-your-actual-key-here

   # Incorrect (no quotes needed)
   OPENAI_API_KEY="sk-your-actual-key-here"
   ```

3. Validate API key format:
   - OpenAI: Starts with `sk-`
   - Anthropic: Starts with `sk-ant-`
   - Google: 39-character string

4. Test API key validity:
   ```bash
   curl -H "Authorization: Bearer $OPENAI_API_KEY" \\
        https://api.openai.com/v1/models
   ```

### Configuration File Errors

**Problem:** JSON parsing errors in `config.json`

**Symptoms:**
```bash
JSONDecodeError: Expecting ',' delimiter
ConfigurationError: Invalid configuration format
```

**Solution:**
1. Validate JSON syntax using online validator or:
   ```bash
   python -m json.tool config.json
   ```

2. Common JSON errors:
   ```json
   // Wrong: trailing comma
   {
     "key": "value",
   }

   // Correct
   {
     "key": "value"
   }
   ```

3. Use CLI validation:
   ```bash
   python main.py
   > validate-config
   ```

## Agent Issues

### Agent Loading Failures

**Problem:** Agent fails to load or initialize

**Symptoms:**
```bash
AgentError: Agent 'my_agent' not found
ImportError: cannot import name 'MyAgent'
```

**Solution:**

1. Verify agent class exists:
   ```python
   from spoon_ai.agents import SpoonReactAI
   # Should not raise ImportError
   ```

2. Check for typos in agent names and class names

3. List available agents:
   ```bash
   python main.py
   > list-agents
   ```

### Tool Loading Issues (CLI `config.json`)

**Problem:** Tools not available or failing to load

**Symptoms:**
```bash
ToolError: Tool 'crypto_tool' not found
ModuleNotFoundError: No module named 'spoon_toolkits'
```

**Solution:**
1. Install spoon-toolkit package:
   ```bash
   pip install spoon-toolkits
   ```

2. Verify environment variables for tools:
   ```bash
   echo $OKX_API_KEY
   echo $COINGECKO_API_KEY
   ```

3. List available tools:
   ```bash
   python main.py
   > list-toolkit-categories
   > list-toolkit-tools crypto
   ```

## LLM Provider Issues

### Provider Connection Failures

**Problem:** Cannot connect to LLM providers

**Symptoms:**
```bash
ConnectionError: Failed to connect to OpenAI API
TimeoutError: Request timed out
```

**Solution:**
1. Check internet connectivity:
   ```bash
   ping api.openai.com
   ping api.anthropic.com
   ```

2. Verify API endpoints are accessible:
   ```bash
   curl -I https://api.openai.com/v1/models
   ```

3. Check firewall and proxy settings

4. Test with different provider:
   ```bash
   python main.py
   > llm-status
   ```

## MCP (Model Context Protocol) Issues

### MCP Server Connection Problems

**Problem:** Cannot connect to MCP servers

**Symptoms:**
```bash
MCPError: Failed to connect to MCP server
ConnectionRefusedError: [Errno 111] Connection refused
```

**Solution:**
1. Verify MCP server is running:
   ```bash
   curl http://localhost:8765/health
   ```

2. Check MCP server configuration:
   ```json
   {
     "mcp_servers": [
       {
         "name": "my_server",
         "url": "http://localhost:8765",
         "transport": "sse"
       }
     ]
   }
   ```

3. Start MCP server:
   ```bash
   python mcp_server.py
   ```

4. Check server logs for errors

### MCP Tool Discovery Issues

**Problem:** MCP tools not discovered or available

**Symptoms:**
```bash
MCPError: No tools found on server
ToolError: MCP tool 'my_tool' not available
```

**Solution:**
1. Verify tools are registered on MCP server:
   ```python
   @mcp.tool()
   def my_tool():
       return "Hello from MCP"
   ```

2. Check MCP server tool listing:
   ```bash
   curl http://localhost:8765/tools
   ```

3. Restart MCP server after adding tools

4. Verify tool permissions and authentication

## Performance Issues

### Slow Response Times

**Problem:** Agent responses are very slow

**Symptoms:**
- Long delays before responses
- Timeout errors
- High CPU/memory usage

**Solution:**
1. Check system resources:
   ```bash
   python main.py
   > system-info
   ```

2. Optimize LLM configuration:
   ```json
   {
     "llm": {
       "temperature": 0.7,
       "max_tokens": 1000,
       "timeout": 30
     }
   }
   ```

3. Enable caching:
   ```json
   {
     "cache": {
       "enabled": true,
       "ttl": 3600
     }
   }
   ```

4. Reduce tool complexity and number of tools

### Memory Issues

**Problem:** High memory usage or out-of-memory errors

**Symptoms:**
```bash
MemoryError: Unable to allocate memory
Process killed (OOM)
```

**Solution:**
1. Monitor memory usage:
   ```bash
   python main.py
   > system-info
   ```

2. Reduce conversation history:
   ```bash
   python main.py
   > new-chat
   ```

3. Optimize agent configuration:
   ```json
   {
     "config": {
       "max_steps": 5,
       "max_tokens": 500
     }
   }
   ```

4. Use lighter LLM models (e.g., GPT-3.5 instead of GPT-4)

## Blockchain Integration Issues

### RPC Connection Problems

**Problem:** Cannot connect to blockchain RPC endpoints

**Symptoms:**
```bash
ConnectionError: Failed to connect to RPC
HTTPError: 403 Forbidden
```

**Solution:**
1. Verify RPC URL is correct:
   ```bash
   curl -X POST -H "Content-Type: application/json" \\
        --data '{"jsonrpc":"2.0","method":"eth_blockNumber","params":[],"id":1}' \\
        $RPC_URL
   ```

2. Check RPC provider limits and authentication

3. Try alternative RPC endpoints:
   ```bash
   # Ethereum
   export RPC_URL="https://eth.llamarpc.com"
   export RPC_URL="https://rpc.ankr.com/eth"
   ```

4. Verify network connectivity and firewall settings

### Transaction Failures

**Problem:** Blockchain transactions fail or revert

**Symptoms:**
```bash
TransactionError: Transaction reverted
InsufficientFundsError: Not enough balance
```

**Solution:**
1. Check wallet balance:
   ```bash
   python main.py
   > token-by-symbol ETH
   ```

2. Verify gas settings:
   ```json
   {
     "blockchain": {
       "gas_limit": 21000,
       "gas_price": "20000000000"
     }
   }
   ```

3. Check transaction parameters and recipient address

4. Verify private key and wallet configuration

## Debugging Techniques

### Enable Debug Logging

```bash
# Set environment variables
export DEBUG=true
export LOG_LEVEL=debug

# Run with verbose output
python main.py
```

### Use System Diagnostics

```bash
python main.py
> system-info
> llm-status
> validate-config
```

### Check Configuration

```bash
# Validate configuration
python main.py
> validate-config

# Check migration status
python main.py
> check-config
```

### Test Individual Components

```python
# Test LLM connection
from spoon_ai.llm import LLMManager
llm = LLMManager()
response = await llm.generate("Hello, world!")

# Test tool execution
from spoon_toolkits.crypto import GetTokenPriceTool
tool = GetTokenPriceTool()
result = await tool.execute(symbol="BTC")
```

## Getting Help

### Documentation Resources
- [Installation Guide](../getting-started/installation.md)
- [Configuration Guide](../getting-started/configuration.md)
- [API Reference](../api-reference/index)
- [How-To Guides](../how-to-guides/)

### ðŸ“š **Working Examples**

#### ðŸŽ¯ [Intent Graph Demo](../examples/intent-graph-demo.md)
**GitHub**: [View Source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/intent_graph_demo.py)

**Perfect for troubleshooting:**
- Graph system setup and configuration
- Memory management and state persistence issues
- Parallel execution and routing problems
- Production deployment patterns

#### ðŸ” [MCP Spoon Search Agent](../examples/mcp-spoon-search-agent.md)
**GitHub**: [View Source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/mcp/spoon_search_agent.py)

**Great for debugging:**
- MCP server connection and integration issues
- Tool discovery and loading problems
- API rate limiting and error handling
- Multi-tool orchestration challenges

#### ðŸ“Š [Graph Crypto Analysis](../examples/graph-crypto-analysis.md)
**GitHub**: [View Source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/graph_crypto_analysis.py)

**Excellent for testing:**
- Real API integration and authentication
- Data processing and validation issues
- Performance optimization problems
- Complex workflow debugging

### Community Support
- GitHub Issues: Report bugs and feature requests
- Discord: Real-time community support
- Documentation: Comprehensive guides and working examples

### Diagnostic Information
When reporting issues, include:
- Python version (`python --version`)
- SpoonOS version
- Operating system
- Error messages and stack traces
- Configuration files (sanitized)
- Steps to reproduce

### Log Collection

```bash
# Enable debug logging
export DEBUG=true
export LOG_LEVEL=debug

# Capture logs
python main.py 2>&1 | tee spoon_debug.log

# Include relevant log sections in issue reports
```

## Prevention Tips

### Regular Maintenance
- Keep dependencies updated
- Rotate API keys regularly
- Monitor system resources
- Backup configuration files
- Test in development environment first

### Best Practices
- Use version control for configurations
- Implement proper error handling
- Monitor API usage and costs
- Set up alerts for critical issues
- Document custom configurations

### Environment Management
- Use separate environments for development/production
- Pin dependency versions in requirements.txt
- Use environment variables for sensitive data
- Regularly test backup and recovery procedures

## See Also

- [Debugging Guide](./debugging.md)
- [Performance Optimization](./performance.md)
- [System Requirements](../getting-started/installation.md)"}

## Prevention Tips

### Regular Maintenance
- Keep dependencies updated
- Rotate API keys regularly
- Monitor system resources
- Backup configuration files
- Test in development environment first

### Best Practices
- Use version control for configurations
- Implement proper error handling
- Monitor API usage and costs
- Set up alerts for critical issues
- Document custom configurations

### Environment Management
- Use separate environments for development/production
- Pin dependency versions in requirements.txt
- Use environment variables for sensitive data
- Regularly test backup and recovery procedures

## See Also

- [Debugging Guide](./debugging.md)
- [Performance Optimization](./performance.md)
- [System Requirements](../getting-started/installation.md)"}

---

FILE: docs/troubleshooting/debugging.md

# Debugging Guide

Comprehensive guide for debugging SpoonOS applications, agents, and tools.

## Debug Configuration

### Environment Variables

```bash
# Enable debug mode
export DEBUG=true
export LOG_LEVEL=debug

# Enable specific debug categories
export DEBUG_AGENTS=true
export DEBUG_TOOLS=true
export DEBUG_LLM=true
export DEBUG_MCP=true

# Enable request/response logging
export LOG_REQUESTS=true
export LOG_RESPONSES=true
```

### Configuration File Debug Settings

```json
{
  "debug": {
    "enabled": true,
    "log_level": "debug",
    "categories": ["agents", "tools", "llm", "mcp"],
    "log_requests": true,
    "log_responses": true,
    "save_logs": true,
    "log_file": "spoon_debug.log"
  }
}
```

## Logging Setup

### Python Logging Configuration

```python
# debug_setup.py
import logging
import sys
from datetime import datetime

def setup_debug_logging():
    """Configure comprehensive debug logging"""
    
    # Create formatter
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s'
    )
    
    # Console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.DEBUG)
    console_handler.setFormatter(formatter)
    
    # File handler
    file_handler = logging.FileHandler(
        f'spoon_debug_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'
    )
    file_handler.setLevel(logging.DEBUG)
    file_handler.setFormatter(formatter)
    
    # Configure root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.DEBUG)
    root_logger.addHandler(console_handler)
    root_logger.addHandler(file_handler)
    
    # Configure SpoonOS loggers
    spoon_logger = logging.getLogger('spoon_ai')
    spoon_logger.setLevel(logging.DEBUG)
    
    toolkit_logger = logging.getLogger('spoon_toolkits')
    toolkit_logger.setLevel(logging.DEBUG)
    
    print("Debug logging configured")

if __name__ == "__main__":
    setup_debug_logging()
```

### Structured Logging

```python
# structured_logging.py
import structlog
import json
from datetime import datetime

def setup_structured_logging():
    """Configure structured logging with JSON output"""
    
    structlog.configure(
        processors=[
            structlog.stdlib.filter_by_level,
            structlog.stdlib.add_logger_name,
            structlog.stdlib.add_log_level,
            structlog.stdlib.PositionalArgumentsFormatter(),
            structlog.processors.TimeStamper(fmt="iso"),
            structlog.processors.StackInfoRenderer(),
            structlog.processors.format_exc_info,
            structlog.processors.UnicodeDecoder(),
            structlog.processors.JSONRenderer()
        ],
        context_class=dict,
        logger_factory=structlog.stdlib.LoggerFactory(),
        wrapper_class=structlog.stdlib.BoundLogger,
        cache_logger_on_first_use=True,
    )
    
    return structlog.get_logger()

# Usage example
logger = setup_structured_logging()
logger.info("Agent started", agent_name="debug_agent", tools_count=5)
logger.error("Tool execution failed", tool_name="crypto_tool", error="API timeout")
```

## Agent Debugging

### Agent State Inspection

```python
# agent_debugger.py
from spoon_ai.agents import SpoonReactAI
from spoon_ai.tools import ToolManager
import json

class DebuggableAgent(SpoonReactAI):
    """Agent with enhanced debugging capabilities"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.debug_info = {
            "steps": [],
            "tool_calls": [],
            "llm_requests": [],
            "errors": []
        }
    
    async def run(self, message: str, **kwargs):
        """Run with debug tracking"""
        self.debug_info["steps"].append({
            "timestamp": datetime.now().isoformat(),
            "action": "run_started",
            "message": message,
            "kwargs": kwargs
        })
        
        try:
            result = await super().run(message, **kwargs)
            
            self.debug_info["steps"].append({
                "timestamp": datetime.now().isoformat(),
                "action": "run_completed",
                "result_length": len(str(result))
            })
            
            return result
            
        except Exception as e:
            self.debug_info["errors"].append({
                "timestamp": datetime.now().isoformat(),
                "error_type": type(e).__name__,
                "error_message": str(e),
                "traceback": traceback.format_exc()
            })
            raise
    
    def get_debug_info(self) -> dict:
        """Get comprehensive debug information"""
        return {
            "agent_name": self.name,
            "system_prompt": self.system_prompt,
            "config": self.config,
            "available_tools": [tool.name for tool in self.tools],
            "debug_info": self.debug_info
        }
    
    def save_debug_info(self, filename: str = None):
        """Save debug information to file"""
        if not filename:
            filename = f"debug_{self.name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(filename, 'w') as f:
            json.dump(self.get_debug_info(), f, indent=2)
        
        print(f"Debug info saved to {filename}")

# Usage
async def debug_agent_example():
    agent = DebuggableAgent(
        name="debug_agent",
        system_prompt="You are a debugging assistant."
    )
    
    try:
        response = await agent.run("Hello, debug me!")
        print(f"Response: {response}")
    except Exception as e:
        print(f"Error: {e}")
    finally:
        agent.save_debug_info()
        print(json.dumps(agent.get_debug_info(), indent=2))
```

### Step-by-Step Execution Tracing

```python
# execution_tracer.py
from spoon_ai.agents.base import BaseAgent
from functools import wraps
import inspect

def trace_execution(func):
    """Decorator to trace function execution"""
    @wraps(func)
    async def wrapper(self, *args, **kwargs):
        func_name = func.__name__
        
        # Log function entry
        logger.debug(
            "Function entry",
            function=func_name,
            args=args,
            kwargs=kwargs,
            agent=getattr(self, 'name', 'unknown')
        )
        
        try:
            # Execute function
            result = await func(self, *args, **kwargs)
            
            # Log successful completion
            logger.debug(
                "Function success",
                function=func_name,
                result_type=type(result).__name__,
                agent=getattr(self, 'name', 'unknown')
            )
            
            return result
            
        except Exception as e:
            # Log error
            logger.error(
                "Function error",
                function=func_name,
                error_type=type(e).__name__,
                error_message=str(e),
                agent=getattr(self, 'name', 'unknown')
            )
            raise
    
    return wrapper

class TracedAgent(BaseAgent):
    """Agent with method tracing"""
    
    @trace_execution
    async def run(self, message: str, **kwargs):
        return await super().run(message, **kwargs)
```

## Tool Debugging

### Tool Execution Monitoring

```python
# tool_debugger.py
from spoon_ai.tools.base import BaseTool
from spoon_ai.tools.errors import ToolError
import time
import traceback

class DebuggableTool(BaseTool):
    """Base tool with debugging capabilities"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.execution_history = []
        self.performance_stats = {
            "total_calls": 0,
            "successful_calls": 0,
            "failed_calls": 0,
            "average_duration": 0,
            "total_duration": 0
        }
    
    async def execute(self, **kwargs):
        """Execute with comprehensive debugging"""
        start_time = time.time()
        execution_id = f"{self.name}_{int(start_time)}"
        
        # Log execution start
        logger.debug(
            "Tool execution started",
            tool=self.name,
            execution_id=execution_id,
            parameters=kwargs
        )
        
        try:
            # Validate parameters
            validated_params = self.validate_parameters(**kwargs)
            
            # Execute tool
            result = await self._execute_impl(**validated_params)
            
            # Calculate duration
            duration = time.time() - start_time
            
            # Update stats
            self._update_success_stats(duration)
            
            # Log execution success
            logger.debug(
                "Tool execution completed",
                tool=self.name,
                execution_id=execution_id,
                duration=duration,
                result_type=type(result).__name__
            )
            
            # Store execution history
            self.execution_history.append({
                "execution_id": execution_id,
                "timestamp": start_time,
                "duration": duration,
                "parameters": validated_params,
                "success": True,
                "result_type": type(result).__name__
            })
            
            return result
            
        except Exception as e:
            duration = time.time() - start_time
            
            # Update stats
            self._update_error_stats(duration)
            
            # Log execution error
            logger.error(
                "Tool execution failed",
                tool=self.name,
                execution_id=execution_id,
                duration=duration,
                error_type=type(e).__name__,
                error_message=str(e),
                traceback=traceback.format_exc()
            )
            
            # Store execution history
            self.execution_history.append({
                "execution_id": execution_id,
                "timestamp": start_time,
                "duration": duration,
                "parameters": kwargs,
                "success": False,
                "error_type": type(e).__name__,
                "error_message": str(e)
            })
            
            raise
    
    async def _execute_impl(self, **kwargs):
        """Override this method in subclasses"""
        raise NotImplementedError
    
    def _update_success_stats(self, duration: float):
        """Update performance statistics for successful execution"""
        self.performance_stats["total_calls"] += 1
        self.performance_stats["successful_calls"] += 1
        self.performance_stats["total_duration"] += duration
        self.performance_stats["average_duration"] = (
            self.performance_stats["total_duration"] / 
            self.performance_stats["total_calls"]
        )
    
    def _update_error_stats(self, duration: float):
        """Update performance statistics for failed execution"""
        self.performance_stats["total_calls"] += 1
        self.performance_stats["failed_calls"] += 1
        self.performance_stats["total_duration"] += duration
        self.performance_stats["average_duration"] = (
            self.performance_stats["total_duration"] / 
            self.performance_stats["total_calls"]
        )
    
    def get_debug_info(self) -> dict:
        """Get comprehensive debug information"""
        return {
            "tool_name": self.name,
            "description": self.description,
            "performance_stats": self.performance_stats,
            "recent_executions": self.execution_history[-10:],  # Last 10 executions
            "total_executions": len(self.execution_history)
        }
```

### Parameter Validation Debugging

```python
# parameter_debugger.py
from spoon_ai.tools.base import BaseTool
from spoon_ai.tools.errors import ValidationError
import jsonschema

class ValidatedTool(DebuggableTool):
    """Tool with enhanced parameter validation and debugging"""
    
    # Define parameter schema
    parameter_schema = {
        "type": "object",
        "properties": {},
        "required": []
    }
    
    def validate_parameters(self, **kwargs) -> dict:
        """Validate parameters with detailed error reporting"""
        logger.debug(
            "Parameter validation started",
            tool=self.name,
            raw_parameters=kwargs,
            schema=self.parameter_schema
        )
        
        try:
            # Validate against schema
            jsonschema.validate(kwargs, self.parameter_schema)
            
            # Custom validation
            validated = self._custom_validation(**kwargs)
            
            logger.debug(
                "Parameter validation successful",
                tool=self.name,
                validated_parameters=validated
            )
            
            return validated
            
        except jsonschema.ValidationError as e:
            logger.error(
                "Schema validation failed",
                tool=self.name,
                validation_error=str(e),
                error_path=list(e.path),
                invalid_value=e.instance
            )
            raise ValidationError(f"Parameter validation failed: {e.message}")
        
        except Exception as e:
            logger.error(
                "Custom validation failed",
                tool=self.name,
                validation_error=str(e)
            )
            raise ValidationError(f"Parameter validation failed: {str(e)}")
    
    def _custom_validation(self, **kwargs) -> dict:
        """Override for custom validation logic"""
        return kwargs
```

## LLM Debugging

### Request/Response Logging

```python
# llm_debugger.py
from spoon_ai.llm.base import BaseLLMProvider
import json
import time

class DebuggableLLMProvider(BaseLLMProvider):
    """LLM provider with request/response logging"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.request_history = []
    
    async def generate(self, messages, **kwargs):
        """Generate with request/response logging"""
        request_id = f"req_{int(time.time() * 1000)}"
        start_time = time.time()
        
        # Log request
        logger.debug(
            "LLM request started",
            provider=self.provider_name,
            request_id=request_id,
            model=kwargs.get('model', self.default_model),
            message_count=len(messages),
            parameters=kwargs
        )
        
        # Log messages (truncated for privacy)
        for i, msg in enumerate(messages):
            content_preview = msg.get('content', '')[:100] + '...' if len(msg.get('content', '')) > 100 else msg.get('content', '')
            logger.debug(
                "LLM message",
                request_id=request_id,
                message_index=i,
                role=msg.get('role'),
                content_preview=content_preview
            )
        
        try:
            # Make request
            response = await super().generate(messages, **kwargs)
            
            duration = time.time() - start_time
            
            # Log response
            logger.debug(
                "LLM request completed",
                provider=self.provider_name,
                request_id=request_id,
                duration=duration,
                response_length=len(str(response)),
                tokens_used=response.get('usage', {}).get('total_tokens', 0)
            )
            
            # Store request history
            self.request_history.append({
                "request_id": request_id,
                "timestamp": start_time,
                "duration": duration,
                "model": kwargs.get('model', self.default_model),
                "message_count": len(messages),
                "success": True,
                "tokens_used": response.get('usage', {}).get('total_tokens', 0)
            })
            
            return response
            
        except Exception as e:
            duration = time.time() - start_time
            
            logger.error(
                "LLM request failed",
                provider=self.provider_name,
                request_id=request_id,
                duration=duration,
                error_type=type(e).__name__,
                error_message=str(e)
            )
            
            # Store request history
            self.request_history.append({
                "request_id": request_id,
                "timestamp": start_time,
                "duration": duration,
                "model": kwargs.get('model', self.default_model),
                "message_count": len(messages),
                "success": False,
                "error_type": type(e).__name__,
                "error_message": str(e)
            })
            
            raise
```

### Token Usage Monitoring

```python
# token_monitor.py
from collections import defaultdict
import time

class TokenUsageMonitor:
    """Monitor and analyze token usage patterns"""
    
    def __init__(self):
        self.usage_stats = defaultdict(lambda: {
            "total_requests": 0,
            "total_tokens": 0,
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_cost": 0.0,
            "average_tokens_per_request": 0,
            "requests_by_hour": defaultdict(int)
        })
    
    def record_usage(self, provider: str, model: str, usage: dict, cost: float = 0.0):
        """Record token usage for analysis"""
        key = f"{provider}:{model}"
        stats = self.usage_stats[key]
        
        # Update counters
        stats["total_requests"] += 1
        stats["total_tokens"] += usage.get("total_tokens", 0)
        stats["prompt_tokens"] += usage.get("prompt_tokens", 0)
        stats["completion_tokens"] += usage.get("completion_tokens", 0)
        stats["total_cost"] += cost
        
        # Update averages
        stats["average_tokens_per_request"] = (
            stats["total_tokens"] / stats["total_requests"]
        )
        
        # Track hourly usage
        hour = int(time.time() // 3600)
        stats["requests_by_hour"][hour] += 1
        
        logger.info(
            "Token usage recorded",
            provider=provider,
            model=model,
            tokens=usage.get("total_tokens", 0),
            cost=cost
        )
    
    def get_usage_report(self) -> dict:
        """Generate comprehensive usage report"""
        report = {
            "total_providers": len(self.usage_stats),
            "providers": {}
        }
        
        for key, stats in self.usage_stats.items():
            provider, model = key.split(":", 1)
            
            if provider not in report["providers"]:
                report["providers"][provider] = {
                    "models": {},
                    "total_requests": 0,
                    "total_tokens": 0,
                    "total_cost": 0.0
                }
            
            # Add model stats
            report["providers"][provider]["models"][model] = stats
            
            # Aggregate provider stats
            report["providers"][provider]["total_requests"] += stats["total_requests"]
            report["providers"][provider]["total_tokens"] += stats["total_tokens"]
            report["providers"][provider]["total_cost"] += stats["total_cost"]
        
        return report
    
    def print_usage_summary(self):
        """Print formatted usage summary"""
        report = self.get_usage_report()
        
        print("\
=== Token Usage Summary ===")
        print(f"Total Providers: {report['total_providers']}")
        
        for provider, provider_stats in report["providers"].items():
            print(f"\
{provider.upper()}:")
            print(f"  Total Requests: {provider_stats['total_requests']:,}")
            print(f"  Total Tokens: {provider_stats['total_tokens']:,}")
            print(f"  Total Cost: ${provider_stats['total_cost']:.4f}")
            
            for model, model_stats in provider_stats["models"].items():
                print(f"  {model}:")
                print(f"    Requests: {model_stats['total_requests']:,}")
                print(f"    Tokens: {model_stats['total_tokens']:,}")
                print(f"    Avg Tokens/Request: {model_stats['average_tokens_per_request']:.1f}")
                print(f"    Cost: ${model_stats['total_cost']:.4f}")
```

## MCP Debugging

### MCP Server Connection Debugging

```python
# mcp_debugger.py
from spoon_ai.tools.mcp_client import MCPClient
import asyncio
import aiohttp

class DebuggableMCPClient(MCPClient):
    """MCP client with enhanced debugging"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.connection_history = []
        self.tool_discovery_history = []
    
    async def connect(self):
        """Connect with connection debugging"""
        start_time = time.time()
        
        logger.debug(
            "MCP connection attempt",
            server_url=self.server_url,
            transport=self.transport
        )
        
        try:
            await super().connect()
            
            duration = time.time() - start_time
            
            logger.info(
                "MCP connection successful",
                server_url=self.server_url,
                duration=duration
            )
            
            self.connection_history.append({
                "timestamp": start_time,
                "duration": duration,
                "success": True
            })
            
        except Exception as e:
            duration = time.time() - start_time
            
            logger.error(
                "MCP connection failed",
                server_url=self.server_url,
                duration=duration,
                error_type=type(e).__name__,
                error_message=str(e)
            )
            
            self.connection_history.append({
                "timestamp": start_time,
                "duration": duration,
                "success": False,
                "error_type": type(e).__name__,
                "error_message": str(e)
            })
            
            raise
    
    async def discover_tools(self):
        """Discover tools with debugging"""
        start_time = time.time()
        
        logger.debug("MCP tool discovery started", server_url=self.server_url)
        
        try:
            tools = await super().discover_tools()
            
            duration = time.time() - start_time
            
            logger.info(
                "MCP tool discovery completed",
                server_url=self.server_url,
                tools_found=len(tools),
                duration=duration,
                tool_names=[tool.name for tool in tools]
            )
            
            self.tool_discovery_history.append({
                "timestamp": start_time,
                "duration": duration,
                "success": True,
                "tools_found": len(tools),
                "tool_names": [tool.name for tool in tools]
            })
            
            return tools
            
        except Exception as e:
            duration = time.time() - start_time
            
            logger.error(
                "MCP tool discovery failed",
                server_url=self.server_url,
                duration=duration,
                error_type=type(e).__name__,
                error_message=str(e)
            )
            
            self.tool_discovery_history.append({
                "timestamp": start_time,
                "duration": duration,
                "success": False,
                "error_type": type(e).__name__,
                "error_message": str(e)
            })
            
            raise
```

## Performance Debugging

### Performance Profiler

```python
# performance_profiler.py
import cProfile
import pstats
import io
from functools import wraps
import time
import psutil
import os

class PerformanceProfiler:
    """Profile performance of SpoonOS components"""
    
    def __init__(self):
        self.profiles = {}
        self.memory_snapshots = []
    
    def profile_function(self, func_name: str = None):
        """Decorator to profile function performance"""
        def decorator(func):
            name = func_name or f"{func.__module__}.{func.__name__}"
            
            @wraps(func)
            async def wrapper(*args, **kwargs):
                # Memory snapshot before
                process = psutil.Process(os.getpid())
                memory_before = process.memory_info().rss
                
                # CPU profiling
                profiler = cProfile.Profile()
                profiler.enable()
                
                start_time = time.time()
                
                try:
                    result = await func(*args, **kwargs)
                    
                    # Stop profiling
                    end_time = time.time()
                    profiler.disable()
                    
                    # Memory snapshot after
                    memory_after = process.memory_info().rss
                    
                    # Store profile data
                    s = io.StringIO()
                    ps = pstats.Stats(profiler, stream=s)
                    ps.sort_stats('cumulative')
                    ps.print_stats()
                    
                    self.profiles[name] = {
                        "timestamp": start_time,
                        "duration": end_time - start_time,
                        "memory_before": memory_before,
                        "memory_after": memory_after,
                        "memory_delta": memory_after - memory_before,
                        "profile_stats": s.getvalue()
                    }
                    
                    logger.debug(
                        "Function profiled",
                        function=name,
                        duration=end_time - start_time,
                        memory_delta=memory_after - memory_before
                    )
                    
                    return result
                    
                except Exception as e:
                    profiler.disable()
                    raise
            
            return wrapper
        return decorator
    
    def take_memory_snapshot(self, label: str = None):
        """Take a memory usage snapshot"""
        process = psutil.Process(os.getpid())
        memory_info = process.memory_info()
        
        snapshot = {
            "timestamp": time.time(),
            "label": label or f"snapshot_{len(self.memory_snapshots)}",
            "rss": memory_info.rss,
            "vms": memory_info.vms,
            "percent": process.memory_percent(),
            "cpu_percent": process.cpu_percent()
        }
        
        self.memory_snapshots.append(snapshot)
        
        logger.debug(
            "Memory snapshot taken",
            label=snapshot["label"],
            rss_mb=snapshot["rss"] / 1024 / 1024,
            cpu_percent=snapshot["cpu_percent"]
        )
        
        return snapshot
    
    def generate_performance_report(self) -> str:
        """Generate comprehensive performance report"""
        report = ["\
=== Performance Report ==="]
        
        # Function profiles
        if self.profiles:
            report.append("\
Function Profiles:")
            for func_name, profile in self.profiles.items():
                report.append(f"\
{func_name}:")
                report.append(f"  Duration: {profile['duration']:.4f}s")
                report.append(f"  Memory Delta: {profile['memory_delta'] / 1024 / 1024:.2f} MB")
        
        # Memory snapshots
        if self.memory_snapshots:
            report.append("\
Memory Snapshots:")
            for snapshot in self.memory_snapshots:
                report.append(
                    f"  {snapshot['label']}: {snapshot['rss'] / 1024 / 1024:.2f} MB "
                    f"({snapshot['percent']:.1f}%) CPU: {snapshot['cpu_percent']:.1f}%"
                )
        
        return "\
".join(report)
```

## Debug CLI Commands

### Enhanced CLI with Debug Commands

```python
# debug_cli.py
from spoon_ai.cli.base import BaseCLI
import json

class DebugCLI(BaseCLI):
    """CLI with enhanced debugging commands"""
    
    def __init__(self):
        super().__init__()
        self.debug_mode = False
        self.profiler = PerformanceProfiler()
    
    def do_debug_on(self, args):
        """Enable debug mode"""
        self.debug_mode = True
        setup_debug_logging()
        print("Debug mode enabled")
    
    def do_debug_off(self, args):
        """Disable debug mode"""
        self.debug_mode = False
        print("Debug mode disabled")
    
    def do_debug_agent(self, args):
        """Show agent debug information"""
        if hasattr(self.current_agent, 'get_debug_info'):
            debug_info = self.current_agent.get_debug_info()
            print(json.dumps(debug_info, indent=2))
        else:
            print("Current agent does not support debugging")
    
    def do_debug_tools(self, args):
        """Show tool debug information"""
        if hasattr(self.current_agent, 'tools'):
            for tool in self.current_agent.tools:
                if hasattr(tool, 'get_debug_info'):
                    debug_info = tool.get_debug_info()
                    print(f"\
{tool.name}:")
                    print(json.dumps(debug_info, indent=2))
        else:
            print("No tools available for debugging")
    
    def do_debug_memory(self, args):
        """Take memory snapshot"""
        snapshot = self.profiler.take_memory_snapshot(args or "manual")
        print(f"Memory snapshot: {snapshot['rss'] / 1024 / 1024:.2f} MB")
    
    def do_debug_performance(self, args):
        """Show performance report"""
        report = self.profiler.generate_performance_report()
        print(report)
    
    def do_debug_save(self, args):
        """Save debug information to file"""
        filename = args or f"debug_session_{int(time.time())}.json"
        
        debug_data = {
            "timestamp": time.time(),
            "agent_info": self.current_agent.get_debug_info() if hasattr(self.current_agent, 'get_debug_info') else None,
            "tool_info": [tool.get_debug_info() for tool in getattr(self.current_agent, 'tools', []) if hasattr(tool, 'get_debug_info')],
            "memory_snapshots": self.profiler.memory_snapshots,
            "performance_profiles": self.profiler.profiles
        }
        
        with open(filename, 'w') as f:
            json.dump(debug_data, f, indent=2)
        
        print(f"Debug information saved to {filename}")
```

## Best Practices

### Debug-Friendly Code
- Add comprehensive logging at key points
- Use structured logging with context
- Implement debug modes in components
- Provide introspection methods
- Store execution history for analysis

### Performance Monitoring
- Profile critical code paths
- Monitor memory usage patterns
- Track API call performance
- Measure end-to-end latency
- Set up alerts for performance degradation

### Error Investigation
- Capture full stack traces
- Log relevant context information
- Implement error categorization
- Store error patterns for analysis
- Provide clear error messages

### Production Debugging
- Use log levels appropriately
- Implement feature flags for debug features
- Provide remote debugging capabilities
- Monitor system health metrics
- Set up automated error reporting

## See Also

- [Common Issues](./common-issues.md)
- [Performance Optimization](./performance.md)
- [Logging Configuration](../getting-started/configuration.md)
- [System Monitoring](../api-reference/index)"}

---

FILE: docs/troubleshooting/performance.md

# Performance Optimization Guide

Comprehensive guide for optimizing SpoonOS performance across agents, tools, and infrastructure.

## Performance Monitoring

### System Metrics

```bash
# Check system performance
python main.py
> system-info

# Monitor resource usage
top -p $(pgrep -f "python main.py")
htop
```

### Built-in Performance Monitoring

```python
# performance_monitor.py
import psutil
import time
from dataclasses import dataclass
from typing import Dict, List

@dataclass
class PerformanceMetrics:
    timestamp: float
    cpu_percent: float
    memory_rss: int
    memory_percent: float
    disk_io_read: int
    disk_io_write: int
    network_sent: int
    network_recv: int

class PerformanceMonitor:
    def __init__(self):
        self.metrics_history: List[PerformanceMetrics] = []
        self.process = psutil.Process()
        self.initial_io = self.process.io_counters()
        self.initial_net = psutil.net_io_counters()

    def collect_metrics(self) -> PerformanceMetrics:
        """Collect current performance metrics"""
        current_io = self.process.io_counters()
        current_net = psutil.net_io_counters()

        metrics = PerformanceMetrics(
            timestamp=time.time(),
            cpu_percent=self.process.cpu_percent(),
            memory_rss=self.process.memory_info().rss,
            memory_percent=self.process.memory_percent(),
            disk_io_read=current_io.read_bytes - self.initial_io.read_bytes,
            disk_io_write=current_io.write_bytes - self.initial_io.write_bytes,
            network_sent=current_net.bytes_sent - self.initial_net.bytes_sent,
            network_recv=current_net.bytes_recv - self.initial_net.bytes_recv
        )

        self.metrics_history.append(metrics)
        return metrics

    def get_performance_summary(self, window_minutes: int = 5) -> Dict:
        """Get performance summary for the last N minutes"""
        cutoff_time = time.time() - (window_minutes * 60)
        recent_metrics = [m for m in self.metrics_history if m.timestamp > cutoff_time]

        if not recent_metrics:
            return {"error": "No metrics available"}

        return {
            "window_minutes": window_minutes,
            "sample_count": len(recent_metrics),
            "cpu": {
                "avg": sum(m.cpu_percent for m in recent_metrics) / len(recent_metrics),
                "max": max(m.cpu_percent for m in recent_metrics),
                "min": min(m.cpu_percent for m in recent_metrics)
            },
            "memory": {
                "current_mb": recent_metrics[-1].memory_rss / 1024 / 1024,
                "peak_mb": max(m.memory_rss for m in recent_metrics) / 1024 / 1024,
                "avg_percent": sum(m.memory_percent for m in recent_metrics) / len(recent_metrics)
            },
            "io": {
                "total_read_mb": recent_metrics[-1].disk_io_read / 1024 / 1024,
                "total_write_mb": recent_metrics[-1].disk_io_write / 1024 / 1024
            }
        }
```

## Agent Performance Optimization

### Efficient Agent Configuration

```json
{
  "agents": {
    "optimized_agent": {
      "class": "SpoonReactAI",
      "config": {
        "max_steps": 5,
        "temperature": 0.7,
        "max_tokens": 1000,
        "timeout": 30,
        "stream": true,
        "cache_enabled": true
      }
    }
  }
}
```

### Memory-Efficient Agent Implementation

```python
# optimized_agent.py
from spoon_ai.agents import SpoonReactAI
from typing import List, Dict
import gc

class OptimizedAgent(SpoonReactAI):
    """Memory-optimized agent implementation"""

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.max_history_length = kwargs.get('max_history_length', 50)
        self.cleanup_interval = kwargs.get('cleanup_interval', 10)
        self.request_count = 0

    async def run(self, message: str, **kwargs):
        """Run with memory management"""
        try:
            result = await super().run(message, **kwargs)

            # Periodic cleanup
            self.request_count += 1
            if self.request_count % self.cleanup_interval == 0:
                self._cleanup_memory()

            return result

        except Exception as e:
            # Force cleanup on error
            self._cleanup_memory()
            raise

    def _cleanup_memory(self):
        """Clean up memory usage"""
        # Limit conversation history
        if hasattr(self, 'conversation_history'):
            if len(self.conversation_history) > self.max_history_length:
                # Keep only recent messages
                self.conversation_history = self.conversation_history[-self.max_history_length:]

        # Force garbage collection
        gc.collect()

        logger.debug("Memory cleanup performed", agent=self.name)
```

### Conversation History Management

```python
# history_manager.py
from collections import deque
from typing import Dict, List, Optional
import json
import hashlib

class ConversationHistoryManager:
    """Efficient conversation history management"""

    def __init__(self, max_length: int = 100, compression_enabled: bool = True):
        self.max_length = max_length
        self.compression_enabled = compression_enabled
        self.messages = deque(maxlen=max_length)
        self.compressed_history = {}

    def add_message(self, message: Dict):
        """Add message with automatic compression"""
        # Add to recent messages
        self.messages.append(message)

        # Compress old messages if enabled
        if self.compression_enabled and len(self.messages) == self.max_length:
            self._compress_old_messages()

    def get_recent_messages(self, count: int = 10) -> List[Dict]:
        """Get recent messages efficiently"""
        return list(self.messages)[-count:]

    def get_context_summary(self) -> str:
        """Get compressed context summary"""
        if not self.compressed_history:
            return ""

        # Generate summary from compressed history
        summaries = []
        for period, summary in self.compressed_history.items():
            summaries.append(f"Period {period}: {summary}")

        return "\
".join(summaries)

    def _compress_old_messages(self):
        """Compress older messages into summaries"""
        # Take first half of messages for compression
        to_compress = list(self.messages)[:self.max_length // 2]

        # Generate summary (simplified)
        summary = self._generate_summary(to_compress)

        # Store compressed summary
        period_key = len(self.compressed_history)
        self.compressed_history[period_key] = summary

        # Remove compressed messages from deque
        for _ in range(len(to_compress)):
            self.messages.popleft()

    def _generate_summary(self, messages: List[Dict]) -> str:
        """Generate summary of message batch"""
        # Simple summarization (could be enhanced with LLM)
        user_messages = [m['content'] for m in messages if m.get('role') == 'user']
        assistant_messages = [m['content'] for m in messages if m.get('role') == 'assistant']

        return f"User asked {len(user_messages)} questions, assistant provided {len(assistant_messages)} responses"
```

## Tool Performance Optimization

### Async Tool Implementation

```python
# async_tool.py
import asyncio
import aiohttp
from spoon_ai.tools.base import BaseTool
from typing import List, Dict, Any

class AsyncHTTPTool(BaseTool):
    """High-performance async HTTP tool"""

    name = "async_http_tool"
    description = "Optimized HTTP requests with connection pooling"

    def __init__(self):
        self.session = None
        self.connector = None

    async def __aenter__(self):
        # Create optimized connector
        self.connector = aiohttp.TCPConnector(
            limit=100,  # Total connection pool size
            limit_per_host=30,  # Per-host connection limit
            ttl_dns_cache=300,  # DNS cache TTL
            use_dns_cache=True,
            keepalive_timeout=30,
            enable_cleanup_closed=True
        )

        # Create session with optimized settings
        timeout = aiohttp.ClientTimeout(total=30, connect=10)
        self.session = aiohttp.ClientSession(
            connector=self.connector,
            timeout=timeout,
            headers={'User-Agent': 'SpoonOS/1.0'}
        )

        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
        if self.connector:
            await self.connector.close()

    async def execute(self, urls: List[str], method: str = "GET", **kwargs) -> List[Dict]:
        """Execute multiple HTTP requests concurrently"""
        if not self.session:
            async with self:
                return await self._make_requests(urls, method, **kwargs)
        else:
            return await self._make_requests(urls, method, **kwargs)

    async def _make_requests(self, urls: List[str], method: str, **kwargs) -> List[Dict]:
        """Make concurrent HTTP requests"""
        semaphore = asyncio.Semaphore(10)  # Limit concurrent requests

        async def make_request(url: str) -> Dict:
            async with semaphore:
                try:
                    async with self.session.request(method, url, **kwargs) as response:
                        return {
                            "url": url,
                            "status": response.status,
                            "data": await response.text(),
                            "headers": dict(response.headers)
                        }
                except Exception as e:
                    return {
                        "url": url,
                        "error": str(e),
                        "status": 0
                    }

        # Execute all requests concurrently
        tasks = [make_request(url) for url in urls]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        return [r for r in results if not isinstance(r, Exception)]
```

### Caching Implementation

```python
# caching_tool.py
import asyncio
import hashlib
import json
import time
from typing import Any, Dict, Optional
from spoon_ai.tools.base import BaseTool

class CachedTool(BaseTool):
    """Tool with intelligent caching"""

    def __init__(self, cache_ttl: int = 3600, max_cache_size: int = 1000):
        self.cache_ttl = cache_ttl
        self.max_cache_size = max_cache_size
        self.cache: Dict[str, Dict] = {}
        self.cache_stats = {
            "hits": 0,
            "misses": 0,
            "evictions": 0
        }

    async def execute(self, **kwargs) -> Any:
        """Execute with caching"""
        # Generate cache key
        cache_key = self._generate_cache_key(**kwargs)

        # Check cache
        cached_result = self._get_from_cache(cache_key)
        if cached_result is not None:
            self.cache_stats["hits"] += 1
            logger.debug("Cache hit", tool=self.name, cache_key=cache_key[:8])
            return cached_result

        # Cache miss - execute tool
        self.cache_stats["misses"] += 1
        logger.debug("Cache miss", tool=self.name, cache_key=cache_key[:8])

        result = await self._execute_impl(**kwargs)

        # Store in cache
        self._set_cache(cache_key, result)

        return result

    def _generate_cache_key(self, **kwargs) -> str:
        """Generate deterministic cache key"""
        # Sort kwargs for consistent key generation
        key_data = json.dumps(kwargs, sort_keys=True, default=str)
        return hashlib.sha256(key_data.encode()).hexdigest()

    def _get_from_cache(self, key: str) -> Optional[Any]:
        """Get value from cache if not expired"""
        if key not in self.cache:
            return None

        entry = self.cache[key]
        if time.time() - entry["timestamp"] > self.cache_ttl:
            # Expired - remove from cache
            del self.cache[key]
            return None

        return entry["value"]

    def _set_cache(self, key: str, value: Any):
        """Set value in cache with eviction"""
        # Evict oldest entries if cache is full
        if len(self.cache) >= self.max_cache_size:
            self._evict_oldest()

        self.cache[key] = {
            "value": value,
            "timestamp": time.time()
        }

    def _evict_oldest(self):
        """Evict oldest cache entries"""
        # Sort by timestamp and remove oldest 10%
        sorted_items = sorted(
            self.cache.items(),
            key=lambda x: x[1]["timestamp"]
        )

        evict_count = max(1, len(sorted_items) // 10)
        for i in range(evict_count):
            key = sorted_items[i][0]
            del self.cache[key]
            self.cache_stats["evictions"] += 1

    def get_cache_stats(self) -> Dict:
        """Get cache performance statistics"""
        total_requests = self.cache_stats["hits"] + self.cache_stats["misses"]
        hit_rate = self.cache_stats["hits"] / total_requests if total_requests > 0 else 0

        return {
            "cache_size": len(self.cache),
            "max_cache_size": self.max_cache_size,
            "hit_rate": hit_rate,
            "total_requests": total_requests,
            **self.cache_stats
        }

    async def _execute_impl(self, **kwargs) -> Any:
        """Override this method in subclasses"""
        raise NotImplementedError
```

## LLM Performance Optimization

### Request Batching

```python
# batch_llm_provider.py
import asyncio
from typing import List, Dict, Any
from spoon_ai.llm.base import BaseLLMProvider

class BatchedLLMProvider(BaseLLMProvider):
    """LLM provider with request batching"""

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.batch_size = kwargs.get('batch_size', 5)
        self.batch_timeout = kwargs.get('batch_timeout', 1.0)
        self.pending_requests = []
        self.batch_lock = asyncio.Lock()

    async def generate(self, messages: List[Dict], **kwargs) -> Dict:
        """Generate with batching optimization"""
        # For single requests, use batching
        if len(messages) == 1:
            return await self._batched_generate(messages[0], **kwargs)
        else:
            # For multi-message requests, process directly
            return await super().generate(messages, **kwargs)

    async def _batched_generate(self, message: Dict, **kwargs) -> Dict:
        """Generate with request batching"""
        # Create request future
        request_future = asyncio.Future()
        request_data = {
            "message": message,
            "kwargs": kwargs,
            "future": request_future
        }

        async with self.batch_lock:
            self.pending_requests.append(request_data)

            # If batch is full or this is the first request, process batch
            if len(self.pending_requests) >= self.batch_size:
                await self._process_batch()
            elif len(self.pending_requests) == 1:
                # Start batch timer for first request
                asyncio.create_task(self._batch_timer())

        # Wait for result
        return await request_future

    async def _batch_timer(self):
        """Timer to process batch after timeout"""
        await asyncio.sleep(self.batch_timeout)

        async with self.batch_lock:
            if self.pending_requests:
                await self._process_batch()

    async def _process_batch(self):
        """Process pending requests as a batch"""
        if not self.pending_requests:
            return

        batch = self.pending_requests.copy()
        self.pending_requests.clear()

        try:
            # Process all requests concurrently
            tasks = [
                self._process_single_request(req["message"], req["kwargs"])
                for req in batch
            ]

            results = await asyncio.gather(*tasks, return_exceptions=True)

            # Set results for each future
            for req, result in zip(batch, results):
                if isinstance(result, Exception):
                    req["future"].set_exception(result)
                else:
                    req["future"].set_result(result)

        except Exception as e:
            # Set exception for all futures
            for req in batch:
                if not req["future"].done():
                    req["future"].set_exception(e)

    async def _process_single_request(self, message: Dict, kwargs: Dict) -> Dict:
        """Process a single request"""
        return await super().generate([message], **kwargs)
```

### Response Streaming

```python
# streaming_provider.py
import asyncio
from typing import AsyncGenerator, Dict, Any
from spoon_ai.llm.base import BaseLLMProvider

class StreamingLLMProvider(BaseLLMProvider):
    """LLM provider with streaming support"""

    async def generate_stream(self, messages: List[Dict], **kwargs) -> AsyncGenerator[str, None]:
        """Generate streaming response"""
        # Implementation depends on provider API
        async for chunk in self._stream_implementation(messages, **kwargs):
            yield chunk

    async def _stream_implementation(self, messages: List[Dict], **kwargs) -> AsyncGenerator[str, None]:
        """Provider-specific streaming implementation"""
        # Example implementation
        response = await self._make_streaming_request(messages, **kwargs)

        async for line in response:
            if line.startswith('data: '):
                chunk_data = line[6:]  # Remove 'data: ' prefix
                if chunk_data.strip() == '[DONE]':
                    break

                try:
                    chunk_json = json.loads(chunk_data)
                    if 'choices' in chunk_json and chunk_json['choices']:
                        delta = chunk_json['choices'][0].get('delta', {})
                        if 'content' in delta:
                            yield delta['content']
                except json.JSONDecodeError:
                    continue
```

## Database and Storage Optimization

### Connection Pooling

```python
# db_pool.py
import asyncpg
import asyncio
from typing import Optional, Dict, Any

class DatabasePool:
    """Optimized database connection pool"""

    def __init__(self, connection_string: str, **pool_kwargs):
        self.connection_string = connection_string
        self.pool_kwargs = {
            'min_size': 5,
            'max_size': 20,
            'command_timeout': 60,
            'server_settings': {
                'jit': 'off',  # Disable JIT for faster startup
                'application_name': 'spoon_ai'
            },
            **pool_kwargs
        }
        self.pool: Optional[asyncpg.Pool] = None

    async def initialize(self):
        """Initialize connection pool"""
        if self.pool is None:
            self.pool = await asyncpg.create_pool(
                self.connection_string,
                **self.pool_kwargs
            )

    async def execute_query(self, query: str, *args) -> List[Dict[str, Any]]:
        """Execute query with connection pooling"""
        if self.pool is None:
            await self.initialize()

        async with self.pool.acquire() as connection:
            rows = await connection.fetch(query, *args)
            return [dict(row) for row in rows]

    async def execute_transaction(self, queries: List[tuple]) -> List[Any]:
        """Execute multiple queries in a transaction"""
        if self.pool is None:
            await self.initialize()

        async with self.pool.acquire() as connection:
            async with connection.transaction():
                results = []
                for query, args in queries:
                    result = await connection.fetch(query, *args)
                    results.append([dict(row) for row in result])
                return results

    async def close(self):
        """Close connection pool"""
        if self.pool:
            await self.pool.close()
            self.pool = None
```

### Efficient Data Serialization

```python
# serialization.py
import orjson  # Faster JSON library
import pickle
import gzip
from typing import Any, Union

class OptimizedSerializer:
    """High-performance data serialization"""

    @staticmethod
    def serialize_json(data: Any) -> bytes:
        """Fast JSON serialization"""
        return orjson.dumps(data)

    @staticmethod
    def deserialize_json(data: bytes) -> Any:
        """Fast JSON deserialization"""
        return orjson.loads(data)

    @staticmethod
    def serialize_compressed(data: Any) -> bytes:
        """Compressed pickle serialization"""
        pickled = pickle.dumps(data, protocol=pickle.HIGHEST_PROTOCOL)
        return gzip.compress(pickled)

    @staticmethod
    def deserialize_compressed(data: bytes) -> Any:
        """Compressed pickle deserialization"""
        decompressed = gzip.decompress(data)
        return pickle.loads(decompressed)

    @staticmethod
    def choose_serialization(data: Any) -> tuple[bytes, str]:
        """Choose optimal serialization method"""
        # Try JSON first (faster, more compatible)
        try:
            json_data = OptimizedSerializer.serialize_json(data)
            compressed_data = OptimizedSerializer.serialize_compressed(data)

            # Use JSON if it's not much larger
            if len(json_data) <= len(compressed_data) * 1.2:
                return json_data, 'json'
            else:
                return compressed_data, 'compressed'

        except (TypeError, ValueError):
            # Fall back to compressed pickle
            return OptimizedSerializer.serialize_compressed(data), 'compressed'
```

## Infrastructure Optimization

### Process Management

```python
# process_manager.py
import asyncio
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor
from typing import Any, Callable, List

class OptimizedProcessManager:
    """Optimized process management for CPU-intensive tasks"""

    def __init__(self, max_workers: int = None):
        self.max_workers = max_workers or mp.cpu_count()
        self.executor = None

    async def __aenter__(self):
        self.executor = ProcessPoolExecutor(max_workers=self.max_workers)
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.executor:
            self.executor.shutdown(wait=True)

    async def run_cpu_intensive(self, func: Callable, *args, **kwargs) -> Any:
        """Run CPU-intensive function in separate process"""
        if self.executor is None:
            async with self:
                return await self._execute(func, *args, **kwargs)
        else:
            return await self._execute(func, *args, **kwargs)

    async def _execute(self, func: Callable, *args, **kwargs) -> Any:
        """Execute function in process pool"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            self.executor,
            lambda: func(*args, **kwargs)
        )

    async def map_parallel(self, func: Callable, items: List[Any]) -> List[Any]:
        """Map function over items in parallel"""
        if self.executor is None:
            async with self:
                return await self._map_execute(func, items)
        else:
            return await self._map_execute(func, items)

    async def _map_execute(self, func: Callable, items: List[Any]) -> List[Any]:
        """Execute map in process pool"""
        loop = asyncio.get_event_loop()
        tasks = [
            loop.run_in_executor(self.executor, func, item)
            for item in items
        ]
        return await asyncio.gather(*tasks)
```

### Memory Management

```python
# memory_manager.py
import gc
import psutil
import os
from typing import Dict, Any

class MemoryManager:
    """Advanced memory management utilities"""

    def __init__(self, warning_threshold: float = 0.8, critical_threshold: float = 0.9):
        self.warning_threshold = warning_threshold
        self.critical_threshold = critical_threshold
        self.process = psutil.Process(os.getpid())

    def get_memory_info(self) -> Dict[str, Any]:
        """Get detailed memory information"""
        memory_info = self.process.memory_info()
        memory_percent = self.process.memory_percent()

        return {
            "rss_mb": memory_info.rss / 1024 / 1024,
            "vms_mb": memory_info.vms / 1024 / 1024,
            "percent": memory_percent,
            "available_mb": psutil.virtual_memory().available / 1024 / 1024,
            "warning_level": self._get_warning_level(memory_percent)
        }

    def _get_warning_level(self, memory_percent: float) -> str:
        """Get memory warning level"""
        if memory_percent >= self.critical_threshold * 100:
            return "critical"
        elif memory_percent >= self.warning_threshold * 100:
            return "warning"
        else:
            return "normal"

    def cleanup_memory(self, force: bool = False) -> Dict[str, Any]:
        """Perform memory cleanup"""
        before_info = self.get_memory_info()

        # Force garbage collection
        collected = gc.collect()

        # Additional cleanup for critical memory usage
        if force or before_info["warning_level"] == "critical":
            # Clear caches
            self._clear_internal_caches()

            # Force another GC cycle
            collected += gc.collect()

        after_info = self.get_memory_info()

        return {
            "before_mb": before_info["rss_mb"],
            "after_mb": after_info["rss_mb"],
            "freed_mb": before_info["rss_mb"] - after_info["rss_mb"],
            "objects_collected": collected,
            "warning_level": after_info["warning_level"]
        }

    def _clear_internal_caches(self):
        """Clear internal caches"""
        # Clear function caches
        import functools
        for obj in gc.get_objects():
            if isinstance(obj, functools._lru_cache_wrapper):
                obj.cache_clear()

        # Clear regex cache
        import re
        re.purge()

    def monitor_memory(self) -> bool:
        """Monitor memory and return True if action needed"""
        info = self.get_memory_info()

        if info["warning_level"] == "critical":
            logger.warning(
                "Critical memory usage detected",
                memory_mb=info["rss_mb"],
                percent=info["percent"]
            )
            return True
        elif info["warning_level"] == "warning":
            logger.info(
                "High memory usage detected",
                memory_mb=info["rss_mb"],
                percent=info["percent"]
            )

        return False
```

## Configuration Optimization

### Production Configuration

```json
{
  "performance": {
    "agents": {
      "max_concurrent": 5,
      "memory_limit_mb": 512,
      "cleanup_interval": 10,
      "max_history_length": 50
    },
    "llm": {
      "connection_pool_size": 10,
      "request_timeout": 30,
      "retry_attempts": 3,
      "batch_size": 5,
      "cache_enabled": true,
      "cache_ttl": 3600
    },
    "tools": {
      "concurrent_limit": 10,
      "cache_enabled": true,
      "cache_size": 1000,
      "timeout": 30
    },
    "database": {
      "pool_min_size": 5,
      "pool_max_size": 20,
      "command_timeout": 60,
      "connection_timeout": 10
    }
  }
}
```

### Environment Variables

```bash
# Performance tuning
export PYTHONOPTIMIZE=1
export PYTHONDONTWRITEBYTECODE=1
export PYTHONUNBUFFERED=1

# Memory management
export MALLOC_TRIM_THRESHOLD_=100000
export MALLOC_MMAP_THRESHOLD_=131072

# Async settings
export PYTHONASYNCIODEBUG=0
export UV_THREADPOOL_SIZE=16

# Logging optimization
export LOG_LEVEL=INFO
export LOG_FORMAT=json
```

## Monitoring and Alerting

### Performance Alerts

```python
# alerts.py
import asyncio
from typing import Dict, Callable, Any

class PerformanceAlerting:
    """Performance monitoring and alerting"""

    def __init__(self):
        self.thresholds = {
            "memory_percent": 80,
            "cpu_percent": 80,
            "response_time": 5.0,
            "error_rate": 0.1
        }
        self.alert_handlers = []

    def add_alert_handler(self, handler: Callable[[str, Dict[str, Any]], None]):
        """Add alert handler function"""
        self.alert_handlers.append(handler)

    async def check_performance(self, metrics: Dict[str, Any]):
        """Check performance metrics against thresholds"""
        alerts = []

        # Check memory usage
        if metrics.get("memory_percent", 0) > self.thresholds["memory_percent"]:
            alerts.append({
                "type": "memory_high",
                "value": metrics["memory_percent"],
                "threshold": self.thresholds["memory_percent"],
                "message": f"Memory usage {metrics['memory_percent']:.1f}% exceeds threshold"
            })

        # Check CPU usage
        if metrics.get("cpu_percent", 0) > self.thresholds["cpu_percent"]:
            alerts.append({
                "type": "cpu_high",
                "value": metrics["cpu_percent"],
                "threshold": self.thresholds["cpu_percent"],
                "message": f"CPU usage {metrics['cpu_percent']:.1f}% exceeds threshold"
            })

        # Check response time
        if metrics.get("avg_response_time", 0) > self.thresholds["response_time"]:
            alerts.append({
                "type": "response_time_high",
                "value": metrics["avg_response_time"],
                "threshold": self.thresholds["response_time"],
                "message": f"Response time {metrics['avg_response_time']:.2f}s exceeds threshold"
            })

        # Send alerts
        for alert in alerts:
            await self._send_alert(alert)

    async def _send_alert(self, alert: Dict[str, Any]):
        """Send alert to all handlers"""
        for handler in self.alert_handlers:
            try:
                await handler(alert["type"], alert)
            except Exception as e:
                logger.error(f"Alert handler failed: {e}")
```

## Best Practices

### Code Optimization
- Use async/await for I/O operations
- Implement connection pooling
- Cache expensive computations
- Batch similar operations
- Use efficient data structures

### Memory Management
- Limit conversation history
- Implement periodic cleanup
- Monitor memory usage
- Use generators for large datasets
- Clear caches regularly

### Network Optimization
- Use connection pooling
- Implement request batching
- Enable compression
- Set appropriate timeouts
- Handle rate limiting

### Database Optimization
- Use connection pooling
- Implement query caching
- Optimize query patterns
- Use transactions efficiently
- Monitor query performance

## See Also

- [Debugging Guide](./debugging.md)
- [Common Issues](./common-issues.md)
- [System Requirements](../getting-started/installation.md)"}

---

FILE: README.md

# SpoonOS Developer Documentation

Welcome to the **SpoonOS Developer Documentation** - the comprehensive guide for building, deploying, and scaling Web3 AI agents with the SpoonOS Agentic Operating System.

This documentation site provides everything you need to get started with SpoonOS, the **Agentic OS for a Sentient Economy**.

If you plan to build or refresh the Python-generated API docs that power this cookbook, use `uv` (Python 3.12+) for a fast, reproducible setup:

```bash
cd cookbook
uv venv .venv
./.venv/Scripts/activate   # Windows
# source .venv/bin/activate # macOS/Linux
uv pip install spoon-ai-sdk
```

## ðŸš€ What is SpoonOS?

SpoonOS is an **Agentic Operating System that enables AI agents to perceive, reason, plan, and execute**. It provides a robust framework for creating, deploying, and managing Web3 AI agents. SpoonOS fosters interoperability, data scalability, and privacyâ€”empowering AI agents to engage in collaborative learning while ensuring secure and efficient data processing.

### Key Features

- ðŸ§  **Intelligent Agent Framework** - Advanced reasoning and action capabilities
- ðŸŒ **Web3-Native Architecture** - Built-in blockchain and DeFi integration
- ðŸ”§ **Modular Tool System** - Extensible architecture with MCP protocol support
- ðŸ¤– **Multi-Model AI Support** - Compatible with OpenAI, Anthropic, DeepSeek, and more
- ðŸ“Š **Graph-Based Workflows** - Complex workflow orchestration and management
- ðŸ’» **Interactive CLI** - Powerful development and deployment interface
- ðŸ”’ **Privacy-First Design** - Secure data processing and collaborative learning
- âš¡ **High Performance** - Optimized for scalability and efficiency

## ðŸ“š Documentation Structure

- **Getting Started** - Installation, configuration, and quick start
- **Core Concepts** - Agents, tools, and system architecture
- **Guides** - Step-by-step tutorials and best practices
- **API Reference** - Complete API documentation
- **Examples** - Real-world use cases and sample code

## ðŸ› ï¸ Development

### Installation

For Python-based API doc generation (recommended when updating the docs), set up `uv` first:

```bash
uv venv .venv
./.venv/Scripts/activate   # Windows
# source .venv/bin/activate # macOS/Linux
uv pip install pydoc-markdown spoon-ai-sdk
```

Then install the site dependencies:

```bash
npm install
```

### Local Development

```bash
npm start
```

This command starts a local development server and opens up a browser window. Most changes are reflected live without having to restart the server.

### Build

```bash
npm run build
```

This command generates static content into the `build` directory and can be served using any static contents hosting service.

### Deployment

Using SSH:

```bash
USE_SSH=true npm run deploy
```

Not using SSH:

```bash
GIT_USER=<Your GitHub username> npm run deploy
```

If you are using GitHub pages for hosting, this command is a convenient way to build the website and push to the `gh-pages` branch.

## ðŸ¤ Contributing

We welcome contributions to improve the documentation! Please:

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

## ðŸ“„ License

This documentation is part of the SpoonOS project and follows the same license terms.

## ðŸ”— Links

- **Main Repository**: [XSpoonAi/spoon-core](https://github.com/XSpoonAi/spoon-core)
- **Official Website**: [SpoonOS Landing](https://spoonai.io)
- **Documentation**: [Developer Documentation](https://xspoonai.github.io/spoon-doc/)
- **Community**: [GitHub Discussions](https://github.com/XSpoonAi/spoon-core/discussions)

---

**Build, deploy and scale Web3 AI agents in an evolving paradigm.**

Built with â¤ï¸ by the SpoonOS Team

---

FILE: src/pages/markdown-page.md

---
title: Markdown page example
---

# Markdown page example

You don't need React to write simple standalone pages.

---

FILE: Toolkit/audio/elevenlabs.md

---
id: elevenlabs
title: ElevenLabs Audio Tools
sidebar_label: ElevenLabs
---

# ElevenLabs Audio Tools

High-quality AI audio tools powered by [ElevenLabs](https://elevenlabs.io). Generate lifelike speech, transcribe audio, design custom voices, clone voices from samples, and dub content into 70+ languages.

## Installation

```bash
pip install spoon-toolkits elevenlabs
```

## Environment Variables

| Variable | Required | Description |
|----------|----------|-------------|
| `ELEVENLABS_API_KEY` | Yes | Your ElevenLabs API key. Get it at [elevenlabs.io/app/settings/api-keys](https://elevenlabs.io/app/settings/api-keys) |

```bash
export ELEVENLABS_API_KEY="your_api_key_here"
```

## Available Tools

### Text-to-Speech

#### `ElevenLabsTextToSpeechTool`

Convert text to high-quality speech audio. Returns base64-encoded audio.

```python
from spoon_toolkits.audio import ElevenLabsTextToSpeechTool
import base64

tts = ElevenLabsTextToSpeechTool()
result = await tts.execute(
    text="Hello, this is a test of ElevenLabs text to speech.",
    voice_id="JBFqnCBsd6RMkjVDRZzb",  # George voice
    model_id="eleven_multilingual_v2",
    output_format="mp3_44100_128"
)

out = result.output if hasattr(result, 'output') else result
print(f"Audio size: {out['audio_size_bytes']} bytes")

# Save to file
audio = base64.b64decode(out['audio_base64'])
with open("output.mp3", "wb") as f:
    f.write(audio)
```

**Parameters:**
- `text` (required): Text to convert to speech
- `voice_id`: Voice ID (default: "JBFqnCBsd6RMkjVDRZzb" - George)
- `model_id`: Model ID (options: `eleven_multilingual_v2`, `eleven_turbo_v2_5`, `eleven_flash_v2_5`)
- `output_format`: Audio format (`mp3_44100_128`, `mp3_22050_32`, `pcm_16000`, `pcm_44100`)
- `language_code`: ISO 639-1 language code to enforce pronunciation

#### `ElevenLabsTextToSpeechStreamTool`

Stream text-to-speech with character-level timestamps (useful for lip-sync/subtitles).

```python
from spoon_toolkits.audio import ElevenLabsTextToSpeechStreamTool

stream_tts = ElevenLabsTextToSpeechStreamTool()
result = await stream_tts.execute(
    text="Streaming text to speech with timestamps.",
    voice_id="JBFqnCBsd6RMkjVDRZzb"
)

out = result.output if hasattr(result, 'output') else result
print(f"Total alignment points: {out['total_alignment_points']}")
# Each alignment point: {'char': 'S', 'start': 0.0, 'end': 0.128}
print(f"Sample: {out['alignment'][:3]}")
```

### Speech-to-Text

#### `ElevenLabsSpeechToTextTool`

Transcribe audio or video files to text. Supports 99 languages.

```python
from spoon_toolkits.audio import ElevenLabsSpeechToTextTool

stt = ElevenLabsSpeechToTextTool()
result = await stt.execute(
    file_path="recording.mp3",
    model_id="scribe_v1",
    language="en"  # Optional, auto-detected if not specified
)

out = result.output if hasattr(result, 'output') else result
print(f"Transcription: {out['text']}")
```

**Parameters:**
- `file_path` (required): Path to audio/video file
- `model_id`: STT model (options: `scribe_v1`, `scribe_v1_experimental`, `scribe_v2`)
- `language`: ISO 639-1 language code (auto-detected if not provided)

### Voice Design

#### `ElevenLabsVoiceDesignTool`

Generate custom voices from text descriptions.

```python
from spoon_toolkits.audio import ElevenLabsVoiceDesignTool

designer = ElevenLabsVoiceDesignTool()
result = await designer.execute(
    voice_description="A warm elderly British man with a gentle, storytelling tone",
    auto_generate_text=True
)

out = result.output if hasattr(result, 'output') else result
# Returns voice previews with generated_voice_id
for preview in out.get('previews', []):
    print(f"Preview ID: {preview['generated_voice_id']}")
```

#### `ElevenLabsCreateVoiceFromPreviewTool`

Save a voice preview as a permanent voice.

```python
from spoon_toolkits.audio import ElevenLabsCreateVoiceFromPreviewTool

creator = ElevenLabsCreateVoiceFromPreviewTool()
result = await creator.execute(
    generated_voice_id="preview_id_from_design",
    voice_name="Grandfather Voice",
    voice_description="Warm British storyteller"
)

out = result.output if hasattr(result, 'output') else result
print(f"Created voice ID: {out['voice_id']}")
```

### Voice Cloning

#### `ElevenLabsInstantVoiceCloneTool`

Clone a voice from audio samples (1-3 files recommended).

```python
from spoon_toolkits.audio import ElevenLabsInstantVoiceCloneTool

cloner = ElevenLabsInstantVoiceCloneTool()
result = await cloner.execute(
    name="My Cloned Voice",
    files=["sample1.mp3", "sample2.mp3"],
    description="Cloned from my recordings",
    remove_background_noise=True
)

out = result.output if hasattr(result, 'output') else result
print(f"Cloned voice ID: {out['voice_id']}")
```

**Parameters:**
- `name` (required): Name for the cloned voice
- `files` (required): List of audio file paths (1-3 recommended)
- `description`: Voice description
- `remove_background_noise`: Clean up samples (default: False)

### Dubbing

#### `ElevenLabsDubbingCreateTool`

Create a dubbing project to localize audio/video content.

```python
from spoon_toolkits.audio import ElevenLabsDubbingCreateTool

dubber = ElevenLabsDubbingCreateTool()
result = await dubber.execute(
    file_path="video.mp4",
    target_lang="es",  # Spanish
    source_lang="en",  # Optional
    name="Spanish Dub"
)

out = result.output if hasattr(result, 'output') else result
print(f"Dubbing ID: {out['dubbing_id']}")
```

#### `ElevenLabsDubbingStatusTool`

Check dubbing project status.

```python
from spoon_toolkits.audio import ElevenLabsDubbingStatusTool

status_tool = ElevenLabsDubbingStatusTool()
result = await status_tool.execute(dubbing_id="your_dubbing_id")

out = result.output if hasattr(result, 'output') else result
print(f"Status: {out['status']}")
```

#### `ElevenLabsDubbingAudioTool`

Download dubbed audio from a completed project.

```python
from spoon_toolkits.audio import ElevenLabsDubbingAudioTool
import base64

downloader = ElevenLabsDubbingAudioTool()
result = await downloader.execute(
    dubbing_id="your_dubbing_id",
    language_code="es"
)

out = result.output if hasattr(result, 'output') else result
audio = base64.b64decode(out['audio_base64'])
with open("dubbed_spanish.mp3", "wb") as f:
    f.write(audio)
```

## Complete Example

Here's a complete workflow using multiple ElevenLabs tools:

```python
import asyncio
import base64
import os
from spoon_toolkits.audio import (
    ElevenLabsTextToSpeechTool,
    ElevenLabsSpeechToTextTool,
)

async def elevenlabs_demo():
    # Check API key
    if not os.getenv("ELEVENLABS_API_KEY"):
        print("Set ELEVENLABS_API_KEY environment variable first!")
        return

    # 1. Generate speech
    tts = ElevenLabsTextToSpeechTool()
    tts_result = await tts.execute(
        text="Welcome to the ElevenLabs toolkit demonstration."
    )
    out = tts_result.output if hasattr(tts_result, 'output') else tts_result
    
    if out.get('success'):
        audio = base64.b64decode(out['audio_base64'])
        with open("demo_output.mp3", "wb") as f:
            f.write(audio)
        print(f"Generated audio: {out['audio_size_bytes']} bytes")
        
        # 2. Transcribe it back
        stt = ElevenLabsSpeechToTextTool()
        stt_result = await stt.execute(file_path="demo_output.mp3", model_id="scribe_v1")
        stt_out = stt_result.output if hasattr(stt_result, 'output') else stt_result
        print(f"Transcription: {stt_out['text']}")

if __name__ == "__main__":
    asyncio.run(elevenlabs_demo())
```

## Supported Languages

ElevenLabs supports 70+ languages for text-to-speech and 99 languages for speech-to-text. Common language codes:

| Code | Language |
|------|----------|
| `en` | English |
| `es` | Spanish |
| `fr` | French |
| `de` | German |
| `it` | Italian |
| `pt` | Portuguese |
| `zh` | Chinese |
| `ja` | Japanese |
| `ko` | Korean |
| `ar` | Arabic |
| `hi` | Hindi |
| `ru` | Russian |

## Voice IDs

Some popular pre-built voice IDs:

| Voice ID | Name | Description |
|----------|------|-------------|
| `JBFqnCBsd6RMkjVDRZzb` | George | Warm British male |
| `21m00Tcm4TlvDq8ikWAM` | Rachel | American female |
| `AZnzlk1XvdvUeBnXmlld` | Domi | Young American female |
| `EXAVITQu4vr4xnSDxMaL` | Bella | Soft American female |
| `ErXwobaYiN019PkySvjV` | Antoni | Well-rounded American male |

Browse more voices at [elevenlabs.io/voice-library](https://elevenlabs.io/voice-library).

## API Reference

- [ElevenLabs API Docs](https://elevenlabs.io/docs/api-reference)
- [ElevenLabs Python SDK](https://github.com/elevenlabs/elevenlabs-python)
- [Pricing](https://elevenlabs.io/pricing)

---

FILE: Toolkit/crypto/data-tools.md

`spoon_toolkits.crypto.crypto_data_tools` is SpoonAIâ€™s Bitquery-centric toolbox for price discovery, liquidity intelligence, lending-rate aggregation, and wallet forensics. Every class in this package plugs directly into the Spoon tool runtime, so agents can reuse the same `BaseTool` lifecycle (validation, structured outputs, telemetry) without additional glue code.


## Environment & Dependencies

```bash
export BITQUERY_API_KEY=your_rest_key              # reserved for future REST helpers
export BITQUERY_CLIENT_ID=your_oauth_client_id     # required by Bitquery OAuth
export BITQUERY_CLIENT_SECRET=your_oauth_secret
export RPC_URL=https://eth.llamarpc.com
```

- `Get*` price tools pull the RPC URL from the environment automatically; alert helpers default to a bundled Alchemy mainnet URL unless you pass a custom endpoint when instantiating them.
- `PredictPrice` requires `pandas`, `scikit-learn`, and `numpy`. Install toolkit extras via `pip install -r requirements.txt` at the project root or install modules individually.
- `LendingRateMonitorTool` performs concurrent HTTP requests using `aiohttp`; event loops must be active (use `nest_asyncio` in notebooks if necessary).
- `SuddenPriceIncreaseTool` currently relies exclusively on CoinGeckoâ€™s REST feed; the Bitquery enrichment hook exists in code but is not wired up yet, so providing `BITQUERY_API_KEY` has no effect today.

## Tool Catalog 

### Spot & Historical Pricing
- `GetTokenPriceTool` resolves supported ERC-20 pairs (ETH, USDC, USDT, DAI, WBTC by default) to the canonical Uniswap v3 pool, fetches slot0, and converts ticks into human prices.
- `Get24hStatsTool` uses the same provider stack for downstream analytics, while `GetKlineDataTool` is currently a placeholder that returns an empty list until a Graph/event-log integration ships.

### Alerting & Monitoring
- `PriceThresholdAlertTool` compares live prices with a configurable Â±% drift versus prior day.
- `LpRangeCheckTool` reads Uniswap positions, current ticks, and warns when LPs approach range boundaries (`buffer_ticks`).
- `SuddenPriceIncreaseTool` filters CoinGeckoâ€™s REST feed for large-cap, high-volume tokens with rapid appreciation; Bitquery enrichment is planned but not enabled.
- `CryptoMarketMonitor` POSTs well-formed payloads to the monitoring daemon bundled at `http://localhost:8888/monitoring/tasks` (`blockchain_monitor.py`). Change the URL in that module if your scheduler lives elsewhere.

### Wallet Intelligence
- `WalletAnalysis`, `TokenHolders`, and `TradingHistory` share the Bitquery GraphQL templates embedded in their modules so you always know the dataset (`combined` vs `archive`) and filters before execution.
- Pagination and filters live directly inside each template string (e.g., `TokenHolders` enforces `Amount >= 10,000,000`). Edit the template if you need a different threshold or window; no exposed class constants exist today. These helpers return the Bitquery JSON fragments directly rather than wrapping results in `ToolResult`.

### DeFi Rates & Liquidity
- `LendingRateMonitorTool` merges DeFiLlama pools with first-party Aave subgraphs today (Compound/Morpho data arrives via DeFiLlamaâ€™s feed only), derives utilization, and wraps the result in `ToolResult`. Every response also includes an `arbitrage_opportunities` array summarizing large APY spreads.
- `UniswapLiquidity` emits the latest Mint/Burn events so you can approximate real-time liquidity deltas without running a listener yourself.
- `PredictPrice` is intentionally heavyweight (builds a RandomForest and scaler); cache the fitted estimator in your application if you call it frequently.

## Usage Patterns

### Synchronous price lookup
```python
from spoon_toolkits.crypto.crypto_data_tools import GetTokenPriceTool

tool = GetTokenPriceTool()
result = await tool.execute(symbol="ETH-USDC")

print(result.output["price"])         # structured response
```

### Long-running alert inside an agent
```python
import asyncio
from spoon_toolkits.crypto.crypto_data_tools import PriceThresholdAlertTool

alerts = PriceThresholdAlertTool()

async def monitor():
    while True:
        tool_result = await alerts.execute(symbol="ETH-USDC", threshold_percent=7)
        if tool_result.output.get("exceeded"):
            await send_notification(tool_result.output["message"])
        await asyncio.sleep(60)

asyncio.run(monitor())
```

Tips:
- Synchronous price tools return dictionaries nested inside `ToolResult.output`, whereas the Bitquery GraphQL helpers (`WalletAnalysis`, `TokenHolders`, `TradingHistory`, `UniswapLiquidity`) currently return the raw template output.
- Async utilities (`PriceThresholdAlertTool`, `LendingRateMonitorTool`) already shield you from rate spikes with short sleeps; avoid spawning excessive parallel loops unless you also raise Bitquery limits.

## Operational Notes

- Centralize credential management: add the Bitquery OAuth keys to your `.env` and load them before instantiating tools to prevent runtime `ValueError`s from `DexBaseTool.oAuth()`.
- Because Uniswap calls hit your RPC, failures there do **not** imply Bitquery downtimeâ€”inspect the `error` string inside `ToolResult.output` or the returned dictionary to isolate the failure domain.
- The monitoring helper in `blockchain_monitor.py` assumes a scheduler reachable at `http://localhost:8888/monitoring/tasks`. Override `api_url` in the module before deploying if your infra differs.
- `PredictPrice` fetches up to ~1000 rows per query. If Bitquery throttles you, lower the template limit or use a paid API plan.

---

FILE: Toolkit/crypto/evm.md

`spoon_toolkits.crypto.evm` gathers the core on-chain primitives agents needâ€”native transfers, ERC-20 operations, swaps, quotes, bridges, and balance lookupsâ€”without shelling out to the JS plugin. Each class inherits `BaseTool`, so input validation, diagnostics, and async receipt waiting all behave consistently across Spoon agents.

## Environment & Dependencies

```bash
export EVM_PROVIDER_URL=https://mainnet.infura.io/v3/...
export EVM_PRIVATE_KEY=0xyourSignerKey
# Optional global fallback used by other crypto toolkits
export RPC_URL=https://eth.llamarpc.com
```

- All tools accept an `rpc_url` override, but only transaction senders (`EvmTransferTool`, `EvmErc20TransferTool`, `EvmSwapTool`, `EvmBridgeTool`) take `private_key`/signer inputs; read-only helpers rely solely on RPC access.
- `web3.py` is lazily imported inside each execute path, so include it (and `requests`) in your runtime environment.
- Aggregator-backed tools call public REST APIs (Bebop or LiFi). If you operate behind a proxy, ensure outbound HTTPS is allowed.

## Quick Start â€“ Native Transfer

```python
from spoon_toolkits.crypto.evm import EvmTransferTool

transfer = EvmTransferTool()
tx = await transfer.execute(
    to_address="0xrecipient...",
    amount_ether="0.05",
    data="0x",                      # optional calldata
)

print(tx.output["hash"])
```

Receipts are awaited with reasonable timeouts; if the transaction reverts, `ToolResult.error` includes the tx hash so you can inspect it with an explorer.

## Toolkit Overview

| Tool | Module | Purpose |
| --- | --- | --- |
| `EvmTransferTool` | `transfer.py` | Send native tokens with optional data payloads, auto gas estimation, and nonce management. |
| `EvmErc20TransferTool` | `erc20.py` | Transfer ERC-20 tokens (balanceOf/decimals aware) with gas estimation and optional gas price overrides. |
| `EvmBalanceTool` | `balance.py` | Read native or ERC-20 balances for any address. |
| `EvmSwapTool` | `swap.py` | Execute same-chain swaps through the Bebop aggregator, including approvals when needed. |
| `EvmSwapQuoteTool` | `quote.py` | Fetch swap quotes without execution; compares Bebop and LiFi outputs when requested. |
| `EvmBridgeTool` | `bridge.py` | Bridge assets across chains via LiFi advanced routes and step transactions. |

## Tool Notes

### `EvmTransferTool`
- Defaults to `EVM_PROVIDER_URL`/`EVM_PRIVATE_KEY` but also checks `RPC_URL` so the tool can run inside broader Spoon stacks.
- Automatically estimates gas; if estimation fails it falls back to 21k for plain transfers or 100k when `data` is present.
- Explicit overrides exist for `gas_limit`, `gas_price_gwei`, and `nonce` so advanced workflows (batched transactions, EOA abstraction) remain possible.

### `EvmErc20TransferTool`
- Resolves token decimals via the contract; if the call fails it assumes 18 decimals, so pass `amount` accordingly.
- Builds and signs a `transfer` call, then waits for completion. Emits `{hash, token, amount, decimals}` in `output`.
- Accepts `gas_price_gwei`; otherwise uses the RPCâ€™s `gas_price`.

### `EvmBalanceTool`
- Returns native balances (Ether-equivalent) when `token_address` is omitted, or ERC-20 balances converted using on-chain decimals. Outputs include only the formatted balance value (no decimals field).
- Useful for guardrails before making transfers or swaps.

### `EvmSwapTool`
- Calls Bebopâ€™s router for supported chains (`1`, `10`, `137`, `42161`, `8453`, `59144`). Unsupported IDs return a descriptive error.
- Handles ERC-20 approvals automatically by checking allowance against `approvalTarget` and submitting an `approve` tx when required.
- Accepts `gas_price_gwei` overrides; otherwise reuses the aggregator-suggested gas price or the RPC default.
- `slippage_bps` is currently informational because Bebop does not accept slippage; expect fills to match aggregator routing.

### `EvmSwapQuoteTool`
- Works in quote-only contexts where you want to compare outputs without signing anything.
- Resolve chain ID via `chain_id` parameter or by pointing `rpc_url` at the target network.
- `aggregator="both"` (default) fetches Bebop and LiFi; response includes `quotes` plus a precalculated `best` entry ranked by min output amount.
- Requires RPC access for ERC-20 decimals when quoting token sales; native sells use zero address.

### `EvmBridgeTool`
- Wraps LiFiâ€™s `/advanced/routes` + `/advanced/stepTransaction`. After selecting the recommended path, it executes the first step locally and waits for the on-chain receipt.
- Performs ERC-20 approvals before bridging when necessary; approvals and bridge txs both honor `gas_price_gwei`.
- `from_address` is implied from the signer, while `to_address` defaults to the same account but can target any destination address.
- Returns hashes plus source/destination chain IDs so orchestrators can track the bridge until completion on the far chain.

## Operational Tips

- Centralize signer management: store hot keys in a secure vault and inject them as environment variables just before launching the agent process.
- Throttle swap/bridge calls if you expect to fire many approvals quickly; both Bebop and LiFi enforce rate limits.
- Log the `ToolResult` payloadsâ€”especially `output["hash"]`â€”so you can reconcile actions if an agent crashes mid-run.
- When rotating RPC endpoints, prefer passing `rpc_url` explicitly to keep observability clear (environment-based fallbacks can mask misconfiguration).

---

FILE: Toolkit/crypto/neo.md

`spoon_toolkits.crypto.neo` bundles async Neo N3 helpers on top of the `neo-mamba` RPC client, covering addresses, assets, blocks, governance, and smart-contract introspection. Every tool subclasses `BaseTool` so agents get consistent parameter validation and `ToolResult` payloads.

## Environment & Network Selection

```bash
export NEO_MAINNET_RPC=https://mainmagnet.ngd.network:443   # overrides default mainnet RPC
export NEO_TESTNET_RPC=https://testmagnet.ngd.network:443   # overrides default testnet RPC
export NEO_RPC_TIMEOUT=20                                   # seconds (defaults to 60 if unset)
export NEO_RPC_ALLOW_INSECURE=false                         # set true to skip TLS verification
```

- Each tool accepts `network` (`"mainnet"` / `"testnet"`) so you can switch clusters per call. If you omit it, `NeoProvider` defaults to testnet.
- Address inputs may be Base58 (e.g., `Nf2C...`) or `0x` script hashes; `NeoProvider` normalizes them internally before hitting RPCs.
- `NEO_RPC_TIMEOUT` defaults to 60â€¯seconds in `NeoProvider`. Set the env var to override that per deployment.

## Toolkit Map

| Module | Highlights |
| --- | --- |
| `address_tools.py` | Address counts/info, RPC validation, per-contract NEP-17 sent/received totals, and paginated transfer history. |
| `asset_tools.py` | Asset metadata by hash/name, bulk metadata lookups, and contract/address holding snapshots via `GetAssetInfoByAssetAndAddress`. |
| `block_tools.py` | Block count, block-by-height/hash, best-hash, rewards, and rolling recent block summaries. |
| `transaction_tools.py` | Transaction counts, raw tx retrieval (by block hash/height/address/transaction hash), and transfer event extraction. |
| `contract_tools.py` / `sc_call_tools.py` | Contract inventories, verified contract metadata, and smart-contract call traces filtered by contract/address/tx. |
| `nep_tools.py` | Specialized NEP-11/NEP-17 transfer feeds (by address, block height, contract hash) plus balance helpers. |
| `governance_tools.py` / `voting_tools.py` | Committee info, candidate/voter tallies, on-chain vote call traces. |
| `log_state_tools.py` | Application log/state fetchers for debugging contract execution. |
| `neo_provider.py`, `base.py` | Shared provider, RPC request helpers, normalization utilities, and graceful fallbacks when neo-mamba lacks a direct method. |

###  Official N3 RPC helpers

| Tool | RPC | Description |
| --- | --- | --- |
| `ValidateAddressTool` (`address_tools.py`) | `validateaddress` | Calls the official RPC to confirm if an address/script hash is valid for the selected network and returns metadata. |


## Usage Patterns

> **Note on outputs:** every `BaseTool` wraps the payload in a formatted string such as `"Address info: {...}"`. When you need the underlying dict, strip the prefix and parse it (e.g., with `ast.literal_eval` or `json.loads` after replacing single quotes).

### Address intelligence
```python
from ast import literal_eval
from spoon_toolkits.crypto.neo import GetAddressInfoTool

tool = GetAddressInfoTool()
result = await tool.execute(
    address="Nf2CXE8s1R6yoZ6e52xX5yb7Z9Uv7S3N1h",
    network="mainnet",
)
prefix, _, payload = result.output.partition(": ")
balances = literal_eval(payload)  # includes NEP-17 balances + script hash
```

```python
from ast import literal_eval
from spoon_toolkits.crypto.neo import ValidateAddressTool

validation = await ValidateAddressTool().execute(
    address="NaU3shtZqnR1H6XnDTxghorgkXN687C444",
    network="testnet",
)
_, _, payload = validation.output.partition(": ")
metadata = literal_eval(payload)  # {"isvalid": True, "address": "...", ...}
```

### Asset metadata & holdings
```python
from ast import literal_eval
from spoon_toolkits.crypto.neo import (
    GetAssetInfoByHashTool,
    GetAssetInfoByAssetAndAddressTool,
)

asset_info = await GetAssetInfoByHashTool().execute(
    asset_hash="0xef4073a0f2b305a38ec4050e4d3d28bc40ea63f5",  # GAS
    network="mainnet",
)
_, _, asset_payload = asset_info.output.partition(": ")
asset_dict = literal_eval(asset_payload)

holding = await GetAssetInfoByAssetAndAddressTool().execute(
    asset_hash="0xef4073a0f2b305a38ec4050e4d3d28bc40ea63f5",
    address="Nf2CXE8s1R6yoZ6e52xX5yb7Z9Uv7S3N1h",
    network="mainnet",
)
_, _, holding_payload = holding.output.partition(": ")
holding_dict = literal_eval(holding_payload)  # contract/address balance snapshot
```

### Block & transaction lookups
```python
from ast import literal_eval
from spoon_toolkits.crypto.neo import (
    GetBlockByHeightTool,
    GetRawTransactionByTransactionHashTool,
)

block = await GetBlockByHeightTool().execute(block_height=4500000, network="mainnet")
_, _, block_payload = block.output.partition(": ")
block_dict = literal_eval(block_payload)

tx = await GetRawTransactionByTransactionHashTool().execute(
    transaction_hash="0x...",
    network="mainnet",
)
_, _, tx_payload = tx.output.partition(": ")
tx_dict = literal_eval(tx_payload)
```

### Governance snapshots
```python
from ast import literal_eval
from spoon_toolkits.crypto.neo import GetCommitteeInfoTool, GetVotesByCandidateAddressTool

committee = await GetCommitteeInfoTool().execute(network="mainnet")
_, _, committee_payload = committee.output.partition(": ")
committee_dict = literal_eval(committee_payload)

votes = await GetVotesByCandidateAddressTool().execute(candidate_address="NVSX...", network="mainnet")
_, _, votes_payload = votes.output.partition(": ")
votes_dict = literal_eval(votes_payload)
```

### Token volume totals per address & contract
```python
from ast import literal_eval
from spoon_toolkits.crypto.neo import GetTotalSentAndReceivedTool

volume = await GetTotalSentAndReceivedTool().execute(
    contract_hash="0xef4073a0f2b305a38ec4050e4d3d28bc40ea63f5",
    address="Nf2CXE8s1R6yoZ6e52xX5yb7Z9Uv7S3N1h",
    network="mainnet",
)
_, _, volume_payload = volume.output.partition(": ")
volume_dict = literal_eval(volume_payload)  # {"received": "...", "sent": "..."}
```
## Operational Notes

- Always `await` tool execution; the provider relies on async context managers to open/close RPC sessions cleanly. If you compose multiple calls, instantiate the tool once and re-use it to avoid repeated class construction overhead.

---

FILE: Toolkit/crypto/powerdata.md

`spoon_toolkits.crypto.crypto_powerdata` fuses CCXT-powered CEX feeds, OKX Web3 DEX data, TA-Lib/enhanced indicators, and an MCP server that can stream results over stdio or SSE. Use it when agents need richer analytics than simple price lookups.

## Environment & Settings

```bash
export OKX_API_KEY=...
export OKX_SECRET_KEY=...
export OKX_API_PASSPHRASE=...
export OKX_PROJECT_ID=...
export OKX_BASE_URL=https://web3.okx.com/api/v5/   # optional override

# Optional overrides (defaults shown)
export RATE_LIMIT_REQUESTS_PER_SECOND=10
export MAX_RETRIES=3
export RETRY_DELAY=1.0
export TIMEOUT_SECONDS=30
```

`data_provider.Settings` ingests these variables (plus indicator defaults such as SMA/EMA periods). Missing OKX keys raise immediately before any HTTP call, so configure them centrallyâ€”either via environment or by passing `env_vars` into the MCP helpers.

## Whatâ€™s Inside the Toolkit

<table>
  <colgroup>
    <col style={{ width: "22%" }} />
    <col style={{ width: "22%" }} />
    <col style={{ width: "56%" }} />
  </colgroup>
  <thead>
    <tr>
      <th>Component</th>
      <th>File(s)</th>
      <th>Purpose</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>`CryptoPowerDataCEXTool`</td>
      <td>`tools.py`</td>
      <td>Pull OHLCV candles from 100+ CCXT exchanges and pipe them through the enhanced indicator stack.</td>
    </tr>
    <tr>
      <td>`CryptoPowerDataDEXTool`</td>
      <td>`tools.py`</td>
      <td>Hit OKX Web3 DEX APIs for on-chain pairs specified by `chain_index` + token address.</td>
    </tr>
    <tr>
      <td>`CryptoPowerDataPriceTool`</td>
      <td>`tools.py`</td>
      <td>Lightweight spot price snapshot (CEX or DEX) without fetching an entire candle set.</td>
    </tr>
    <tr>
      <td>`CryptoPowerDataIndicatorsTool`</td>
      <td>`tools.py`</td>
      <td>Enumerate every indicator name/parameter accepted by the enhanced TA registry (TA-Lib + custom extras).</td>
    </tr>
    <tr>
      <td>`Settings`, `OKXDEXClient`, `TechnicalAnalysis`</td>
      <td>`data_provider.py`</td>
      <td>Central place for rate limiting, retries, authenticated OKX calls, and TA-Lib helpers.</td>
    </tr>
    <tr>
      <td>MCP server runners (`start_crypto_powerdata_mcp_*`)</td>
      <td>`server.py`, `dual_transport_server.py`</td>
      <td>Start stdio or HTTP/SSE transports so UI agents can subscribe to continuous feeds.</td>
    </tr>
    <tr>
      <td>Analytics core</td>
      <td>`main.py`, `enhanced_indicators.py`, `talib_registry.py`</td>
      <td>Parse indicator configs, register TA functions, and expose them via FastMCP tools.</td>
    </tr>
  </tbody>
</table>

All tools inherit `CryptoPowerDataBaseTool`, which lazily initializes global settings and reuses throttled clients; you rarely need to micromanage sessions yourself.

## Indicator Configuration Cheatsheet

- Accepts either JSON strings (most MCP clients) or native dicts. Double-encoded JSON like `"\"{\\\"ema\\\": ...}\""` is auto-decoded.
- Mix-and-match multiple parameters per indicator:  
  `{"ema": [{"timeperiod": 12}, {"timeperiod": 26}], "macd": [{"fastperiod": 12, "slowperiod": 26, "signalperiod": 9}]}`
- Enhanced registry supports 150+ TA-Lib functions plus custom composites (VWAP, BB width/position, Aroon oscillators, etc.).
- Validation errors bubble back as descriptive `ToolResult.error` messages so you can surface them directly to users.

## Usage Patterns

### CEX candles + indicators

```python
from spoon_toolkits.crypto.crypto_powerdata import CryptoPowerDataCEXTool

tool = CryptoPowerDataCEXTool()
result = await tool.execute(
    exchange="binance",
    symbol="BTC/USDT",
    timeframe="1h",
    limit=200,
    indicators_config='{"ema": [{"timeperiod": 12}, {"timeperiod": 26}], "rsi": [{"timeperiod": 14}]}',
)

ohlcv_rows = result.output            # only the candle rows are returned
# metadata is not exposed by the ToolResult wrapper yet
```

### DEX analytics on OKX Web3

```python
from spoon_toolkits.crypto.crypto_powerdata import CryptoPowerDataDEXTool

dex = CryptoPowerDataDEXTool()
result = await dex.execute(
    chain_index="1",                      # Ethereum
    token_address="0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2",  # WETH
    timeframe="1H",
    limit=150,
    indicators_config='{"macd": [{"fastperiod": 12, "slowperiod": 26, "signalperiod": 9}], "bb": [{"period": 20, "std": 2}]}',
)

candles = result.output               # ToolResult output already contains the payload
```

### Real-time price snapshot

```python
from spoon_toolkits.crypto.crypto_powerdata import CryptoPowerDataPriceTool

price_tool = CryptoPowerDataPriceTool()
btc = await price_tool.execute(source="cex", exchange="okx", symbol="BTC/USDT")
dex_price = await price_tool.execute(source="dex", chain_index="42161", token_address="0xFF970A61A04b1cA14834A43f5de4533eBDDB5CC8")
```

### Discover supported indicators

```python
from spoon_toolkits.crypto.crypto_powerdata import CryptoPowerDataIndicatorsTool

catalog = await CryptoPowerDataIndicatorsTool().execute()
print(catalog.output["indicators"])   # list of indicator metadata with defaults
```

## MCP Server & Streaming

- `start_crypto_powerdata_mcp_stdio(env_vars=...)` spins up a FastMCP stdio server; pass `background=True` if you need it alongside other async work.
- `start_crypto_powerdata_mcp_sse(host, port, env_vars)` exposes identical tools over HTTP + Server-Sent Events (see `dual_transport_server.py` for endpoints `/mcp` and `/health`).
- `start_crypto_powerdata_mcp_auto` picks stdio vs SSE automatically based on environment; handy for container images.
- `CryptoPowerDataMCPServer` keeps track of running threads so you can query `status()` or stop everything on shutdown.
- `mcp_bridge.py` wires FastMCP methods into the dual transport so CLI agents and browser extensions consume the same tool definitions.

The current HTTP/SSE server keeps an `/mcp` SSE connection alive with heartbeats, but tool responses are delivered via JSON-RPC POST replies rather than pushed over the SSE channel. Keep calling the tools periodically if you need fresh data; the same OKX rate limiting (`rate_limit_requests_per_second`) and retry envelopes apply regardless of transport.

---

FILE: Toolkit/crypto/solana.md

---
id: solana
title: Solana Tools
---

# Solana Tools

`spoon_toolkits.crypto.solana` provides Python-based Solana blockchain tools built on top of `solana-py`, offering native SOL transfers, SPL token operations, and Jupiter-powered token swaps. Populate the tooling constants in `spoon_toolkits.crypto.solana.constants` (token program IDs, common mint addresses) before relying on SPL features or symbol shortcutsâ€”the defaults are intentionally left as `None`.

## Environment & Dependencies

```bash
# Required runtime settings / env vars
export SOLANA_RPC_URL=https://api.mainnet-beta.solana.com
export SOLANA_PRIVATE_KEY=your_base58_or_base64_private_key


# Optional extras
export HELIUS_API_KEY=your_helius_key   # Enables enriched RPC + webhooks
export BIRDEYE_API_KEY=your_birdeye_key # Enables live price + portfolio data
```

The keypair loader accepts both base58 (phantom export) and base64 strings, and you can override any parameter per call via the tool arguments.

**Dependencies:**
- `solana-py` â€“ RPC client used by transfers, swaps, and wallet reads
- `solders` â€“ Fast keypair/pubkey primitives for signing
- `spl.token` â€“ SPL Token program bindings (ATA creation, transfers)
- `httpx` â€“ Async Jupiter and Birdeye integrations

## Tool Catalog

### Transfer Tools

#### `SolanaTransferTool`

Transfer SOL or SPL tokens to another address.

**Parameters:**
- `recipient` (str, **required**) - Destination Solana address
- `amount` (str/number, **required**) - Amount in human-readable units
- `token_address` (str, optional) - SPL token mint address; omit for SOL
- `rpc_url` (str, optional) - RPC endpoint override
- `private_key` (str, optional) - Sender private key override

**Returns:**
```python
ToolResult(output={
    "success": True,
    "signature": "5j7s...",
    "amount": "1.5",
    "recipient": "9jW8F..."
})
```

**Example:**
```python
from spoon_toolkits.crypto.solana import SolanaTransferTool

# Transfer SOL
transfer_tool = SolanaTransferTool()
result = await transfer_tool.execute(
    recipient="9jW8FPr6BSSsemWPV22UUCzSqkVdTp6HTyPqeqyuBbCa",
    amount="0.1"
)
print(f"Signature: {result.output['signature']}")

# Transfer SPL token (USDC)
result = await transfer_tool.execute(
    recipient="9jW8FPr6BSSsemWPV22UUCzSqkVdTp6HTyPqeqyuBbCa",
    amount="100",
    token_address="EPjFW..."
)
```

**Features:**
-  Automatic ATA (Associated Token Account) creation for recipients
-  Transfers execute immediately after submission; confirmations and decimal precision checks should be handled by the caller if needed.

### Swap Tools

#### `SolanaSwapTool`

Execute token swaps using Jupiter aggregator with intelligent token resolution.

**Parameters:**
- `input_token` (str, **required**) - Input token (symbol/mint/address)
- `output_token` (str, **required**) - Output token (symbol/mint/address)
- `amount` (str/number, **required**) - Amount to swap
- `slippage_bps` (int, optional) - Slippage tolerance in basis points (default: dynamic)
- `priority_level` (str, optional) - Transaction priority: `low`, `medium`, `high`, `veryHigh` (default)
- `rpc_url` (str, optional) - RPC endpoint override
- `private_key` (str, optional) - Wallet private key override

**Returns:**
```python
ToolResult(output={
    "success": True,
    "signature": "3xK9...",
    "input_token": "SOL",
    "input_mint": "So111...",
    "output_token": "USDC",
    "output_mint": "EPjFW...",
    "input_amount": "1.0",
    "output_amount": "150.23",
    "price_impact": 0.002,
    "slippage_bps": 50,
    "route_plan": [...],
    "fees": {"transaction_fee": 5000, "fee_sol": 0.000005}
})
```

**Example:**
```python
from spoon_toolkits.crypto.solana import SolanaSwapTool

swap_tool = SolanaSwapTool()

# Swap using symbols (automatically resolved from wallet)
result = await swap_tool.execute(
    input_token="SOL",
    output_token="USDC",
    amount="1.0",
    slippage_bps=50,  # 0.5% slippage
    priority_level="high"
)

# Swap using mint addresses
result = await swap_tool.execute(
    input_token="So11111111111111111111111111111111111111112",
    output_token="EPjF...",
    amount="1.0"
)
```

**Advanced Features:**

**1. Smart Token Resolution**

Supports multiple input formats:
- Symbol: `"SOL"`, `"USDC"`, `"$BONK"`
- Mint address: `"So111..."`
- Portfolio lookup: Searches your wallet's token holdings

```python
# All these work:
await swap_tool.execute(input_token="SOL", ...)        # Symbol
await swap_tool.execute(input_token="$BONK", ...)      # From wallet
await swap_tool.execute(input_token="So111...", ...)   # Mint address
```

**2. Priority Fee Levels**

| Level | Max Lamports | Use Case |
|-------|-------------|----------|
| `low` | 50 | Non-urgent swaps |
| `medium` | 200 | Normal operations |
| `high` | 1,000 | Time-sensitive |
| `veryHigh` | 4,000,000 | MEV protection (default) |

**3. Dynamic Quote Context Building**

The tool validates inputs, resolves decimals, and fetches Jupiter quotes before execution:
```python
# Internally handles:
# - Token existence validation
# - Decimal precision checks
# - Amount > 0 validation
# - Slippage bounds (1-10000 bps)
# - Jupiter quote fetching
# - Output amount formatting
```

### Wallet Tools

#### `SolanaWalletInfoTool`

Query comprehensive wallet information including SOL balance and SPL token holdings.

**Parameters:**
- `address` (str, optional) - Wallet address; defaults to configured wallet
- `include_tokens` (bool, optional) - Include SPL token balances (default: `True`)
- `token_limit` (int, optional) - Max tokens to return (default: 20)
- `rpc_url` (str, optional) - RPC endpoint override

**Returns:**
```python
ToolResult(output={
    "address": "9jW8F...",
    "truncated_address": "9jW8...BbCa",
    "sol_balance": 1.523456789,
    "lamports": 1523456789,
    "token_count": 5,
    "tokens": [
        {
            "mint": "EPjFW...",
            "balance": "150.23",
            "decimals": 6,
            "raw_balance": "150230000"
        },
        # ...
    ]
})
```

**Example:**
```python
from spoon_toolkits.crypto.solana import SolanaWalletInfoTool

wallet_tool = SolanaWalletInfoTool()

# Query configured wallet
result = await wallet_tool.execute()
print(f"SOL Balance: {result.output['sol_balance']}")
print(f"Token Count: {result.output['token_count']}")

# Query specific wallet
result = await wallet_tool.execute(
    address="9jW8FPr6BSSsemWPV22UUCzSqkVdTp6HTyPqeqyuBbCa",
    include_tokens=True,
    token_limit=10
)
```

- **Features:**
- âœ… **Wallet Cache Scheduler** â€“ Results are cached per `(rpc_url, address)` so repeated reads avoid RPC calls. The scheduler refresh cadence is 120s by default and can be forced manually.
- âœ… **Token metadata basics** â€“ Each entry includes the mint, UI balance, decimals, and raw balance. Birdeye metadata (names, symbols, USD totals) is not injected automaticallyâ€”use service helpers to enrich the cached data when an API key is present.
- âœ… **Portfolio cache hook** â€“ The same cache powers the swap helperâ€™s â€œsmart token resolution,â€ so swaps can reference symbols (`SOL`, `$BONK`, etc.) without extra lookups.
- âœ… **Optional price data** â€“ When `BIRDEYE_API_KEY` is present, the scheduler records token prices and wallet USD totals, which can be consumed through the service helpers.

## Service Helpers

The toolkit provides 30+ utility functions in `service.py`:

### Validation

```python
from spoon_toolkits.crypto.solana import (
    validate_solana_address,
    validate_private_key,
    is_native_sol
)

# Address validation
is_valid = validate_solana_address("9jW8FPr6BSSsemWPV22UUCzSqkVdTp6HTyPqeqyuBbCa")

# Private key validation (base58/base64)
is_valid = validate_private_key("5j7s...")

# Check if token is native SOL
is_sol = is_native_sol("So11111111111111111111111111111111111111112")
```

### Conversion

```python
from spoon_toolkits.crypto.solana import (
    lamports_to_sol,
    sol_to_lamports,
    format_token_amount,
    parse_token_amount
)

# SOL <-> Lamports
lamports = sol_to_lamports(1.5)  # â†’ 1500000000
sol = lamports_to_sol(1500000000)  # â†’ 1.5

# Token amount formatting
ui_amount = format_token_amount(150230000, decimals=6)  # â†’ 150.23
raw_amount = parse_token_amount(150.23, decimals=6)  # â†’ 150230000
```

### Address Utilities

```python
from spoon_toolkits.crypto.solana import (
    get_associated_token_address,
    truncate_address,
    detect_pubkeys_from_string
)

# Get ATA for token
ata = get_associated_token_address(
    token_mint="EPjFW...",
    owner="9jW8F..."
)

# Shorten address for display
short = truncate_address("9jW8FPr6BSSsemWPV22UUCzSqkVdTp6HTyPqeqyuBbCa")
# â†’ "9jW8...BbCa"

# Extract addresses from text
pubkeys = detect_pubkeys_from_string("Send 1 SOL to 9jW8F...")
```

### Error Parsing

```python
from spoon_toolkits.crypto.solana import parse_transaction_error

# Error normalization (currently pass-through)
friendly = parse_transaction_error("Error: 0x1")
# â†’ "Error: 0x1"
```

## Keypair Management

```python
from spoon_toolkits.crypto.solana import (
    get_wallet_keypair,
    get_wallet_key,
    get_private_key,
    get_public_key
)

# Get full keypair (requires private key)
keypair_result = get_wallet_keypair(require_private_key=True)
if keypair_result.keypair:
    print(f"Public Key: {keypair_result.keypair.pubkey()}")

# Get public key only
keypair_result = get_wallet_keypair(require_private_key=False)
if keypair_result.public_key:
    print(f"Public Key: {keypair_result.public_key}")

# Dynamic private key support
keypair_result = get_wallet_key(
    require_private_key=True,
    private_key="5j7s..."  # Override env var
)
```

## Advanced: Wallet Cache Scheduler

**Unique to Python version** - Background service that keeps wallet data fresh.

```python
from spoon_toolkits.crypto.solana import get_wallet_cache_scheduler

scheduler = get_wallet_cache_scheduler()

# Start background refresh (runs every 60s)
await scheduler.ensure_running(
    rpc_url="https://api.mainnet-beta.solana.com",
    wallet_address="9jW8F...",
    include_tokens=True
)

# Get cached data (no RPC call)
cached = await scheduler.get_cached(
    rpc_url="https://api.mainnet-beta.solana.com",
    wallet_address="9jW8F..."
)

if cached:
    wallet_data = cached["data"]
    print(f"SOL: {wallet_data['sol_balance']}")

# Force immediate refresh
fresh_data = await scheduler.force_refresh(
    rpc_url="https://api.mainnet-beta.solana.com",
    wallet_address="9jW8F...",
    include_tokens=True
)
```

**Benefits:**
-  Automatic background updates
-  Instant wallet info access for swap token resolution

## Constants

```python
from spoon_toolkits.crypto.solana import (
    TOKEN_ADDRESSES,
    DEFAULT_SLIPPAGE_BPS,
    JUPITER_PRIORITY_LEVELS
)

# Provide your own well-known addresses; the shipped defaults are None placeholders.
TOKEN_ADDRESSES["USDC"] = "Wdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v"
TOKEN_ADDRESSES["BONK"] = "DezX..."

# Default configuration helpers
printEPjF(DEFAULT_SLIPPAGE_BPS)       # e.g. 100 (1%)
print(JUPITER_PRIORITY_LEVELS)    # {"low": 50, "medium": 200, "high": 1000, "veryHigh": 4000000}
```

## Operational Notes

### Error Handling

Always check `ToolResult.error`:

```python
result = await swap_tool.execute(...)

if result.error:
    print(f" Swap failed: {result.error}")
    if result.diagnostic:
        print(f"Details: {result.diagnostic}")
else:
    print(f" Swap successful: {result.output['signature']}")
```

### Rate Limiting

Jupiter API limits:
- Quote endpoint: ~10 req/s
- Swap endpoint: ~5 req/s

Use the wallet cache scheduler to minimize RPC calls:

```python
# Bad: Queries RPC every time
for token in tokens:
    await wallet_tool.execute(address="...", include_tokens=True)

# Good: Use cache scheduler
scheduler = get_wallet_cache_scheduler()
await scheduler.ensure_running("...", "...", True)
for token in tokens:
    cached = await scheduler.get_cached("...", "...")
```

---

FILE: Toolkit/data-platforms/chainbase.md

`spoon_toolkits.data_platforms.chainbase` wraps the Chainbase REST API in async `BaseTool` classes and mounts them on a FastMCP server, so agents can query block, transaction, account, and token data without writing raw HTTP calls.

## Environment & Configuration

```bash
export CHAINBASE_API_KEY=your_chainbase_key             # required for every request
export CHAINBASE_HOST=0.0.0.0                           # optional when running the MCP server
export CHAINBASE_PORT=8000                              # optional (default 8000)
export CHAINBASE_PATH=/sse                              # optional SSE path
```

- Every tool loads `CHAINBASE_API_KEY` at execution time and aborts with a descriptive error if it is missing.
- HTTP calls go to `https://api.chainbase.online/v1/...`; you can override host/port/path only when launching the bundled FastMCP server.

## Package Layout

| Module | Purpose |
| --- | --- |
| `chainbase_tools.py` | Source of the `BaseTool` implementations (`GetLatestBlockNumberTool`, `GetAccountTokensTool`, `ContractCallTool`, etc.). |
| `balance.py`, `basic.py`, `token_api.py` | Individual FastMCP sub-servers exposing account, base, and token endpoints as MCP tools/resources (with docs baked in). |
| `__init__.py` | Aggregates all MCP sub-servers via `FastMCP` and re-exports the tool classes. Running the module spins up an SSE server. |
| `README.md` | Lists supported chains (`chain_id` values) and shows quick-start snippets. |

## Tooling Highlights

### Blocks & Transactions
- `GetLatestBlockNumberTool` â€“ current block height by `chain_id`.
- `GetBlockByNumberTool` â€“ detailed block payload (transactions, miner, timestamp).
- `GetTransactionByHashTool` â€“ fetch by tx hash or `(block_number, tx_index)` combo.
- `GetAccountTransactionsTool` â€“ paginated tx history per address with optional block/timestamp filters.

### Accounts & Portfolios
- `GetAccountBalanceTool` â€“ native coin balance with optional `to_block`.
- `GetAccountTokensTool` â€“ ERC-20 balances, limit/page params, optional contract filter.
- `GetAccountNFTsTool` â€“ NFT holdings, same pagination semantics.

### Contracts & Tokens
- `ContractCallTool` â€“ invoke read-only contract functions by supplying ABI JSON and params.
- `GetTokenMetadataTool` â€“ ERC-20 metadata (name, symbol, decimals, total supply).

On success every tool returns Chainbaseâ€™s raw JSON payload: `{"code": ..., "message": ..., "data": [...]}`. When anything fails (missing API key, HTTP error, etc.) the wrapper returns `{"error": "..."}` instead of raising, so always check for an `error` key before consuming `data`.

## Usage Examples

### Fetch the latest Ethereum block
```python
from spoon_toolkits.data_platforms.chainbase import GetLatestBlockNumberTool

height_tool = GetLatestBlockNumberTool()
block = await height_tool.execute(chain_id=1)
print(block["data"])
```

### Enumerate ERC-20 holdings for a wallet
```python
from spoon_toolkits.data_platforms.chainbase import GetAccountTokensTool

tokens = await GetAccountTokensTool().execute(
    chain_id=1,
    address="0xd8dA6BF26964aF9D7eEd9e03E53415D37aA96045",
    limit=50,
)

for token in tokens.get("data", []):
    print(token["symbol"], token["balance"])
```

### Run a read-only contract call
```python
from spoon_toolkits.data_platforms.chainbase import ContractCallTool

result = await ContractCallTool().execute(
    chain_id=1,
    contract_address="0x6B175474E89094C44Da98b954EedeAC495271d0F",  # DAI
    function_name="totalSupply",
    abi='[{"inputs":[],"name":"totalSupply","outputs":[{"type":"uint256"}],"stateMutability":"view","type":"function"}]',
    params=[],
)
print(result["data"])
```

## FastMCP Server Mode

To expose these tools over SSE (useful for MCP-compatible frontends), run:

```bash
python -m spoon_toolkits.data_platforms.chainbase
# or, in code:
from spoon_toolkits.data_platforms.chainbase import mcp_server
mcp_server.run(transport="sse", host="0.0.0.0", port=8000, path="/sse")
```

Internally, the server mounts three subdomains (`Balance`, `Basic`, `TokenAPI`) so you can enable only the scopes you need.

## Operational Notes

- Chainbase enforces per-key rate limits; handle `{"error": "...429..."}` responses by backing off or narrowing ranges (`limit`, `page`).
- Because the SDK just wraps REST, you can set `CHAINBASE_API_KEY` at runtime (e.g., from a vault) before invoking any tool to support multi-tenant agents.
- For deterministic pipelines, inspect the `code` and `message` fields returned by Chainbase before assuming `data` is presentâ€”errors come back with HTTP 200 but non-zero `code`.

---

FILE: Toolkit/data-platforms/desearch.md

`spoon_toolkits.data_platforms.desearch` wraps the official DeSearch SDK in async helpers, builtin Spoon tools, and FastMCP servers so agents can query real-time web, social, and academic sources without writing raw HTTP logic. The async helpers (used by MCP and builtin tools) decorate responses with a `{"error": "..."}` payload when the SDK raises, so callers should check for that key before dereferencing results.

## Environment & Configuration

```bash
export DESEARCH_API_KEY=your_actual_key            # required
export DESEARCH_BASE_URL=https://api.desearch.ai   # optional (not currently consumed)
export DESEARCH_TIMEOUT=30                         # optional (read but not yet wired into SDK calls)
```

- `env.py` loads `.env` via `python-dotenv`, so store the variables beside the process that imports the package.
- `DESEARCH_BASE_URL`/`DESEARCH_TIMEOUT` are parsed for future use but the current helpers always talk to the SDK defaults; changing these env vars today has no effect without modifying the code.
- The helpers enforce `limit >= 10`, mirroring the SDK requirements.

## Package Layout

| Module | Purpose |
| --- | --- |
| `__init__.py` | Mounts `ai_search` and `web_search` FastMCP sub-servers and re-exports helper coroutines plus `mcp_server`. |
| `ai_search_official.py` | Async tools for AI/meta search, Reddit/Twitter feeds, and academic datasets. Decorated with `@mcp.tool()`. |
| `web_search_official.py` | Web search plus Twitter post/link lookups using the official SDK. |
| `builtin_tools.py` | `BaseTool` wrappers (`DesearchAISearchTool`, etc.) for spoon-core usage without MCP. These return the same dicts as the async helpers (including any `"error"` payloads), so callers must handle error cases explicitly. |
| `cache.py` | `time_cache` decorator that memoizes tool responses for ~5 minutes. |
| `example.py`, `test_integration.py`, `README.md` | Usage demos, live API smoke tests, and a deeper quick-start. |

## Tooling Highlights

### AI and Social Search
- `search_ai_data(query, platforms, limit)` aggregates results from web, Reddit, Wikipedia, YouTube, Twitter, ArXiv, and HackerNews.
- `search_social_media(query, platform, limit)` targets Twitter or Reddit directly.
- `search_academic(query, platform, limit)` narrows to ArXiv or Wikipedia research corpora.

### Web and Twitter Search
- `search_web(query, num_results, start)` calls `basic_web_search` and returns snippet-rich results.
- `search_twitter_posts(query, limit, sort)` pulls live tweets with sort control (Top, Latest, etc.).
- `search_twitter_links(query, limit)` surfaces URLs that are trending on Twitter.

### Builtin Tools and Caching
- `DesearchAISearchTool`, `DesearchWebSearchTool`, `DesearchAcademicSearchTool`, and `DesearchTwitterSearchTool` validate API keys up front and expose JSON schemas for planners.
- `time_cache` decorates every MCP tool, preventing duplicate outbound calls during iterative planning loops.

## Usage Examples

### Mount the FastMCP server inside a Spoon agent
```python
from spoon_toolkits.data_platforms.desearch import mcp_server

agent_config = {
    "tools": [mcp_server],   # exposes ai_search.* and web_search.* namespaces
}
```

### Call async helpers directly
```python
from spoon_toolkits.data_platforms.desearch import (
    search_ai_data,
    search_web,
)

async def summarize(topic: str):
    ai = await search_ai_data(query=f"{topic} 2024", platforms="web,reddit,wikipedia", limit=12)
    web = await search_web(query=topic, num_results=5)
    return {"ai": ai, "web": web}
```

### Use builtin tools when working in spoon-core
```python
from spoon_toolkits.data_platforms.desearch.builtin_tools import DesearchAISearchTool

tool = DesearchAISearchTool()
response = await tool.execute(query="Solana MEV research", platforms="web,reddit", limit=10)

if not response.get("success"):
    raise RuntimeError(response.get("error"))

print(response["data"]["results"].keys())
```

## FastMCP Server Mode

Run the package as a server to expose the tools over SSE (compatible with MCP-aware clients):

```bash
python -m spoon_toolkits.data_platforms.desearch
# or in code:
from spoon_toolkits.data_platforms.desearch import mcp_server
mcp_server.run(transport="sse", host="0.0.0.0", port=8000, path="/sse")
```

Internally, the server mounts `ai_search` and `web_search` namespaces, so you can scope access per agent.

## Operational Notes

- Use the provided `test_integration.py` before shipping: it validates your API key and SDK wiring.
- Responses are dictionaries (e.g., `{"query": ..., "results": ..., "count": ...}`); check for `"error"` keys when handling failures.
- `DESEARCH_TIMEOUT` is parsed but currently unused; adjust it only after wiring the value into the helper functions or SDK client initialization in your fork.
- Remove cached entries by restarting the process if you need uncached data while debugging.
- Rate limits come from the DeSearch API; stagger large batches or broaden queries rather than hammering the same endpoint.

---

FILE: Toolkit/data-platforms/thirdweb.md

`spoon_toolkits.data_platforms.third_web` wraps the Thirdweb Insight REST API in async `BaseTool` classes so Spoon agents can fetch contract events, multichain transfers, transactions, and block data without crafting HTTP requests by hand. Some tools require you to supply a `client_id` argument, while others read `THIRDWEB_CLIENT_ID` from the environmentâ€”make sure to supply credentials in the format each tool expects.

## Environment & Configuration

```bash
export THIRDWEB_CLIENT_ID=your_client_id          # used by tools that read from env
```

- Tools fall into two credential styles:
  1. `GetContractEventsFromThirdwebInsight`, `GetBlocksFromThirdwebInsight`, and `GetWalletTransactionsFromThirdwebInsight` require a `client_id` argument on every call and never look at `THIRDWEB_CLIENT_ID`.
  2. `GetMultichainTransfersFromThirdwebInsight`, `GetTransactionsTool`, `GetContractTransactionsTool`, and `GetContractTransactionsBySignatureTool` exclusively read `THIRDWEB_CLIENT_ID` and do not expose a per-call override.
- Requests use a 100-second timeout where implemented; a few helpers (notably the block fetcher) currently omit the timeout parameter and will rely on `requests` defaults until updated.
- Each tool catches exceptions and returns either a formatted status string (prefixed with âœ…/âŒ) or a dict like `{"error": "..."}`â€”errors do not raise, and successful calls may return strings rather than raw JSON.

## Package Layout

| Module | Purpose |
| --- | --- |
| `third_web_tools.py` | Houses every `BaseTool` plus lightweight async test helpers. Some `execute` methods return human-readable strings (with emojis and counts) instead of the raw Insight JSON objectâ€”inspect the tool docstrings before assuming `dict` output. Import from `spoon_toolkits.data_platforms.third_web.third_web_tools`. |

## Tooling Highlights

### Events and Transfers
- `GetContractEventsFromThirdwebInsight` - fetch decoded events for a contract + signature (`Transfer(address,address,uint256)`, etc.) with paging metadata. Requires you to pass `client_id` explicitly; returns a status string summarizing the page and event count plus the JSON dump.
- `GetMultichainTransfersFromThirdwebInsight` - scan recent transfers for a list of chain IDs (defaults to USDT events exposed by Insight). Reads `THIRDWEB_CLIENT_ID` from the environment and returns the raw Insight JSON dict.

### Transactions and Blocks
- `GetTransactionsTool` - consolidate recent transactions across multiple chains. Reads `THIRDWEB_CLIENT_ID` and returns the raw Insight JSON dict.
- `GetContractTransactionsTool` - view activity for a single contract. Reads `THIRDWEB_CLIENT_ID`.
- `GetContractTransactionsBySignatureTool` - narrow contract activity down to a specific function signature. Reads `THIRDWEB_CLIENT_ID`.
- `GetBlocksFromThirdwebInsight` - stream the latest blocks per chain with optional sort field and order. Requires a per-call `client_id` and currently returns a formatted status string (`"âœ… Success ..."`) rather than the JSON dict; also omits the explicit 100-second timeout, so the default `requests` timeout applies.
- `GetWalletTransactionsFromThirdwebInsight` - list wallet transactions across multiple chains, sorted by block number or timestamp. Requires a per-call `client_id` and returns a status string similar to the block tool.

Depending on the helper, `client_id` may need to be passed explicitly, and successful responses may be either raw JSON dicts or formatted status strings. If you need structured data, parse the portion after the newline in the status strings (they contain the serialized JSON response). Always check for `âŒ` or an `"error"` key to detect failures because network/Insight errors are caught instead of raised.

## Usage Examples

### Fetch contract events for a signature
```python
from spoon_toolkits.data_platforms.third_web.third_web_tools import GetContractEventsFromThirdwebInsight

tool = GetContractEventsFromThirdwebInsight()
result = await tool.execute(
    client_id="your-client-id",
    chain_id=1,
    contract_address="0xdAC17F958D2ee523a2206206994597C13D831ec7",
    event_signature="Transfer(address,address,uint256)",
    limit=5,
)
if result.startswith("âŒ"):
    raise RuntimeError(result)

# Returned string includes JSON after the newline
summary, _, raw_json = result.partition("\n")
print(summary)
print(raw_json)
```

### Aggregate transfers across chains
```python
from spoon_toolkits.data_platforms.third_web.third_web_tools import GetMultichainTransfersFromThirdwebInsight

tool = GetMultichainTransfersFromThirdwebInsight()
transfers = await tool.execute(chains=[1, 137, 8453], limit=10)
print(transfers["data"][0])
```

### Inspect wallet transactions with sorting
```python
from spoon_toolkits.data_platforms.third_web.third_web_tools import GetWalletTransactionsFromThirdwebInsight

wallet_tool = GetWalletTransactionsFromThirdwebInsight()
history = await wallet_tool.execute(
    client_id="your-client-id",
    wallet_address="0xabc...",
    chains=[1, 137],
    limit=10,
    sort_by="block_timestamp",
    sort_order="desc",
)
if history.startswith("âŒ"):
    raise RuntimeError(history)

_, _, raw_history = history.partition("\n")
print(raw_history)
```

## Operational Notes

- Set `THIRDWEB_CLIENT_ID` in your runtime environment for the env-driven tools, and pass `client_id` for the helpers that require it; missing credentials surface as `ValueError` strings in the result.
- HTTP errors are caught and reported in the returned string/dict instead of bubbling up through `requests.raise_for_status()`, so alerting logic should look for `âŒ` prefixes or `"error"` keys.
- Test helpers at the bottom of `third_web_tools.py` (`test_get_contract_events`, etc.) offer quick sanity checks - run them with `python third_web_tools.py` once your credentials are configured.
- Insight endpoints enforce per-client rate limits; stagger large batch pulls by adjusting `limit` or splitting chain lists across multiple requests.

---

FILE: Toolkit/github/analysis-tools.md

`spoon_toolkits.github.github_analysis_tool` bundles three ready-made `BaseTool` classes that call the GitHub GraphQL API via `GitHubProvider`. Each tool simply returns the raw list of entities (`issues`, `pull requests`, `commits`) interpolated into a single string (e.g., `"GitHub issues: [{...}, ...]"`), so you get the data without crafting queries yourself but will need to parse/count manually if you want aggregates.

## Environment

```bash
export GITHUB_TOKEN=ghp_your_personal_access_token   # repo scope recommended
```

- Each tool pulls `GITHUB_TOKEN` unless you pass `token="..."` explicitly. Missing credentials surface as `ToolResult(error="GitHub token is required...")`.
- GitHubâ€™s GraphQL endpoint enforces a rate limit of 5,000 points/hour per token. These tools request up to 100 nodes per call; batch long-range reports accordingly.

## Tool Catalog

| Tool | Parameters | Raw data returned | Use cases |
| --- | --- | --- | --- |
| `GetGitHubIssuesTool` | `owner`, `repo`, `start_date`, `end_date`, optional `token` | List of up to 100 issues with `title`, `state`, labels, comment totals, author, timestamps | Bug triage dashboards, changelog assembly, governance transparency |
| `GetGitHubPullRequestsTool` | Same window parameters | List of up to 100 pull requests with merge info, review counts, commit totals, labels | Contributor scorecards, weekly PR digests |
| `GetGitHubCommitsTool` | Same window parameters | Default branch history (first 100 commits) including message, author name/email, additions/deletions, `changedFiles` | Release notes, productivity metrics, feature tracking |

All parameters accept ISO `YYYY-MM-DD` strings. The window is inclusive and aligned to UTC. Because the code returns only the raw list embedded in a string, there is no built-in `total_count` or `date_range` metadataâ€”add that logic in your calling code if required.

## Usage Patterns

### Issues snapshot
```python
from spoon_toolkits.github.github_analysis_tool import GetGitHubIssuesTool

issues_tool = GetGitHubIssuesTool()
issues_result = await issues_tool.execute(
    owner="XSpoonAi",
    repo="spoon-core",
    start_date="2024-01-01",
    end_date="2024-02-01",
)

if issues_result.error:
    raise RuntimeError(issues_result.error)

raw_issues = issues_result.output  # e.g., "GitHub issues: [{...}, {...}]"
print(raw_issues)

# Optionally convert the trailing list into Python data:
from ast import literal_eval
issues_payload = literal_eval(raw_issues.replace("GitHub issues: ", "", 1))
for issue in issues_payload:
    print(issue["title"], issue["state"])
```

### Weekly contributor digest (issues â†’ PRs â†’ commits)
```python
from datetime import date, timedelta
from spoon_toolkits.github.github_analysis_tool import (
    GetGitHubIssuesTool,
    GetGitHubPullRequestsTool,
    GetGitHubCommitsTool,
)

today = date.today()
week_ago = today - timedelta(days=7)
window = dict(
    owner="XSpoonAi",
    repo="spoon-core",
    start_date=week_ago.isoformat(),
    end_date=today.isoformat(),
)

issues = await GetGitHubIssuesTool().execute(**window)
prs = await GetGitHubPullRequestsTool().execute(**window)
commits = await GetGitHubCommitsTool().execute(**window)

if any(result.error for result in (issues, prs, commits)):
    raise RuntimeError("GitHub API call failed")

summary = {
    "issues": len(literal_eval(issues.output.replace("GitHub issues: ", "", 1))),
    "pull_requests": len(literal_eval(prs.output.replace("GitHub pull requests: ", "", 1))),
    "commits": len(literal_eval(commits.output.replace("GitHub commits: ", "", 1))),
}
```

## Error Handling Tips

- Inspect `ToolResult.error` before consuming `output`. Authentication failures, invalid repo names, or GraphQL validation errors are surfaced there.
- If GitHub throttles you, the GraphQL API returns `errors: [{"type": "RATE_LIMITED", ...}]`; the tool passes that text through the `error` field.
- Dates outside the repository lifetime simply return an empty list (so the output string becomes `"GitHub <entity>: []"`); no error is raised.

Use these tools when you need standardized analytics quickly. If you outgrow the default fields, switch to the lower-level `GitHubProvider` documented separately to craft custom queries.

---

FILE: Toolkit/github/provider.md

`spoon_toolkits.github.github_provider.GitHubProvider` gives you a thin but flexible GraphQL client built on `gql`. Use it when the stock analysis tools donâ€™t expose the fields or filtering you need.

## Environment & Auth

```bash
export GITHUB_TOKEN=ghp_your_personal_access_token   # repo scope recommended
```

- The constructor reads `GITHUB_TOKEN` automatically and raises `ValueError` immediately if no token is available. Pass `GitHubProvider(token="ghp_...")` to override per request.
- The underlying transport hits `https://api.github.com/graphql`. Rate limits (5,000 points/hour) and GraphQL errors are raised as Python exceptions, so always wrap calls in `try/except`.

## Initialization

```python
from spoon_toolkits.github.github_provider import GitHubProvider

provider = GitHubProvider()
```

Provide the token only once per provider instance. Each method is `async`, so you can reuse the same instance across awaits.

When you need deterministic cleanup (for example in long-running services), use the provider as a context manager; it will close the underlying `gql.Client` transport automatically:

```python
with GitHubProvider() as provider:
    repo = await provider.fetch_repository_info("XSpoonAi", "spoon-core")
```

## Built-in Methods

| Method | What it queries | Useful fields returned |
| --- | --- | --- |
| `fetch_issues(owner, repo, start_date, end_date)` | `issues` connection filtered by creation date | `title`, `state`, `labels`, `comments.totalCount`, `author.login`, timestamps |
| `fetch_pull_requests(...)` | `pullRequests` connection in the same date window | `mergedAt`, `reviews.totalCount`, `commits.totalCount`, labels, author |
| `fetch_commits(...)` | `defaultBranchRef.target.history` limited to 100 commits (GraphQL limit) | `message`, `committedDate`, `additions`, `deletions`, `changedFiles`, `author` |
| `fetch_repository_info(owner, repo)` | `repository` node | `stargazerCount`, `forkCount`, `watcherCount`, open issue/pr counts, topics, license |

The default history queries request 100 nodes. To page further, copy the query strings in `github_provider.py` and extend them with `after` cursors.

## Example: Custom GraphQL + Provider Methods

```python
from spoon_toolkits.github.github_provider import GitHubProvider
from gql import gql

provider = GitHubProvider()

# Use the prebuilt helper
repo = await provider.fetch_repository_info("XSpoonAi", "spoon-core")
print("Stars:", repo["stargazerCount"])

# Extend with your own query
custom_query = gql("""
  query($owner: String!, $repo: String!) {
    repository(owner: $owner, name: $repo) {
      releases(last: 5) {
        nodes {
          name
          tagName
          publishedAt
        }
      }
    }
  }
""")

result = provider.client.execute(custom_query, variable_values={"owner": "XSpoonAi", "repo": "spoon-core"})
```

## Error Handling & Best Practices

- Each method wraps `client.execute` and raises `Exception` with the underlying GraphQL message (`RATE_LIMITED`, `NOT_FOUND`, `FORBIDDEN`, etc.). Catch `Exception` at call sites to degrade gracefully.
- For long-running agents, instantiate the provider once and reuse itâ€”this avoids fetching the schema repeatedly.
- GraphQL schemas are fetched on first transport use (`fetch_schema_from_transport=True`). When developing offline, consider setting that flag to `False` in a forked provider to skip the extra call.

Reach for `GitHubProvider` when you need to compose bespoke queries, add pagination, or stitch multiple GraphQL responses together within a single agent step.

---

FILE: Toolkit/index.md

---
id: index
title: Toolkit Overview
---

# Toolkit Overview

Welcome to the Spoon Toolkit catalog. Everything here is a ready-to-use agent helper that ships with the SpoonOS runtime. The toolkit is organized by domain so you can jump straight to the integrations you need:

- **Audio** â€“ AI-powered audio tools including ElevenLabs for text-to-speech, speech-to-text, voice cloning, voice design, and dubbing in 70+ languages.
- **Crypto** â€“ Market data, EVM utilities, Neo RPC helpers, Chainbase-powered analytics, and more. Use these when your agent needs to trade, monitor, or analyze on-chain activity.
- **Data Platforms** â€“ Providers such as Chainbase, Thirdweb, and Desearch for structured blockchain data without writing bespoke HTTP clients.
- **GitHub Intelligence** â€“ Repository scanners and provider hooks that summarize repos, issues, and activity.
- **Social Media** â€“ Discord, Telegram, Twitter, and Email helpers for outreach and notification workflows.
- **Storage** â€“ Off-chain storage bridges (AIOZ, Oort, Foureverland) so agents can archive artifacts or publish outputs.

Navigate the sidebar on the left to dive into any module. Each page documents the environment variables, key classes, and concrete usage snippets for that toolkit.

---

FILE: Toolkit/memory/mem0.md

# Memory Tools (Mem0)

Spoon-toolkit provides Mem0-powered tools that plug into spoon-core agents for long-term memory. They wrap `spoon_ai.memory.mem0_client.SpoonMem0` and expose a consistent tool interface. `ToolResult.output` carries the raw Mem0 response (not a formatted string); validation failures or an unready client return `ToolResult.error` (e.g., â€œClient not readyâ€, â€œNo content provided.â€) instead of raising.

## Available tools
- `AddMemoryTool` â€” store text or conversation snippets.
- `SearchMemoryTool` â€” semantic/natural-language search over stored memories.
- `GetAllMemoryTool` â€” list memories (with paging/filters).
- `UpdateMemoryTool` â€” update an existing memory by memory_id.
- `DeleteMemoryTool` â€” delete a memory by memory_id.

All tools accept `mem0_config` (api_key/user_id/collection/metadata/filters/etc.) or an injected `SpoonMem0` client. If `mem0ai` or `MEM0_API_KEY` is missing, client initialization may fail; otherwise an unready client yields `ToolResult.error` rather than an exception.

Parameter merging behavior:
- `user_id` defaults to `mem0_config.user_id`/`agent_id` (or the clientâ€™s user_id) and is also injected into filters if missing.
- `collection`, `metadata`, and `filters` from the injected client/config are merged into each call; per-call metadata/filters override on conflict.
- `async_mode` is only forwarded when passed to `AddMemoryTool.execute`; it is not auto-propagated from `mem0_config` by these wrappers (rely on `SpoonMem0` defaults if needed).

## Quick usage (agent-side)
```python
from spoon_ai.agents.spoon_react import SpoonReactAI
from spoon_ai.chat import ChatBot
from spoon_ai.tools.tool_manager import ToolManager
from spoon_toolkits.memory import (
    AddMemoryTool, SearchMemoryTool, GetAllMemoryTool, UpdateMemoryTool, DeleteMemoryTool
)

MEM0_CFG = {
    "user_id": "defi_user_001",
    "metadata": {"project": "demo"},
    "async_mode": False,   # sync writes to avoid read-after-write delays
    "collection": "demo_mem",
}

class MemoryAgent(SpoonReactAI):
    mem0_config = MEM0_CFG
    available_tools = ToolManager([
        AddMemoryTool(mem0_config=MEM0_CFG),
        SearchMemoryTool(mem0_config=MEM0_CFG),
        GetAllMemoryTool(mem0_config=MEM0_CFG),
        UpdateMemoryTool(mem0_config=MEM0_CFG),
        DeleteMemoryTool(mem0_config=MEM0_CFG),
    ])

agent = MemoryAgent(
    llm=ChatBot(llm_provider="openrouter", model_name="anthropic/claude-3.5-sonnet", enable_long_term_memory=False),
)
```

## Demo flow (from `spoon-core/examples/mem0_tool_agent.py`)
The example walks through capture â†’ recall â†’ update â†’ delete:

1) **Capture**: `add_memory` with user preferences. Immediately verify with `get_all_memory` (same `user_id`) and then `search_memory` to show stored content.
2) **Recall**: Build a fresh agent with the same `mem0_config` and `search_memory` to retrieve prior preferences.
3) **Update**: `add_memory` new preference + `update_memory` a specific record (by id) to reflect the pivot; then `search_memory` again to confirm recency.
4) **Delete**: `delete_memory` the updated record; `get_all_memory` to show remaining memories.

Run the example:
```bash
python spoon-core/examples/mem0_tool_agent.py
```

## Tool parameters (summary)
- Shared: `user_id`, `metadata`, `filters`, `collection` inherited via `mem0_config` or passed per-call.
- Add: `content` or `messages`, `role` (default `user`), `async_mode`.
- Search: `query`, `limit`/`top_k`, optional filters.
- GetAll: `page`, `page_size`/`limit`/`top_k`, filters.
- Update: `memory_id` (required), `text`, `metadata`.
- Delete: `memory_id` (required).

## Best practices
- Keep a stable `user_id`/`collection` for scoping; pass it explicitly on each call for consistency.
- Use `async_mode=False` when you need immediate read-after-write in demos/tests.
- If you pass a custom `SpoonMem0` via `mem0_client`, you can reuse a single client across tools to avoid repeated pings/initialization.

---

FILE: Toolkit/social-media/index.md

---
title: Social Media Toolkit
---

# Social Media Toolkit

`spoon_toolkits.social_media` centralizes outbound and bidirectional messaging for Discord, Telegram, Twitter/X, and Email. All adapters share the same base classes, request/response models, and MCP-friendly interfaces, so you can swap channels without rewriting agent logic.

## Shared Architecture

- **Base classes** â€“ Every tool inherits from `SocialMediaToolBase`, which defines `send_message()` and `validate_config()`. Notification-only adapters extend `NotificationToolBase`; interactive bots (Discord/Telegram) extend `InteractiveToolBase` and add `start_bot()` / `stop_bot()`.
- **Models** â€“ Most `execute()` methods accept a `MessageRequest` subclass (e.g., `DiscordMessageRequest`, `EmailMessageRequest`) and return a `MessageResponse` with `{success: bool, message: str, data?: dict}`. Twitter exposes dedicated entry points `execute_tweet`, `execute_reply`, and `execute_like` instead of a single `execute()`.
- **Helper coroutines** â€“ Each adapter exports convenience helpers: Discord/Telegram/Email provide `send_*` functions that create the tool, run `validate_config()`, and return `{success, message}`; Twitter offers `post_tweet`, `reply_to_tweet`, and `like_tweet` helpers that also include a `data` payload on success.
- **Config validation** â€“ `validate_config()` logs a warning when required credentials are missing; call it during startup to fail fast rather than silently dropping messages.

## Environment Reference

| Channel | Required variables | Optional variables |
| --- | --- | --- |
| **Discord** | `DISCORD_BOT_TOKEN` | `DISCORD_DEFAULT_CHANNEL_ID` |
| **Telegram** | `TELEGRAM_BOT_TOKEN` | `TELEGRAM_DEFAULT_CHAT_ID`* |
| **Twitter/X** | `TWITTER_CONSUMER_KEY`, `TWITTER_CONSUMER_SECRET`, `TWITTER_ACCESS_TOKEN`, `TWITTER_ACCESS_TOKEN_SECRET` | `TWITTER_BEARER_TOKEN`, `TWITTER_USER_ID` |
| **Email (SMTP)** | `EMAIL_SMTP_SERVER`, `EMAIL_SMTP_PORT`, `EMAIL_SMTP_USER`, `EMAIL_SMTP_PASSWORD`, `EMAIL_FROM` | `EMAIL_DEFAULT_RECIPIENTS` |

Set these variables before importing the corresponding tool. Missing credentials surface when `validate_config()` runs or when the client attempts its first API call.

\*The current Telegram implementation falls back to a hard-coded placeholder chat ID when `chat_id` isnâ€™t provided. Supply the chat explicitly or override `default_chat_id` after instantiation until configuration support is added.

## Discord

- **Module**: `discord_tool.py`
- **Client**: `spoon_ai.social_media.discord.DiscordClient`
- **Key methods**:
  - `send_message(message, channel_id=None)` â€“ pushes text to a specific channel or falls back to `DISCORD_DEFAULT_CHANNEL_ID`.
  - `start_bot(agent=None)` / `stop_bot()` â€“ wraps the async gateway loop for interactive bots; pass your Spoon agent at construction time so inbound events are routed through planners.
  - `execute(DiscordMessageRequest)` â€“ MCP entry point; accepts `message` and optional `channel_id`.
- **Usage**:

```python
from spoon_toolkits.social_media.discord_tool import DiscordTool

discord = DiscordTool(agent=my_agent)
if discord.validate_config():
    await discord.send_message("Graph agent finished ingesting blocks")
```

Errors (invalid token, missing intents, closed WebSocket) bubble up as exceptions; wrap calls with retries or queue them through a supervisor task to avoid duplicate sessions.

## Telegram

- **Module**: `telegram_tool.py`
- **Client**: `spoon_ai.social_media.telegram.TelegramClient`
- **Key methods**:
  - `send_message(message, chat_id=None)` â€“ proactive messaging. If the tool was instantiated without an agent, it spins a temporary client just for this send call.
  - `start_bot()` / `stop_bot()` â€“ register webhook/polling handlers so agents can react to user prompts in real time.
  - `execute(TelegramMessageRequest)` â€“ MCP-friendly send wrapper.
- **Usage**:

```python
from spoon_toolkits.social_media.telegram_tool import TelegramTool

telegram = TelegramTool()
await telegram.send_message("ðŸ”” Validator risk threshold exceeded")
```

Rate limits are per bot token; Telegram will throttle after ~30 messages/s. `send_message` logs failures and returns `False` instead of raising, so check the boolean result and trigger your own retry/backoff logic. Provide a `chat_id` unless you have overridden `default_chat_id`, because the bundled placeholder (`"0000000000"`) is not a usable channel.

## Twitter / X

- **Module**: `twitter_tool.py`
- **Client**: `spoon_ai.social_media.twitter.TwitterClient`
- **Key methods**:
  - `execute_tweet(TwitterTweetRequest)` / `execute_reply(TwitterReplyRequest)` / `execute_like(TwitterLikeRequest)` â€“ MCP-friendly entry points for posting, replying, and liking.
  - `send_message(message, tags=None)` â€“ convenience layer that appends hashtags or mentions before delegating to the client.
  - `post_tweet(message)` â€“ publish a new status.
  - `reply_to_tweet(tweet_id, message)` â€“ threaded replies.
  - `like_tweet(tweet_id)` â€“ reaction utility for engagement workflows.
  - `read_timeline(count=None)` / `get_tweet_replies(tweet_id, count)` â€“ read helpers for downstream analysis.
- **Usage**:

```python
from spoon_toolkits.social_media.twitter_tool import TwitterTool

twitter = TwitterTool()
if twitter.validate_config():
    twitter.post_tweet("ðŸ“ˆ Treasury rebalance complete. New target: BTC 40%, ETH 35%.")
```

Be mindful of Twitter v2 rate limits (tweets: 300 per 3 hours; likes/replies have separate quotas). Wrap the client in a queue or add exponential backoff to avoid HTTP 429 responses.

## Email (SMTP)

- **Module**: `email_tool.py`
- **Client**: `spoon_ai.social_media.email.EmailNotifier`
- **Key methods**:
  - `send_message(message, to_emails=None, subject="Crypto Monitoring Alert", html_format=True)` â€“ send transactional emails; defaults to `EMAIL_DEFAULT_RECIPIENTS` when `to_emails` is omitted.
  - `execute(EmailMessageRequest)` â€“ MCP wrapper returning `MessageResponse`.
- **Usage**:

```python
from spoon_toolkits.social_media.email_tool import EmailTool

email_tool = EmailTool()
await email_tool.send_message(
    message="<h2>Risk Alert</h2><p>New honeypot detected.</p>",
    to_emails=["security@example.com"],
    subject="ðŸš¨ Token Risk Detected",
)
```

Most SMTP providers enforce connection limits and per-minute quotas; reuse the same tool instance to keep TLS sessions warm, and handle `smtplib` exceptions to trigger retries or fallback to another channel.

## MCP & Convenience Functions

Each adapter exposes an `execute()` method that accepts the corresponding Pydantic request model, making it trivial to register the tool with FastMCP:

```python
from spoon_toolkits.social_media.discord_tool import DiscordTool, DiscordMessageRequest

discord = DiscordTool()
response = await discord.execute(DiscordMessageRequest(message="Hello MCP"))
```

For lightweight scripts, use the helper coroutines:

```python
from spoon_toolkits.social_media.discord_tool import send_discord_message
from spoon_toolkits.social_media.email_tool import send_email
from spoon_toolkits.social_media.twitter_tool import post_tweet

await send_discord_message("Indexing complete")
await send_email("Pipeline healthy", ["ops@example.com"])
tweet_result = await post_tweet("Publishing release notes #SpoonAI")
```

Discord/Telegram/Email helpers return `{success, message}` dictionaries. Twitter helpers also include a `data` object with the API response (tweet ID, etc.) when successful.

## Operational Tips

- **Credential hygiene**: Load all tokens via environment variables or a secrets manager; never hardcode them in agent configs.
- **Retry/backoff**: Social APIs frequently throttle. Catch exceptions from `send_message`, apply exponential backoff, or offload to a job queue.
- **Bot lifecycle**: For Discord/Telegram `start_bot()`, run the bot in a dedicated task or process and call `stop_bot()` during shutdown to avoid orphaned connections.
- **Multi-channel redundancy**: Pair channels (e.g., Telegram + Email) for critical alerts so a single API outage doesnâ€™t silence notifications.

---

FILE: Toolkit/storage/aioz.md

---
title: AIOZ Storage
---

# AIOZ Storage

`spoon_toolkits.storage.aioz` adapts AIOZ's S3-compatible service for SpoonOS agents.

## Environment

```bash
export AIOZ_ENDPOINT_URL=https://gateway.aioz.io
export AWS_ACCESS_KEY=your_access_key
export AWS_SECRET_KEY=your_secret_key
```

Set optional `BUCKET_NAME` when using the helper coroutines in `aioz_tools.py`; the tools themselves only require the three variables above.

## Tools

- `AiozListBucketsTool` â€“ list all accessible buckets.
- `UploadFileToAiozTool` â€“ push a local file to a bucket.
- `DownloadFileFromAiozTool` â€“ fetch an object to a local path.
- `DeleteAiozObjectTool` â€“ remove a single object.
- `GenerateAiozPresignedUrlTool` â€“ produce temporary download URLs.

Bucket/object mutations emit emoji-prefixed status strings (âœ… success / âŒ failure). Wrap calls in agents that interpret the prefix to decide next steps.

### Return semantics & shared helpers
- Success responses look like `âœ… Uploaded 'file' to 'bucket'`.
- Failures bubble up `botocore` messages but remain human readable: `âŒ Failed to upload ... (Error: ...)`.
- `AiozListBucketsTool` returns a newline-separated string (emojis included) such as `ðŸ“ bucket-a`; parse the string if you need structured output.
- `GenerateAiozPresignedUrlTool` returns the raw URL string on success (no emoji prefix). Failures return `âŒ ...` strings coming from `_generate_presigned_url`.
- `GenerateAiozPresignedUrlTool` accepts `expires_in` in seconds; the default is 3600. AIOZ supports up to 7 daysâ€”set higher values when sharing large datasets.

## Usage Examples

### Upload & presign
```python
from spoon_toolkits.storage.aioz.aioz_tools import UploadFileToAiozTool, GenerateAiozPresignedUrlTool

uploader = UploadFileToAiozTool()
status = await uploader.execute(bucket_name="research-artifacts", file_path="/tmp/report.pdf")
print(status)

# Object keys default to the filename (here: report.pdf). Rename after upload if needed.
presigner = GenerateAiozPresignedUrlTool()
url = await presigner.execute(bucket_name="research-artifacts", object_key="report.pdf", expires_in=900)
print(url)
```

### Module self-test
Running the module directly executes the async test harness in `aioz_tools.py`, which performs list/upload/presign/download/delete flows using the configured credentials and `BUCKET_NAME`. Use this as a credential check, but be aware it will create/delete real objects rather than accept CLI arguments.

## Operation Tips
- **Endpoint style**: `S3Tool` forces path-style URLs; keep bucket names DNS-safe to avoid signature issues.
- **Access denied**: If you see `âŒ Failed ... (Error: An error occurred (AccessDenied))`, verify the access key/secret pair. AIOZ keys are distinct from AWS IAM keys.
- **Large uploads**: For >5GB objects, extend `AIOZMultipartUploadTool` (not shipped yet) using `S3Tool`'s `_create_multipart_upload` helpers; standard uploads will fail with `EntityTooLarge`.
- **Presign mismatch**: If the generated URL returns 403, ensure the bucket policy allows `s3:GetObject` for presigned requests and that your system clock is accurate.

## MCP / Agent integration

Each class inherits `BaseTool`. Register them in FastMCP like:
```python
from spoon_toolkits.storage.aioz.aioz_tools import AiozListBucketsTool

tool = AiozListBucketsTool()
result = await tool.execute()
```
To expose via FastMCP server, import the tool into your MCP registry or call the moduleâ€™s `main` script. Within Spoon agents, include `Aioz*` tools in the tool set and pass bucket parameters directly from planner prompts.

---

FILE: Toolkit/storage/foureverland.md

# 4EVERLAND Storage

`spoon_toolkits.storage.foureverland` integrates the 4EVERLAND decentralized storage service.

## Environment

```bash
export FOUREVERLAND_ENDPOINT_URL=https://endpoint.4everland.org
export FOUREVERLAND_ACCESS_KEY=your_access_key
export FOUREVERLAND_SECRET_KEY=your_secret_key
export FOUREVERLAND_BUCKET_NAME=default_bucket  # optional for helper scripts
```

## Tools

- `ListFourEverlandBuckets`
- `UploadFileToFourEverland`
- `DownloadFileFromFourEverland`
- `DeleteFourEverlandObject`
- `GenerateFourEverlandPresignedUrl`

Object listing currently requires calling the 4EVERLAND console or a custom script; there is no dedicated `ListObjectsFourEverland` helper yet.

All inherit from `FourEverlandStorageTool`, which itself extends `S3Tool`. That means you can expect identical method signatures and status string formats across the storage adapters, simplifying multi-provider automation.

## Return formats & shared behavior
- Most methods return strings starting with `âœ…`/`âŒ`. Agents can parse the first character to branch logic quickly.
- `ListFourEverlandBuckets` returns a newline-separated string with emoji prefixes (e.g., `ðŸ“ bucket-name`). Parse the string manually if structured data is required.
- `GenerateFourEverlandPresignedUrl` returns the presigned URL directly (no emoji prefix) and accepts `expires_in` (default 3600). 4EVERLAND caps presigned URLs at 24hâ€”higher values raise a validation error.
- Exceptions from boto3 bubble up only when the helper must return structured data; otherwise theyâ€™re converted to the `âŒ ... (Error: ...)` string.
- Upload helpers derive the destination key from `os.path.basename(file_path)`; rename via `copy_object` after upload if you need nested prefixes.

## Usage examples

```python
from spoon_toolkits.storage.foureverland.foureverland_tools import (
    UploadFileToFourEverland,
    ListFourEverlandBuckets,
)

uploader = UploadFileToFourEverland()
print(await uploader.execute(bucket_name="governance-data", file_path="/tmp/summary.json"))

lister = ListFourEverlandBuckets()
print(await lister.execute())
```

## Provider-specific notes

- **Endpoint nuance**: 4EVERLAND requires HTTPS; unsigned HTTP calls fail with `SSL required`. Ensure `FOUREVERLAND_ENDPOINT_URL` includes `https://`.
- **Bucket namespace**: Bucket names are global per account. If `CreateBucket` fails with `BucketAlreadyOwnedByYou`, delete or reuse the existing one.
- **Presign errors**: 4EVERLAND enforces lowercase bucket names; uppercase characters cause signature mismatches (403).

## MCP / agent integration

Register any `FourEverland*` tool with FastMCP:
```python
from spoon_toolkits.storage.foureverland.foureverland_tools import GenerateFourEverlandPresignedUrl

tool = GenerateFourEverlandPresignedUrl()
url = await tool.execute(bucket_name="datasets", object_key="daily.csv")
```
Because these classes inherit `BaseTool`, they plug into Spoon agent tool lists directly. For service-style exposure, mount them in your MCP server alongside other storage providers.

---

FILE: Toolkit/storage/oort.md

---
title: OORT Storage
---

# OORT Storage

`spoon_toolkits.storage.oort` provides S3-style tools for the OORT network.

## Environment

```bash
export OORT_ENDPOINT_URL=https://s3.oortech.com
export OORT_ACCESS_KEY=your_access_key
export OORT_SECRET_KEY=your_secret_key
```

## Tools

- `OortCreateBucketTool` / `OortDeleteBucketTool`
- `OortListBucketsTool`
- `OortListObjectsTool`
- `OortUploadFileTool` / `OortDownloadFileTool`
- `OortDeleteObjectTool` / `OortDeleteObjectsTool`
- `OortGeneratePresignedUrlTool`

These tools inherit retry and formatting logic from `S3Tool`; responses are concise strings suited for immediate user feedback or logging. Use `object_keys` arrays for batch deletions when cleaning up agent artifacts.

## Return & error semantics

- Bucket/object operations return emoji-prefixed strings (`âœ… Created bucket ...`, `âŒ Failed to delete object ... (Error: AccessDenied)`); parse them if you need structured status codes.
- `OortListObjectsTool` returns a newline-separated string of bullet points (e.g., `â€¢ key (Size: 123)`), and `OortListBucketsTool` emits emoji-prefixed bucket names. Convert these strings yourself if your workflow requires JSON. Errors raised by `_list_objects` propagate as `RuntimeError`, so wrap the call if you need graceful handling.
- Batch delete accepts an `object_keys` list and replies with the count requested for deletion (`ðŸ—‘ï¸ Deleted N objects ...`). It does not currently surface per-key AWS errors.
- Presigned URLs honour `expires_in` (default 3600). OORT supports up to 12 hours per token. The `OortGeneratePresignedUrlTool` returns the raw URL string without emoji prefixes.
- Boto3 exceptions only bubble up when returning structured data (list objects) or presigned URL failures; otherwise they are embedded in the failure string.
- Upload helpers derive the destination key from `os.path.basename(file_path)`. Use `_copy_object` if you need to rename or place objects under prefixes.

## Usage examples

```python
from spoon_toolkits.storage.oort.oort_tools import (
    OortCreateBucketTool,
    OortUploadFileTool,
    OortDeleteObjectsTool,
)

creator = OortCreateBucketTool()
print(await creator.execute(bucket_name="agent-artifacts"))

uploader = OortUploadFileTool()
print(await uploader.execute(bucket_name="agent-artifacts", file_path="/tmp/logs.zip"))

deleter = OortDeleteObjectsTool()
print(await deleter.execute(bucket_name="agent-artifacts", object_keys=["logs.zip", "old-report.pdf"]))
```

CLI check:
```
python spoon_toolkits/storage/oort/oort_tools.py list-buckets
```
When run without arguments the module executes the async tests defined at the bottom (create/upload/download flows). Treat it as an integration check rather than an argument-driven CLI.

## Operational Tips

- **Batch delete**: `OortDeleteObjectsTool` leverages `_delete_objects`; pass up to 1000 keys per call, but expect only a count in the response. Retry the entire batch or reissue with smaller sets if something fails.
- **Bucket lifecycle**: Newly created buckets may take a few seconds to propagate; handle `BucketAlreadyExists` by retrying with backoff or generating unique names.
- **Endpoint**: Keep `OORT_ENDPOINT_URL` aligned with your accountâ€™s region; otherwise you may see `PermanentRedirect`.
- **Permissions**: Ensure your key pair has `s3:ListAllMyBuckets` and object-level permissions; lacking one results in `âŒ ... AccessDenied`.

## MCP / agent usage

```python
from spoon_toolkits.storage.oort.oort_tools import OortGeneratePresignedUrlTool

tool = OortGeneratePresignedUrlTool()
url = await tool.execute(bucket_name="agent-artifacts", object_key="logs.zip", expires_in=600)
```

Because every tool extends `BaseTool`, add them to your agentâ€™s toolset or mount them in a FastMCP server to share across workspaces.

